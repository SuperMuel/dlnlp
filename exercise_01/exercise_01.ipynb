{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd8ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/s.mallet/passau/dlnlp/.venv/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59a5ec-4e46-4277-a58e-70ba131703a8",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language and Code: Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b027f58-0bcf-4e69-afb8-a538753b88a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:27:10.097734Z",
     "start_time": "2025-04-16T07:27:02.573774Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d974f0-1885-437d-b0fb-adf7d95484b6",
   "metadata": {},
   "source": [
    "# Task 1: LIBSVM - BOW\n",
    "\n",
    "## Load data\n",
    "\n",
    "1. Download the dataset from [here](https://ai.stanford.edu/%7Eamaas/data/sentiment)\n",
    "1. Copy the dataset next to this Jupyter (.ipynb file)\n",
    "1. Install:\n",
    "    * Sklearn (This library is only allowed to use for reading the BOW in LIBSVM format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efb3393-4a19-40bf-9e5c-557bd1331da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bow_libsvm(path_data):\n",
    "    \"\"\" Function for reading the data in the LIBSVM format (.feat files). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname: str\n",
    "        path of this file \n",
    "    path_data: str\n",
    "        path of the folder that contains the data that is going to be used. (should be test or train)\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    X,y: array_like\n",
    "        Data arrays, X is an array of shape [#documents of the dataset, #words in the vocabulary], y is an array of shape [#documents,] \n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = load_svmlight_file(os.path.join(\".\",\"aclImdb\", path_data, \"labeledBow.feat\")) \n",
    "    \n",
    "    # why?\n",
    "    X = X.todense()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db1fe5f-277e-4bf4-9498-8225fe7e9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab():\n",
    "    \"\"\" Function for reading the vocabulary (.vocab file). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    vocab: list\n",
    "        list with the values of y ...... \n",
    "    \"\"\"\n",
    "    \n",
    "    path_vocab = os.path.join(\".\", \"aclImdb\", \"imdb.vocab\")\n",
    "    \n",
    "    with open(path_vocab) as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    lines = lines.split('\\n')\n",
    "    \n",
    "    vocab = []\n",
    "    for line in lines:\n",
    "        vocab.append(line)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d859674d-6750-4159-a5d0-10238bcce7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vocabulary\n",
    "vocabulary = read_vocab()\n",
    "# Read the the bag of words and the Y for the training data\n",
    "X, y = read_bow_libsvm('train')\n",
    "# Read the the bag of words and the Y for the test data\n",
    "X_test, y_test = read_bow_libsvm('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b19b0-95c0-4675-8e31-299bada4893a",
   "metadata": {},
   "source": [
    "## Understanding the LIBSVM-BOW\n",
    "1. What are the values in y? \n",
    "1. Why do we use  todense() in the read_bow_libsvm function?\n",
    "1. How could you know which word in the vocabulary corresponds to each index in the BOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372421f6-c841-4352-8c85-fab7d9c58ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with the previous loaded variables and understand the values of each one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed720048-0950-4dc1-b78b-29bed1c002ed",
   "metadata": {},
   "source": [
    "# Task 2: Bag of Words (BOW)\n",
    "## Load Raw text and scores \n",
    "\n",
    "1. Be sure to have downloaded the dataset from the link provided in the exercise and have read the README file\n",
    "1. Be sure to have copied the dataset next to this Jupyter (.ipynb file)\n",
    "1. Be sure to have installed:\n",
    "    * Numpy\n",
    "    * NLTK (only for the stemming process)\n",
    "    * Sklearn (only for building a Random Forest)\n",
    "1. In this part of the exercise it is not allowed to use Sklearn\n",
    "1. Build the Bag Of Words (BOW) with the raw data, for this you need to:\n",
    "    * Tokenize on spaces and punctuation\n",
    "    * Lower case\n",
    "    * Remove punctuation\n",
    "    * Remove terms appearing more often than X percent, this X percent should be variable. Which means that you should be able to change the percentage as a parameter.\n",
    "    * Use NLTK porter stemmer\n",
    "1. Build a classifier with the BOW previously built. Take into account:\n",
    "    * The RF should be a binary classification positive (i.e., score >=7) and negative (i.e., score <= 4)\n",
    "    * Test the classifier with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c27fe9c1-7683-4509-9298-0f3723687178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:16:11.712919Z",
     "start_time": "2025-04-16T09:16:08.462275Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653a40-5528-4a9f-b136-eaa094e3136c",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Read all the training data, including the reviews and the scores associated to each one. Be sure to explore the data and learn characteristics of them, such as the type of encoding and special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee137acb-9b2b-448e-b1f1-0db37a5891ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7ae0a-ea88-430c-bebf-fccb7b2ca242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0338e082-2c77-4a74-97c0-235954eb8edb",
   "metadata": {},
   "source": [
    "## Clean HTML and tokenize text\n",
    "Clean the review, handle the special characters, remove the html tags and tokenize the text based on the instructions given in the exercise sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efea862-1a2d-4989-b509-3855e0ee02f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17db482c8ac12c44",
   "metadata": {},
   "source": [
    "## Convert to lower case and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef0a10b81efc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30e60927-be67-4ce4-a547-cbabdc80a7a8",
   "metadata": {},
   "source": [
    "## Remove X percentage and build vocabulary \n",
    "Remove all the tokens that do not meet the requirements based on the exercise sheet and build the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d6bad-697c-4197-b793-6daee70361eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a68d0a87d09b21b8",
   "metadata": {},
   "source": [
    "## Use Porter Stemmer for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55caca67c358ae1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9044aac-ffe8-4c00-8c1f-72e74ad1f214",
   "metadata": {},
   "source": [
    "## Build the bag of words (BOW)\n",
    "For building the matrix for the representation of bag of words use the previously built vocabulary and tokens for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b877c-30f7-4fa3-9ed0-29229aa421b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787c3809-f76c-4246-b4d3-07b88c2780a1",
   "metadata": {},
   "source": [
    "# Task 3: Comparing BOWs\n",
    "\n",
    "1. Use the previous steps to build a bag of words with the training data in which the tokens that appear more than 1% are discarded. \n",
    "1. Compare your BOW with LIBSVM BOW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17defc98-b2e2-4b18-b31e-53f5e591ed54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25602e22-bd28-468b-bfc7-331e9e60fcc8",
   "metadata": {},
   "source": [
    "# Task 4: Train a Random Forest and test it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80371ade-b3ba-4400-a53a-d992986c2332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431e66c7-5999-4861-a3ea-c172552d1f01",
   "metadata": {},
   "source": [
    "# Task 5: Markov chain\n",
    "Tip: For memory optimization use sparse structures not a matrix mostrly filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2a401-c7e0-4728-a7e0-cc4901ac9b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aa44a3b-38b1-4618-81f0-080e613b7f0f",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "Read the data and using the previous built functions for the BOW representation create a list of words per each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a587818-9507-4e35-9698-82db66404a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b344c6d-a9c3-4422-a464-25f58c875ba4",
   "metadata": {},
   "source": [
    "## Chain words\n",
    "Identify all the possible pairs of words (w0, w1) in all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4f1f-5c76-44d5-9711-57e12b5c2f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ce3183-6431-4a4c-a3a8-ed9fa51f7680",
   "metadata": {},
   "source": [
    "## Initialize the Markov's Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edee359-021e-4e0a-99d6-72b3d86ecdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3479a238-1e1e-4884-a951-cda7f848d40d",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Here you could also try to generate words for the unlabeled part of the dataset. Try to meassure the quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d8f92-a060-4f0b-8576-2e43bea89189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
