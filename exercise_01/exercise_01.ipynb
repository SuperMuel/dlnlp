{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd8ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/s.mallet/passau/dlnlp/.venv/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install scikit-learn numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59a5ec-4e46-4277-a58e-70ba131703a8",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language and Code: Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d974f0-1885-437d-b0fb-adf7d95484b6",
   "metadata": {},
   "source": [
    "# Task 1: LIBSVM - BOW\n",
    "\n",
    "## Load data\n",
    "\n",
    "1. Download the dataset from [here](https://ai.stanford.edu/%7Eamaas/data/sentiment)\n",
    "1. Copy the dataset next to this Jupyter (.ipynb file)\n",
    "1. Install:\n",
    "    * Sklearn (This library is only allowed to use for reading the BOW in LIBSVM format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8981d6",
   "metadata": {},
   "source": [
    "A **LIBSVM file** is a plain text file format used to store **sparse datasets** for machine learning tasks, especially **classification** and **regression**. It's called \"LIBSVM\" because it was originally used by the **LIBSVM** library, a very popular library for Support Vector Machines.\n",
    "\n",
    "The format looks like this:\n",
    "\n",
    "```\n",
    "<label> <index1>:<value1> <index2>:<value2> <index3>:<value3> ...\n",
    "```\n",
    "\n",
    "- `<label>` = the **target value** (for example, `1` for positive, `-1` for negative).\n",
    "- `<index>:<value>` = the **non-zero features**.\n",
    "  - `<index>` is the feature number (starting at 1),\n",
    "  - `<value>` is the value of that feature (usually the count or some preprocessed weight).\n",
    "\n",
    "If a feature is **zero**, it is simply **omitted** from the line (to save space — this is why it's called *sparse* format).\n",
    "\n",
    "---\n",
    "\n",
    "### A real small example:\n",
    "\n",
    "Suppose we have two movie reviews turned into a Bag-of-Words (BoW):\n",
    "- Feature 1 = \"awesome\"\n",
    "- Feature 2 = \"terrible\"\n",
    "- Feature 3 = \"boring\"\n",
    "- Feature 4 = \"amazing\"\n",
    "\n",
    "And two reviews:\n",
    "- Review 1 (positive): \"awesome amazing\"\n",
    "- Review 2 (negative): \"terrible boring boring\"\n",
    "\n",
    "The LIBSVM file would look like:\n",
    "\n",
    "```\n",
    "1 1:1 4:1\n",
    "-1 2:1 3:2\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The first line:\n",
    "  - `1` → label is positive\n",
    "  - `1:1` → \"awesome\" appeared once\n",
    "  - `4:1` → \"amazing\" appeared once\n",
    "- The second line:\n",
    "  - `-1` → label is negative\n",
    "  - `2:1` → \"terrible\" appeared once\n",
    "  - `3:2` → \"boring\" appeared twice\n",
    "\n",
    "---\n",
    "\n",
    "### Why use LIBSVM format?\n",
    "\n",
    "- It's super lightweight for huge datasets where most feature values are 0.\n",
    "- It's easy to parse and generate manually.\n",
    "- Many machine learning tools accept this format directly (e.g., SVM, Random Forest, logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288552ef",
   "metadata": {},
   "source": [
    "**What's inside `./aclImdb/train/labeledBow.feat` ?**\n",
    "\n",
    "- There are 25 000 lines\n",
    "- One line per sample\n",
    "- first element of each line is the label (frol 1 to 10)\n",
    "- others are the number of apparences of each feature (e.g `0:9 1:1` means feature 0 appeared 9 times and the feature 1 appeared only once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f44a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "type Feature = int\n",
    "type Occurrence = int\n",
    "type Label = int\n",
    "\n",
    "def parse_libsvm_line(line:str) -> tuple[list[tuple[Feature, Occurrence]], Label]:\n",
    "    label, features = line.split(' ', 1)\n",
    "    features = features.split(' ')\n",
    "    features = cast(list[tuple[int, int]], [tuple(map(int, feature.split(':'))) for feature in features])\n",
    "    return features, int(label)\n",
    "\n",
    "assert parse_libsvm_line('1 0:9 1:1 14:87') == ([(0, 9), (1, 1), (14, 87)], 1)\n",
    "\n",
    "def parse_libsvm_content(content:str)->tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    data=  [parse_libsvm_line(line) for line in content.split('\\n') if line]\n",
    "\n",
    "    vocabulary_size= max((feature for features, _ in data for feature, _ in features), default=0) + 1\n",
    "    X = np.zeros((len(data), vocabulary_size))\n",
    "    y = np.zeros(len(data))\n",
    "    for i, (line, label) in enumerate(data):\n",
    "        for feature, occurrence in line:\n",
    "            X[i, feature] = occurrence\n",
    "        y[i] = label\n",
    "    return X, y\n",
    "\n",
    "X, y = parse_libsvm_content('1 0:9 1:1 3:87\\n-1 2:1 3:2')\n",
    "assert np.all(X== [[9, 1, 0, 87],[0,0,1,2]])\n",
    "assert np.all(y== [1,-1])\n",
    "\n",
    "def load_libsvm_file(path:str)->tuple[np.ndarray, np.ndarray]:\n",
    "    with open(path, 'r') as f:\n",
    "        return parse_libsvm_content(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6422fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_libsvm_file('./aclImdb/train/labeledBow.feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a02a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.,  1.,  4., ...,  0.,  0.,  0.],\n",
       "       [ 7.,  4.,  2., ...,  0.,  0.,  0.],\n",
       "       [ 4.,  4.,  4., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [17.,  6.,  7., ...,  0.,  0.,  0.],\n",
       "       [15.,  8.,  3., ...,  0.,  0.,  0.],\n",
       "       [10.,  2.,  2., ...,  0.,  0.,  0.]], shape=(25000, 89527))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b182ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9., 7., 9., ..., 4., 2., 2.], shape=(25000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efb3393-4a19-40bf-9e5c-557bd1331da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "X_sklearn, y_sklearn = load_svmlight_file('./aclImdb/train/labeledBow.feat')\n",
    "\n",
    "assert X_sklearn.shape == X.shape\n",
    "assert np.allclose(X_sklearn.todense(), X)\n",
    "assert np.allclose(y_sklearn, y)\n",
    "# ->  Good, my implementation is consistent with the sklearn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b556ad",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db1fe5f-277e-4bf4-9498-8225fe7e9fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "VOCAB_FILE = Path('./aclImdb/imdb.vocab')\n",
    "assert VOCAB_FILE.exists()\n",
    "\n",
    "def read_vocab():\n",
    "    return VOCAB_FILE.read_text().splitlines()\n",
    "\n",
    "\n",
    "vocab = read_vocab()\n",
    "\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d859674d-6750-4159-a5d0-10238bcce7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the the bag of words and the Y for the training data\n",
    "X, y = load_libsvm_file('./aclImdb/train/labeledBow.feat')\n",
    "# Read the the bag of words and the Y for the test data\n",
    "X_test, y_test = load_libsvm_file('./aclImdb/test/labeledBow.feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed720048-0950-4dc1-b78b-29bed1c002ed",
   "metadata": {},
   "source": [
    "# Task 2: Bag of Words (BOW)\n",
    "## Load Raw text and scores \n",
    "\n",
    "1. Be sure to have downloaded the dataset from the link provided in the exercise and have read the README file\n",
    "1. Be sure to have copied the dataset next to this Jupyter (.ipynb file)\n",
    "1. Be sure to have installed:\n",
    "    * Numpy\n",
    "    * NLTK (only for the stemming process)\n",
    "    * Sklearn (only for building a Random Forest)\n",
    "1. In this part of the exercise it is not allowed to use Sklearn\n",
    "1. Build the Bag Of Words (BOW) with the raw data, for this you need to:\n",
    "    * Tokenize on spaces and punctuation\n",
    "    * Lower case\n",
    "    * Remove punctuation\n",
    "    * Remove terms appearing more often than X percent, this X percent should be variable. Which means that you should be able to change the percentage as a parameter.\n",
    "    * Use NLTK porter stemmer\n",
    "1. Build a classifier with the BOW previously built. Take into account:\n",
    "    * The RF should be a binary classification positive (i.e., score >=7) and negative (i.e., score <= 4)\n",
    "    * Test the classifier with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c27fe9c1-7683-4509-9298-0f3723687178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:16:11.712919Z",
     "start_time": "2025-04-16T09:16:08.462275Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653a40-5528-4a9f-b136-eaa094e3136c",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Read all the training data, including the reviews and the scores associated to each one. Be sure to explore the data and learn characteristics of them, such as the type of encoding and special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee137acb-9b2b-448e-b1f1-0db37a5891ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7ae0a-ea88-430c-bebf-fccb7b2ca242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0338e082-2c77-4a74-97c0-235954eb8edb",
   "metadata": {},
   "source": [
    "## Clean HTML and tokenize text\n",
    "Clean the review, handle the special characters, remove the html tags and tokenize the text based on the instructions given in the exercise sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efea862-1a2d-4989-b509-3855e0ee02f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17db482c8ac12c44",
   "metadata": {},
   "source": [
    "## Convert to lower case and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef0a10b81efc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30e60927-be67-4ce4-a547-cbabdc80a7a8",
   "metadata": {},
   "source": [
    "## Remove X percentage and build vocabulary \n",
    "Remove all the tokens that do not meet the requirements based on the exercise sheet and build the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d6bad-697c-4197-b793-6daee70361eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a68d0a87d09b21b8",
   "metadata": {},
   "source": [
    "## Use Porter Stemmer for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55caca67c358ae1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9044aac-ffe8-4c00-8c1f-72e74ad1f214",
   "metadata": {},
   "source": [
    "## Build the bag of words (BOW)\n",
    "For building the matrix for the representation of bag of words use the previously built vocabulary and tokens for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b877c-30f7-4fa3-9ed0-29229aa421b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787c3809-f76c-4246-b4d3-07b88c2780a1",
   "metadata": {},
   "source": [
    "# Task 3: Comparing BOWs\n",
    "\n",
    "1. Use the previous steps to build a bag of words with the training data in which the tokens that appear more than 1% are discarded. \n",
    "1. Compare your BOW with LIBSVM BOW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17defc98-b2e2-4b18-b31e-53f5e591ed54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25602e22-bd28-468b-bfc7-331e9e60fcc8",
   "metadata": {},
   "source": [
    "# Task 4: Train a Random Forest and test it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80371ade-b3ba-4400-a53a-d992986c2332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431e66c7-5999-4861-a3ea-c172552d1f01",
   "metadata": {},
   "source": [
    "# Task 5: Markov chain\n",
    "Tip: For memory optimization use sparse structures not a matrix mostrly filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2a401-c7e0-4728-a7e0-cc4901ac9b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aa44a3b-38b1-4618-81f0-080e613b7f0f",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "Read the data and using the previous built functions for the BOW representation create a list of words per each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a587818-9507-4e35-9698-82db66404a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b344c6d-a9c3-4422-a464-25f58c875ba4",
   "metadata": {},
   "source": [
    "## Chain words\n",
    "Identify all the possible pairs of words (w0, w1) in all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4f1f-5c76-44d5-9711-57e12b5c2f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ce3183-6431-4a4c-a3a8-ed9fa51f7680",
   "metadata": {},
   "source": [
    "## Initialize the Markov's Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edee359-021e-4e0a-99d6-72b3d86ecdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3479a238-1e1e-4884-a951-cda7f848d40d",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Here you could also try to generate words for the unlabeled part of the dataset. Try to meassure the quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d8f92-a060-4f0b-8576-2e43bea89189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
