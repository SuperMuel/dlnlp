{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e944d31a1f9c682",
   "metadata": {},
   "source": [
    "# Exercise 4: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a26dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c6b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define base path to the dataset\n",
    "BASE_DATA_PATH = Path(\"../exercise_01/aclImdb\") \n",
    "\n",
    "TRAIN_PATH = BASE_DATA_PATH / \"train\"\n",
    "TEST_PATH = BASE_DATA_PATH / \"test\"\n",
    "\n",
    "TRAIN_POS_PATH = TRAIN_PATH / \"pos\"\n",
    "TRAIN_NEG_PATH = TRAIN_PATH / \"neg\"\n",
    "TEST_POS_PATH = TEST_PATH / \"pos\"\n",
    "TEST_NEG_PATH = TEST_PATH / \"neg\"\n",
    "\n",
    "assert TRAIN_POS_PATH.exists() and TRAIN_NEG_PATH.exists() and TEST_POS_PATH.exists() and TEST_NEG_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539063c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 training reviews (subset).\n",
      "Training labels distribution: Positive (1): 1006, Negative (0): 994\n",
      "Loaded 25000 test reviews.\n",
      "Test labels distribution: Positive (1): 12500, Negative (0): 12500\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def load_imdb_data(\n",
    "    data_path: Path, \n",
    "    subset_size: int | None = None, \n",
    "    random_seed: int = 42\n",
    ") -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Loads movie reviews and their sentiments from the specified path.\n",
    "    If subset_size is provided, returns a random subset of that size (with fixed seed).\n",
    "    \"\"\"\n",
    "    texts: list[str] = []\n",
    "    labels: list[int] = [] \n",
    "\n",
    "    for sentiment, folder_path in [(\"pos\", data_path / \"pos\"), (\"neg\", data_path / \"neg\")]:\n",
    "        if not folder_path.exists():\n",
    "            raise FileNotFoundError(f\"Warning: Path {folder_path} does not exist.\")\n",
    "\n",
    "        binary_label = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "        for file_path in folder_path.glob(\"*.txt\"):\n",
    "            texts.append(file_path.read_text(encoding='utf-8'))\n",
    "            labels.append(binary_label)  # Use binary label, not score\n",
    "\n",
    "    if subset_size is not None and subset_size < len(texts):\n",
    "        random.seed(random_seed)\n",
    "        indices = list(range(len(texts)))\n",
    "        random.shuffle(indices)\n",
    "        selected_indices = indices[:subset_size]\n",
    "        texts = [texts[i] for i in selected_indices]\n",
    "        labels = [labels[i] for i in selected_indices]\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Set the subset size for training data (e.g., 5000 for a smaller subset)\n",
    "TRAIN_SUBSET_SIZE = 2000  # Change as needed\n",
    "\n",
    "# Load training data (random subset)\n",
    "train_texts_raw, train_labels = load_imdb_data(TRAIN_PATH, subset_size=TRAIN_SUBSET_SIZE, random_seed=42)\n",
    "print(f\"Loaded {len(train_texts_raw)} training reviews (subset).\")\n",
    "print(f\"Training labels distribution: Positive (1): {sum(train_labels)}, Negative (0): {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Load test data (full set)\n",
    "test_texts_raw, test_labels = load_imdb_data(TEST_PATH)\n",
    "print(f\"Loaded {len(test_texts_raw)} test reviews.\")\n",
    "print(f\"Test labels distribution: Positive (1): {sum(test_labels)}, Negative (0): {len(test_labels) - sum(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58625878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This is one of those movies that you and a bunch of friends sit around drinking beers, eating pizza, and laugh at. Unfortunately for me I found myself watching this one alone. My friends and I rented a big block of movies and never got around to seeing this one. It was due back and I figured that it was a waste not to watch it. So I did, and I was impressed at how absolutely terrible this movie is.<br /><br />Now, I love bad movies quite a bit, and I probably would have liked this one if the \"hero\" wasn\\'t so utterly loathsome. The entire movie I was hoping that he\\'d put that stupid sword down and let someone kill him! He does very little heroic things in the movie. He\\'s a beefy, disgusting, stupid thing. He has less redeeming qualities than the villains do. And what was it with all the naked chicks? I mean, I love naked chicks just as much as the next guy, but this movie went a tad overboard in that department.<br /><br />Well, anyway, if you love bad movies and can stand a disgusting \"hero\" then I\\'m sure you\\'ll like this schlock of a film.',\n",
       "  'This documentary on schlockmeister William Castle takes a few cheap shots at the naive \\'50s-\\'60s environment in which he did his most characteristic work--look at the funny, silly people with the ghost-glasses--but it\\'s also affectionate and lively, with particularly bright commentary from John Waters, who was absolutely the target audience for such things at the time, and from Castle\\'s daughter, who adored her dad and also is pretty perceptive about how he plied his craft. (We never find out what became of the other Castle offspring.) The movies were not very good, it makes clear, but his marketing of them was brilliant, and he appears to have been a sweet, hardworking family man. Fun people keep popping up, like \"Straight Jacket\"\\'s Diane Baker, who looks great, and Anne Helm, whom she replaced at the instigation of star Joan Crawford. Darryl Hickman all but explodes into giggles at the happy memory of working with Castle on \"The Tingler,\" and there\\'s enough footage to give us an idea of the level of Castle\\'s talent--not very high, but very energetic. A pleasant look at a time when audiences were more easily pleased, and it does make you nostalgic for simpler movie-going days.'],\n",
       " [0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_raw[:2], train_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae05810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing default behavior...\n",
      "Testing lowercase=False...\n",
      "Testing replace_numbers=False...\n",
      "Testing tokenize_punct=True, remove_punct=False...\n",
      "Testing tokenize_punct=True, remove_punct=True...\n",
      "Testing tokenize_punct=False, remove_punct=False...\n",
      "Testing high-frequency removal (threshold=0.7)...\n",
      "Testing high-frequency removal (threshold=0.6)...\n",
      "Testing edge cases...\n",
      "\n",
      "✅ All tests for the recommended pre_process function passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import List, Set\n",
    "\n",
    "\n",
    "def pre_process(\n",
    "    reviews: List[str],\n",
    "    tokenize_punct: bool = False,\n",
    "    lowercase: bool = True,\n",
    "    remove_punct: bool = True,\n",
    "    remove_high_freq_terms: bool = False,\n",
    "    high_freq_threshold: float = 0.5,\n",
    "    replace_numbers: bool = True\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Pre-processes a list of text reviews by tokenizing, cleaning, and normalizing them.\n",
    "\n",
    "    Args:\n",
    "        reviews (List[str]): A list of raw review strings.\n",
    "        tokenize_punct (bool): If True, punctuation marks are treated as separate tokens.\n",
    "                               If False, punctuation is discarded during tokenization.\n",
    "        lowercase (bool): If True, converts all text to lowercase.\n",
    "        remove_punct (bool): If True, removes punctuation tokens.\n",
    "                             Note: This is only effective if `tokenize_punct` is True.\n",
    "        remove_high_freq_terms (bool): If True, removes words that appear in more than\n",
    "                                       `high_freq_threshold` proportion of documents.\n",
    "        high_freq_threshold (float): The document frequency threshold for removing terms.\n",
    "        replace_numbers (bool): If True, replaces all sequences of digits with a 'num' token.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list of processed reviews, where each review is a list of tokens.\n",
    "    \"\"\"\n",
    "    processed_reviews: List[List[str]] = []\n",
    "    punct_set: Set[str] = set(string.punctuation)\n",
    "    \n",
    "    for review in reviews:\n",
    "        text: str = review\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_numbers:\n",
    "            # Use a simple token 'num' that is compatible with the tokenizer.\n",
    "            text = re.sub(r'\\d+', 'num', text)\n",
    "        \n",
    "        # Tokenize based on the specified flag\n",
    "        if tokenize_punct:\n",
    "            # Keeps words and punctuation as separate tokens\n",
    "            tokens: List[str] = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        else:\n",
    "            # Keeps only word characters, effectively discarding punctuation\n",
    "            tokens: List[str] = re.findall(r'\\w+', text)\n",
    "        \n",
    "        # This step is only effective if tokenize_punct=True\n",
    "        if remove_punct and tokenize_punct:\n",
    "            tokens = [token for token in tokens if token not in punct_set]\n",
    "        \n",
    "        processed_reviews.append(tokens)\n",
    "    \n",
    "    if remove_high_freq_terms:\n",
    "        doc_freq = Counter()\n",
    "        # Calculate document frequency (word appears in how many docs)\n",
    "        for tokens_list in processed_reviews:\n",
    "            doc_freq.update(set(tokens_list))\n",
    "            \n",
    "        num_docs: int = len(processed_reviews)\n",
    "        # Identify terms that are too frequent\n",
    "        high_freq_terms = {term for term, freq in doc_freq.items() if (freq / num_docs) > high_freq_threshold}\n",
    "        \n",
    "        # Filter out the high-frequency terms from all reviews\n",
    "        processed_reviews = [[token for token in tokens_list if token not in high_freq_terms] for tokens_list in processed_reviews]\n",
    "    \n",
    "    return processed_reviews\n",
    "\n",
    "# ==============================================================================\n",
    "# ==================== FINAL CORRECTED TEST SUITE ==============================\n",
    "# ==============================================================================\n",
    "\n",
    "# Define a controlled test corpus to check all behaviors\n",
    "test_corpus = [\n",
    "    \"Hello common world! This is test 123.\",\n",
    "    \"Another common sentence, with a unique word.\",\n",
    "    \"Just a common sentence.\"\n",
    "]\n",
    "\n",
    "# --- Test 1: Default behavior ---\n",
    "print(\"Testing default behavior...\")\n",
    "processed = pre_process(test_corpus)\n",
    "expected_default = [\n",
    "    ['hello', 'common', 'world', 'this', 'is', 'test', 'num'],\n",
    "    ['another', 'common', 'sentence', 'with', 'a', 'unique', 'word'],\n",
    "    ['just', 'a', 'common', 'sentence']\n",
    "]\n",
    "assert processed == expected_default, f\"Default test failed. Got: {processed}\"\n",
    "\n",
    "# --- Test 2: `lowercase=False` ---\n",
    "print(\"Testing lowercase=False...\")\n",
    "processed = pre_process(test_corpus, lowercase=False)\n",
    "expected_no_lower = [\n",
    "    ['Hello', 'common', 'world', 'This', 'is', 'test', 'num'],\n",
    "    ['Another', 'common', 'sentence', 'with', 'a', 'unique', 'word'],\n",
    "    ['Just', 'a', 'common', 'sentence']\n",
    "]\n",
    "assert processed == expected_no_lower, f\"lowercase=False test failed. Got: {processed}\"\n",
    "\n",
    "# --- Test 3: `replace_numbers=False` ---\n",
    "print(\"Testing replace_numbers=False...\")\n",
    "processed = pre_process(test_corpus, replace_numbers=False)\n",
    "expected_no_num_replace = [\n",
    "    ['hello', 'common', 'world', 'this', 'is', 'test', '123'],\n",
    "    ['another', 'common', 'sentence', 'with', 'a', 'unique', 'word'],\n",
    "    ['just', 'a', 'common', 'sentence']\n",
    "]\n",
    "assert processed == expected_no_num_replace, f\"replace_numbers=False test failed. Got: {processed}\"\n",
    "\n",
    "# --- Test 4: Punctuation Handling ---\n",
    "print(\"Testing tokenize_punct=True, remove_punct=False...\")\n",
    "processed = pre_process(test_corpus, tokenize_punct=True, remove_punct=False)\n",
    "expected_keep_punct = [\n",
    "    ['hello', 'common', 'world', '!', 'this', 'is', 'test', 'num', '.'],\n",
    "    ['another', 'common', 'sentence', ',', 'with', 'a', 'unique', 'word', '.'],\n",
    "    ['just', 'a', 'common', 'sentence', '.']\n",
    "]\n",
    "assert processed == expected_keep_punct, f\"Keep punctuation test failed. Got: {processed}\"\n",
    "\n",
    "print(\"Testing tokenize_punct=True, remove_punct=True...\")\n",
    "processed = pre_process(test_corpus, tokenize_punct=True, remove_punct=True)\n",
    "assert processed == expected_default, f\"Tokenize then remove punctuation test failed. Got: {processed}\"\n",
    "\n",
    "print(\"Testing tokenize_punct=False, remove_punct=False...\")\n",
    "processed = pre_process(test_corpus, tokenize_punct=False, remove_punct=False)\n",
    "assert processed == expected_default, f\"Discard punctuation test failed. Got: {processed}\"\n",
    "\n",
    "# --- Test 5: High-Frequency Term Removal ---\n",
    "print(\"Testing high-frequency removal (threshold=0.7)...\")\n",
    "processed = pre_process(test_corpus, remove_high_freq_terms=True, high_freq_threshold=0.7)\n",
    "expected_remove_common = [\n",
    "    ['hello', 'world', 'this', 'is', 'test', 'num'],\n",
    "    ['another', 'sentence', 'with', 'a', 'unique', 'word'],\n",
    "    ['just', 'a', 'sentence']\n",
    "]\n",
    "assert processed == expected_remove_common, f\"High-freq removal (0.7) test failed. Got: {processed}\"\n",
    "\n",
    "print(\"Testing high-frequency removal (threshold=0.6)...\")\n",
    "processed = pre_process(test_corpus, remove_high_freq_terms=True, high_freq_threshold=0.6)\n",
    "# CORRECTED: The word 'a' should also be removed as its doc freq (0.667) > 0.6\n",
    "expected_remove_common_and_sentence = [\n",
    "    ['hello', 'world', 'this', 'is', 'test', 'num'],\n",
    "    ['another', 'with', 'unique', 'word'],\n",
    "    ['just']\n",
    "]\n",
    "assert processed == expected_remove_common_and_sentence, f\"High-freq removal (0.6) test failed. Got: {processed}\"\n",
    "\n",
    "# --- Test 6: Edge Cases ---\n",
    "print(\"Testing edge cases...\")\n",
    "assert pre_process([]) == [], \"Edge case: Empty list failed.\"\n",
    "assert pre_process([\"\"]) == [[]], \"Edge case: List with empty string failed.\"\n",
    "assert pre_process([\"!@#$\"]) == [[]], \"Edge case: Punctuation-only string (default) failed.\"\n",
    "assert pre_process([\"!@#$\"], tokenize_punct=True, remove_punct=False) == [['!', '@', '#', '$']], \"Edge case: Punctuation-only string (keep) failed.\"\n",
    "\n",
    "print(\"\\n✅ All tests for the recommended pre_process function passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755f7b4069b6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the corpus if you are facing performance issues\n",
    "tokenized_corpus = pre_process(train_texts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25715b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'documentary',\n",
       " 'on',\n",
       " 'schlockmeister',\n",
       " 'william',\n",
       " 'castle',\n",
       " 'takes',\n",
       " 'a',\n",
       " 'few',\n",
       " 'cheap',\n",
       " 'shots',\n",
       " 'at',\n",
       " 'the',\n",
       " 'naive',\n",
       " 'nums',\n",
       " 'nums',\n",
       " 'environment',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'did',\n",
       " 'his',\n",
       " 'most',\n",
       " 'characteristic',\n",
       " 'work',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'funny',\n",
       " 'silly',\n",
       " 'people',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ghost',\n",
       " 'glasses',\n",
       " 'but',\n",
       " 'it',\n",
       " 's',\n",
       " 'also',\n",
       " 'affectionate',\n",
       " 'and',\n",
       " 'lively',\n",
       " 'with',\n",
       " 'particularly',\n",
       " 'bright',\n",
       " 'commentary',\n",
       " 'from',\n",
       " 'john',\n",
       " 'waters',\n",
       " 'who',\n",
       " 'was',\n",
       " 'absolutely',\n",
       " 'the',\n",
       " 'target',\n",
       " 'audience',\n",
       " 'for',\n",
       " 'such',\n",
       " 'things',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'and',\n",
       " 'from',\n",
       " 'castle',\n",
       " 's',\n",
       " 'daughter',\n",
       " 'who',\n",
       " 'adored',\n",
       " 'her',\n",
       " 'dad',\n",
       " 'and',\n",
       " 'also',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'perceptive',\n",
       " 'about',\n",
       " 'how',\n",
       " 'he',\n",
       " 'plied',\n",
       " 'his',\n",
       " 'craft',\n",
       " 'we',\n",
       " 'never',\n",
       " 'find',\n",
       " 'out',\n",
       " 'what',\n",
       " 'became',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'castle',\n",
       " 'offspring',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'were',\n",
       " 'not',\n",
       " 'very',\n",
       " 'good',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'clear',\n",
       " 'but',\n",
       " 'his',\n",
       " 'marketing',\n",
       " 'of',\n",
       " 'them',\n",
       " 'was',\n",
       " 'brilliant',\n",
       " 'and',\n",
       " 'he',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'sweet',\n",
       " 'hardworking',\n",
       " 'family',\n",
       " 'man',\n",
       " 'fun',\n",
       " 'people',\n",
       " 'keep',\n",
       " 'popping',\n",
       " 'up',\n",
       " 'like',\n",
       " 'straight',\n",
       " 'jacket',\n",
       " 's',\n",
       " 'diane',\n",
       " 'baker',\n",
       " 'who',\n",
       " 'looks',\n",
       " 'great',\n",
       " 'and',\n",
       " 'anne',\n",
       " 'helm',\n",
       " 'whom',\n",
       " 'she',\n",
       " 'replaced',\n",
       " 'at',\n",
       " 'the',\n",
       " 'instigation',\n",
       " 'of',\n",
       " 'star',\n",
       " 'joan',\n",
       " 'crawford',\n",
       " 'darryl',\n",
       " 'hickman',\n",
       " 'all',\n",
       " 'but',\n",
       " 'explodes',\n",
       " 'into',\n",
       " 'giggles',\n",
       " 'at',\n",
       " 'the',\n",
       " 'happy',\n",
       " 'memory',\n",
       " 'of',\n",
       " 'working',\n",
       " 'with',\n",
       " 'castle',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tingler',\n",
       " 'and',\n",
       " 'there',\n",
       " 's',\n",
       " 'enough',\n",
       " 'footage',\n",
       " 'to',\n",
       " 'give',\n",
       " 'us',\n",
       " 'an',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'the',\n",
       " 'level',\n",
       " 'of',\n",
       " 'castle',\n",
       " 's',\n",
       " 'talent',\n",
       " 'not',\n",
       " 'very',\n",
       " 'high',\n",
       " 'but',\n",
       " 'very',\n",
       " 'energetic',\n",
       " 'a',\n",
       " 'pleasant',\n",
       " 'look',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'when',\n",
       " 'audiences',\n",
       " 'were',\n",
       " 'more',\n",
       " 'easily',\n",
       " 'pleased',\n",
       " 'and',\n",
       " 'it',\n",
       " 'does',\n",
       " 'make',\n",
       " 'you',\n",
       " 'nostalgic',\n",
       " 'for',\n",
       " 'simpler',\n",
       " 'movie',\n",
       " 'going',\n",
       " 'days']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc156b148c732d4",
   "metadata": {},
   "source": [
    "## Task 1: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afc8ca6d284bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 24877\n",
      "Context Size 2\n",
      "Embedding Dimension 100\n"
     ]
    }
   ],
   "source": [
    "# Parameters (change these as wanted)\n",
    "CONTEXT_SIZE = 2  # Window size on each side\n",
    "EMBEDDING_DIM = 100\n",
    "PAD_TOKEN = '<PAD>'\n",
    "\n",
    "# Vocabulary\n",
    "vocab = list(set(word for sentence in tokenized_corpus for word in sentence))\n",
    "\n",
    "if PAD_TOKEN not in vocab:\n",
    "    vocab.append(PAD_TOKEN)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "print('Vocab Size', vocab_size)\n",
    "print('Context Size', CONTEXT_SIZE)\n",
    "print('Embedding Dimension', EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0d180b5bc84ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('titles', 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word = vocab[0]\n",
    "\n",
    "assert all(idx_to_word[i] == word and word_to_idx[word] == i for i, word in enumerate(vocab))\n",
    "\n",
    "idx_to_word[0], word_to_idx[first_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2caf88602aabaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...', 'gilbert', 'yurek', '<PAD>']\n",
      "PAD_TOKEN index: 24876\n"
     ]
    }
   ],
   "source": [
    "# Assert that PAD_TOKEN is only present once\n",
    "assert len([x for x in vocab if x == PAD_TOKEN]) == 1\n",
    "\n",
    "print([\"...\"] + vocab[-3:])\n",
    "pad_token_idx = word_to_idx[PAD_TOKEN]\n",
    "print(f\"PAD_TOKEN index: {pad_token_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d513c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "cbow_data = []\n",
    "for sentence in tokenized_corpus:\n",
    "    # Skip very short sentences that don't have enough words for a context and a target\n",
    "    if len(sentence) < 2 * CONTEXT_SIZE + 1:\n",
    "        continue\n",
    "    \n",
    "    # Pad the sentence to handle words at the beginning and end\n",
    "    padded_sentence = [PAD_TOKEN] * CONTEXT_SIZE + sentence + [PAD_TOKEN] * CONTEXT_SIZE\n",
    "    \n",
    "    # Slide a window across the padded sentence\n",
    "    for i in range(CONTEXT_SIZE, len(padded_sentence) - CONTEXT_SIZE):\n",
    "        # The word at the center of the window is the target\n",
    "        target_word = padded_sentence[i]\n",
    "        \n",
    "        # The words around the center are the context\n",
    "        # Get the words to the left\n",
    "        context_left = padded_sentence[i - CONTEXT_SIZE : i]\n",
    "        # Get the words to the right\n",
    "        context_right = padded_sentence[i + 1 : i + CONTEXT_SIZE + 1]\n",
    "        \n",
    "        context_words = context_left + context_right\n",
    "        \n",
    "        # Append the (context, target) pair to our data\n",
    "        cbow_data.append((context_words, target_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d2347804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 466988 context-target pairs.\n",
      "First 5 training samples:\n",
      "  Context: ['<PAD>', '<PAD>', 'is', 'one'], Target: 'this'\n",
      "  Context: ['<PAD>', 'this', 'one', 'of'], Target: 'is'\n",
      "  Context: ['this', 'is', 'of', 'those'], Target: 'one'\n",
      "  Context: ['is', 'one', 'those', 'movies'], Target: 'of'\n",
      "  Context: ['one', 'of', 'movies', 'that'], Target: 'those'\n",
      "\n",
      "Last 5 training samples:\n",
      "  Context: ['ever', 'to', 'in', 'to'], Target: 'give'\n",
      "  Context: ['to', 'give', 'to', 'being'], Target: 'in'\n",
      "  Context: ['give', 'in', 'being', 'dead'], Target: 'to'\n",
      "  Context: ['in', 'to', 'dead', '<PAD>'], Target: 'being'\n",
      "  Context: ['to', 'being', '<PAD>', '<PAD>'], Target: 'dead'\n"
     ]
    }
   ],
   "source": [
    "# Let's see what our data looks like\n",
    "print(f\"Created {len(cbow_data)} context-target pairs.\")\n",
    "print(\"First 5 training samples:\")\n",
    "for context, target in cbow_data[:5]:\n",
    "    print(f\"  Context: {context}, Target: '{target}'\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Last 5 training samples:\")\n",
    "for context, target in cbow_data[-5:]:\n",
    "    print(f\"  Context: {context}, Target: '{target}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc701f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string-based data into integer-based tensors\n",
    "cbow_train_data_tensors = []\n",
    "for context, target in cbow_data:\n",
    "    context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
    "    target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "    cbow_train_data_tensors.append((context_idxs, target_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "804595ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample as tensors:\n",
      "  Context Tensor: tensor([24876, 24876,   855, 20975]) (Shape: torch.Size([4]))\n",
      "  Context Tensor: ['<PAD>', '<PAD>', 'is', 'one']\n",
      "  Target Tensor: tensor([6361]) (Shape: torch.Size([1]))\n",
      "  Target Tensor: this\n"
     ]
    }
   ],
   "source": [
    "# Check the first sample as a tensor\n",
    "print(\"First training sample as tensors:\")\n",
    "context_tensor, target_tensor = cbow_train_data_tensors[0]\n",
    "print(f\"  Context Tensor: {context_tensor} (Shape: {context_tensor.shape})\")\n",
    "print(f\"  Context Tensor: {[idx_to_word[i.item()] for i in context_tensor]}\")\n",
    "print(f\"  Target Tensor: {target_tensor} (Shape: {target_tensor.shape})\")\n",
    "print(f\"  Target Tensor: {idx_to_word[target_tensor[0].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4c1b87710020fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, context_idxs):\n",
    "        embeddings = self.embedding(context_idxs)\n",
    "        summed_embeddings = torch.sum(embeddings, dim=1)\n",
    "        predictions = self.linear(summed_embeddings)\n",
    "        # should we add a Relu ? \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3debed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for CBOW training data.\"\"\"\n",
    "    def __init__(self, data_tensors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_tensors (list of tuples): A list where each element is a \n",
    "                                           tuple (context_tensor, target_tensor).\n",
    "        \"\"\"\n",
    "        self.data = data_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "        \"\"\"\n",
    "        # The DataLoader will call this method to get each sample.\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6a27a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Batch ---\n",
      "Context Batch Shape: torch.Size([128, 4]) (['of', 'goofy', 'scenes', 'sounds'])\n",
      "Target Batch Shape: torch.Size([128, 1]) (death)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 \n",
    "\n",
    "cbow_dataset = CBOWDataset(cbow_train_data_tensors)\n",
    "\n",
    "cbow_dataloader = DataLoader(\n",
    "    dataset=cbow_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "context_batch, target_batch = next(iter(cbow_dataloader))\n",
    "\n",
    "print(\"--- Sample Batch ---\")\n",
    "print(f\"Context Batch Shape: {context_batch.shape} ({[idx_to_word[i.item()] for i in context_batch[0]]})\")\n",
    "print(f\"Target Batch Shape: {target_batch.shape} ({idx_to_word[target_batch[0].item()]})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b1323b4691c8e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 50/3649 | Batch Loss: 9.7170 | Batch Acc: 0.0078\n",
      "Epoch 1 | Batch 100/3649 | Batch Loss: 8.8499 | Batch Acc: 0.0312\n",
      "Epoch 1 | Batch 150/3649 | Batch Loss: 8.3469 | Batch Acc: 0.0781\n",
      "Epoch 1 | Batch 200/3649 | Batch Loss: 8.1420 | Batch Acc: 0.0469\n",
      "Epoch 1 | Batch 250/3649 | Batch Loss: 8.6080 | Batch Acc: 0.0156\n",
      "Epoch 1 | Batch 300/3649 | Batch Loss: 7.5379 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 350/3649 | Batch Loss: 8.0225 | Batch Acc: 0.0391\n",
      "Epoch 1 | Batch 400/3649 | Batch Loss: 7.7044 | Batch Acc: 0.0312\n",
      "Epoch 1 | Batch 450/3649 | Batch Loss: 7.2661 | Batch Acc: 0.0781\n",
      "Epoch 1 | Batch 500/3649 | Batch Loss: 7.5384 | Batch Acc: 0.0781\n",
      "Epoch 1 | Batch 550/3649 | Batch Loss: 7.9299 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 600/3649 | Batch Loss: 7.1670 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 650/3649 | Batch Loss: 7.0220 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 700/3649 | Batch Loss: 7.3752 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 750/3649 | Batch Loss: 6.6843 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 800/3649 | Batch Loss: 7.3813 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 850/3649 | Batch Loss: 6.9112 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 900/3649 | Batch Loss: 7.2623 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 950/3649 | Batch Loss: 6.8079 | Batch Acc: 0.0703\n",
      "Epoch 1 | Batch 1000/3649 | Batch Loss: 6.6790 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 1050/3649 | Batch Loss: 7.0935 | Batch Acc: 0.1172\n",
      "Epoch 1 | Batch 1100/3649 | Batch Loss: 6.8541 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 1150/3649 | Batch Loss: 6.7111 | Batch Acc: 0.1406\n",
      "Epoch 1 | Batch 1200/3649 | Batch Loss: 7.1445 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 1250/3649 | Batch Loss: 7.0206 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 1300/3649 | Batch Loss: 7.2861 | Batch Acc: 0.1172\n",
      "Epoch 1 | Batch 1350/3649 | Batch Loss: 7.3500 | Batch Acc: 0.0312\n",
      "Epoch 1 | Batch 1400/3649 | Batch Loss: 6.7315 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 1450/3649 | Batch Loss: 7.1017 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 1500/3649 | Batch Loss: 7.0958 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 1550/3649 | Batch Loss: 6.6393 | Batch Acc: 0.1328\n",
      "Epoch 1 | Batch 1600/3649 | Batch Loss: 7.0298 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 1650/3649 | Batch Loss: 7.0716 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 1700/3649 | Batch Loss: 6.4144 | Batch Acc: 0.1562\n",
      "Epoch 1 | Batch 1750/3649 | Batch Loss: 6.3916 | Batch Acc: 0.0703\n",
      "Epoch 1 | Batch 1800/3649 | Batch Loss: 6.6081 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 1850/3649 | Batch Loss: 6.9296 | Batch Acc: 0.1172\n",
      "Epoch 1 | Batch 1900/3649 | Batch Loss: 6.5302 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 1950/3649 | Batch Loss: 6.6634 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 2000/3649 | Batch Loss: 6.0741 | Batch Acc: 0.2031\n",
      "Epoch 1 | Batch 2050/3649 | Batch Loss: 6.7604 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 2100/3649 | Batch Loss: 6.4036 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 2150/3649 | Batch Loss: 6.6615 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 2200/3649 | Batch Loss: 6.6179 | Batch Acc: 0.0781\n",
      "Epoch 1 | Batch 2250/3649 | Batch Loss: 6.6931 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 2300/3649 | Batch Loss: 6.7067 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 2350/3649 | Batch Loss: 6.6825 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 2400/3649 | Batch Loss: 6.5541 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 2450/3649 | Batch Loss: 6.4557 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 2500/3649 | Batch Loss: 6.6634 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 2550/3649 | Batch Loss: 6.2929 | Batch Acc: 0.1328\n",
      "Epoch 1 | Batch 2600/3649 | Batch Loss: 6.2212 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 2650/3649 | Batch Loss: 6.6344 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 2700/3649 | Batch Loss: 6.9427 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 2750/3649 | Batch Loss: 6.4990 | Batch Acc: 0.1484\n",
      "Epoch 1 | Batch 2800/3649 | Batch Loss: 6.3107 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 2850/3649 | Batch Loss: 7.0750 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 2900/3649 | Batch Loss: 6.7364 | Batch Acc: 0.0859\n",
      "Epoch 1 | Batch 2950/3649 | Batch Loss: 6.9466 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 3000/3649 | Batch Loss: 6.4271 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 3050/3649 | Batch Loss: 6.6308 | Batch Acc: 0.1328\n",
      "Epoch 1 | Batch 3100/3649 | Batch Loss: 6.7778 | Batch Acc: 0.1094\n",
      "Epoch 1 | Batch 3150/3649 | Batch Loss: 6.1795 | Batch Acc: 0.1406\n",
      "Epoch 1 | Batch 3200/3649 | Batch Loss: 6.7312 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 3250/3649 | Batch Loss: 6.8838 | Batch Acc: 0.0938\n",
      "Epoch 1 | Batch 3300/3649 | Batch Loss: 6.5430 | Batch Acc: 0.1016\n",
      "Epoch 1 | Batch 3350/3649 | Batch Loss: 7.3534 | Batch Acc: 0.0781\n",
      "Epoch 1 | Batch 3400/3649 | Batch Loss: 5.9011 | Batch Acc: 0.1562\n",
      "Epoch 1 | Batch 3450/3649 | Batch Loss: 6.5938 | Batch Acc: 0.0547\n",
      "Epoch 1 | Batch 3500/3649 | Batch Loss: 6.5321 | Batch Acc: 0.1172\n",
      "Epoch 1 | Batch 3550/3649 | Batch Loss: 7.0243 | Batch Acc: 0.0625\n",
      "Epoch 1 | Batch 3600/3649 | Batch Loss: 6.5744 | Batch Acc: 0.1250\n",
      "Epoch 1 | Batch 3649/3649 | Batch Loss: 6.2551 | Batch Acc: 0.1591\n",
      "Epoch 1 completed in 55.03s | Average Loss: 6.9775 | Accuracy: 0.0901\n",
      "  Min batch loss: 5.4431 | Max batch loss: 10.9211\n",
      "\n",
      "Epoch 2 | Batch 50/3649 | Batch Loss: 5.5215 | Batch Acc: 0.1641\n",
      "Epoch 2 | Batch 100/3649 | Batch Loss: 5.6003 | Batch Acc: 0.1719\n",
      "Epoch 2 | Batch 150/3649 | Batch Loss: 6.1379 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 200/3649 | Batch Loss: 6.1158 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 250/3649 | Batch Loss: 5.6377 | Batch Acc: 0.1250\n",
      "Epoch 2 | Batch 300/3649 | Batch Loss: 6.1886 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 350/3649 | Batch Loss: 5.7686 | Batch Acc: 0.1641\n",
      "Epoch 2 | Batch 400/3649 | Batch Loss: 5.6198 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 450/3649 | Batch Loss: 5.8497 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 500/3649 | Batch Loss: 5.3587 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 550/3649 | Batch Loss: 5.5474 | Batch Acc: 0.1875\n",
      "Epoch 2 | Batch 600/3649 | Batch Loss: 6.0669 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 650/3649 | Batch Loss: 5.7127 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 700/3649 | Batch Loss: 5.6634 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 750/3649 | Batch Loss: 5.5742 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 800/3649 | Batch Loss: 5.6960 | Batch Acc: 0.0859\n",
      "Epoch 2 | Batch 850/3649 | Batch Loss: 5.2516 | Batch Acc: 0.1406\n",
      "Epoch 2 | Batch 900/3649 | Batch Loss: 6.1555 | Batch Acc: 0.0859\n",
      "Epoch 2 | Batch 950/3649 | Batch Loss: 5.7887 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 1000/3649 | Batch Loss: 5.9688 | Batch Acc: 0.1641\n",
      "Epoch 2 | Batch 1050/3649 | Batch Loss: 5.7498 | Batch Acc: 0.1719\n",
      "Epoch 2 | Batch 1100/3649 | Batch Loss: 5.9815 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 1150/3649 | Batch Loss: 6.0544 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 1200/3649 | Batch Loss: 6.0601 | Batch Acc: 0.0859\n",
      "Epoch 2 | Batch 1250/3649 | Batch Loss: 5.8260 | Batch Acc: 0.0781\n",
      "Epoch 2 | Batch 1300/3649 | Batch Loss: 6.0351 | Batch Acc: 0.0781\n",
      "Epoch 2 | Batch 1350/3649 | Batch Loss: 5.9621 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 1400/3649 | Batch Loss: 5.8257 | Batch Acc: 0.1250\n",
      "Epoch 2 | Batch 1450/3649 | Batch Loss: 6.0996 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 1500/3649 | Batch Loss: 6.0340 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 1550/3649 | Batch Loss: 5.8706 | Batch Acc: 0.1406\n",
      "Epoch 2 | Batch 1600/3649 | Batch Loss: 5.6227 | Batch Acc: 0.0859\n",
      "Epoch 2 | Batch 1650/3649 | Batch Loss: 6.2001 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 1700/3649 | Batch Loss: 5.9970 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 1750/3649 | Batch Loss: 6.0821 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 1800/3649 | Batch Loss: 5.8537 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 1850/3649 | Batch Loss: 5.9445 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 1900/3649 | Batch Loss: 5.8774 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 1950/3649 | Batch Loss: 5.8116 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 2000/3649 | Batch Loss: 5.9212 | Batch Acc: 0.1250\n",
      "Epoch 2 | Batch 2050/3649 | Batch Loss: 5.9051 | Batch Acc: 0.1484\n",
      "Epoch 2 | Batch 2100/3649 | Batch Loss: 6.1871 | Batch Acc: 0.1406\n",
      "Epoch 2 | Batch 2150/3649 | Batch Loss: 5.8425 | Batch Acc: 0.1406\n",
      "Epoch 2 | Batch 2200/3649 | Batch Loss: 5.6950 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 2250/3649 | Batch Loss: 5.7412 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 2300/3649 | Batch Loss: 6.5306 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 2350/3649 | Batch Loss: 5.9953 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 2400/3649 | Batch Loss: 6.1285 | Batch Acc: 0.1484\n",
      "Epoch 2 | Batch 2450/3649 | Batch Loss: 5.6601 | Batch Acc: 0.1250\n",
      "Epoch 2 | Batch 2500/3649 | Batch Loss: 5.7711 | Batch Acc: 0.1562\n",
      "Epoch 2 | Batch 2550/3649 | Batch Loss: 6.2905 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 2600/3649 | Batch Loss: 5.9186 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 2650/3649 | Batch Loss: 6.0421 | Batch Acc: 0.1484\n",
      "Epoch 2 | Batch 2700/3649 | Batch Loss: 5.8754 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 2750/3649 | Batch Loss: 6.0460 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 2800/3649 | Batch Loss: 6.0215 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 2850/3649 | Batch Loss: 6.1625 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 2900/3649 | Batch Loss: 5.9035 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 2950/3649 | Batch Loss: 5.8250 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 3000/3649 | Batch Loss: 6.0211 | Batch Acc: 0.0625\n",
      "Epoch 2 | Batch 3050/3649 | Batch Loss: 5.8990 | Batch Acc: 0.0938\n",
      "Epoch 2 | Batch 3100/3649 | Batch Loss: 6.0180 | Batch Acc: 0.1016\n",
      "Epoch 2 | Batch 3150/3649 | Batch Loss: 5.7787 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 3200/3649 | Batch Loss: 5.5341 | Batch Acc: 0.1719\n",
      "Epoch 2 | Batch 3250/3649 | Batch Loss: 5.4336 | Batch Acc: 0.2031\n",
      "Epoch 2 | Batch 3300/3649 | Batch Loss: 6.0169 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 3350/3649 | Batch Loss: 6.0344 | Batch Acc: 0.1484\n",
      "Epoch 2 | Batch 3400/3649 | Batch Loss: 5.6932 | Batch Acc: 0.1641\n",
      "Epoch 2 | Batch 3450/3649 | Batch Loss: 6.1646 | Batch Acc: 0.1172\n",
      "Epoch 2 | Batch 3500/3649 | Batch Loss: 6.0605 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 3550/3649 | Batch Loss: 6.6032 | Batch Acc: 0.1094\n",
      "Epoch 2 | Batch 3600/3649 | Batch Loss: 6.0504 | Batch Acc: 0.1328\n",
      "Epoch 2 | Batch 3649/3649 | Batch Loss: 5.4430 | Batch Acc: 0.1818\n",
      "Epoch 2 completed in 54.17s | Average Loss: 5.8351 | Accuracy: 0.1246\n",
      "  Min batch loss: 4.9075 | Max batch loss: 6.6731\n",
      "\n",
      "Epoch 3 | Batch 50/3649 | Batch Loss: 5.1114 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 100/3649 | Batch Loss: 5.1260 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 150/3649 | Batch Loss: 5.4798 | Batch Acc: 0.1016\n",
      "Epoch 3 | Batch 200/3649 | Batch Loss: 5.2994 | Batch Acc: 0.1250\n",
      "Epoch 3 | Batch 250/3649 | Batch Loss: 5.2712 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 300/3649 | Batch Loss: 4.9945 | Batch Acc: 0.1953\n",
      "Epoch 3 | Batch 350/3649 | Batch Loss: 5.2445 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 400/3649 | Batch Loss: 5.4100 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 450/3649 | Batch Loss: 4.8922 | Batch Acc: 0.2031\n",
      "Epoch 3 | Batch 500/3649 | Batch Loss: 5.3516 | Batch Acc: 0.1250\n",
      "Epoch 3 | Batch 550/3649 | Batch Loss: 5.4417 | Batch Acc: 0.0938\n",
      "Epoch 3 | Batch 600/3649 | Batch Loss: 5.4295 | Batch Acc: 0.1172\n",
      "Epoch 3 | Batch 650/3649 | Batch Loss: 5.1301 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 700/3649 | Batch Loss: 5.5160 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 750/3649 | Batch Loss: 5.8029 | Batch Acc: 0.1172\n",
      "Epoch 3 | Batch 800/3649 | Batch Loss: 5.1460 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 850/3649 | Batch Loss: 5.2891 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 900/3649 | Batch Loss: 5.3698 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 950/3649 | Batch Loss: 5.2523 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 1000/3649 | Batch Loss: 5.4871 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 1050/3649 | Batch Loss: 5.3910 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 1100/3649 | Batch Loss: 4.8754 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 1150/3649 | Batch Loss: 5.0212 | Batch Acc: 0.1875\n",
      "Epoch 3 | Batch 1200/3649 | Batch Loss: 4.9971 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 1250/3649 | Batch Loss: 5.4747 | Batch Acc: 0.0938\n",
      "Epoch 3 | Batch 1300/3649 | Batch Loss: 5.3586 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 1350/3649 | Batch Loss: 5.3216 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 1400/3649 | Batch Loss: 5.0860 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 1450/3649 | Batch Loss: 5.3429 | Batch Acc: 0.1797\n",
      "Epoch 3 | Batch 1500/3649 | Batch Loss: 5.2934 | Batch Acc: 0.1172\n",
      "Epoch 3 | Batch 1550/3649 | Batch Loss: 4.9862 | Batch Acc: 0.1875\n",
      "Epoch 3 | Batch 1600/3649 | Batch Loss: 5.4961 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 1650/3649 | Batch Loss: 5.2995 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 1700/3649 | Batch Loss: 5.2803 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 1750/3649 | Batch Loss: 5.5642 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 1800/3649 | Batch Loss: 5.2829 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 1850/3649 | Batch Loss: 5.4972 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 1900/3649 | Batch Loss: 5.0734 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 1950/3649 | Batch Loss: 5.8201 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 2000/3649 | Batch Loss: 5.5207 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 2050/3649 | Batch Loss: 5.6337 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 2100/3649 | Batch Loss: 5.3337 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 2150/3649 | Batch Loss: 5.4483 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 2200/3649 | Batch Loss: 5.1261 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 2250/3649 | Batch Loss: 5.2481 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 2300/3649 | Batch Loss: 5.6023 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 2350/3649 | Batch Loss: 4.9569 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 2400/3649 | Batch Loss: 5.4573 | Batch Acc: 0.1172\n",
      "Epoch 3 | Batch 2450/3649 | Batch Loss: 5.4856 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 2500/3649 | Batch Loss: 5.7173 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 2550/3649 | Batch Loss: 5.3646 | Batch Acc: 0.1562\n",
      "Epoch 3 | Batch 2600/3649 | Batch Loss: 5.1504 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 2650/3649 | Batch Loss: 5.3787 | Batch Acc: 0.1328\n",
      "Epoch 3 | Batch 2700/3649 | Batch Loss: 5.5394 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 2750/3649 | Batch Loss: 5.5740 | Batch Acc: 0.0781\n",
      "Epoch 3 | Batch 2800/3649 | Batch Loss: 5.4131 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 2850/3649 | Batch Loss: 5.3592 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 2900/3649 | Batch Loss: 5.5009 | Batch Acc: 0.1719\n",
      "Epoch 3 | Batch 2950/3649 | Batch Loss: 5.9448 | Batch Acc: 0.0625\n",
      "Epoch 3 | Batch 3000/3649 | Batch Loss: 5.6089 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 3050/3649 | Batch Loss: 5.2184 | Batch Acc: 0.1797\n",
      "Epoch 3 | Batch 3100/3649 | Batch Loss: 5.5309 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 3150/3649 | Batch Loss: 5.4532 | Batch Acc: 0.1094\n",
      "Epoch 3 | Batch 3200/3649 | Batch Loss: 5.4392 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 3250/3649 | Batch Loss: 5.4322 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 3300/3649 | Batch Loss: 5.3919 | Batch Acc: 0.1250\n",
      "Epoch 3 | Batch 3350/3649 | Batch Loss: 5.8342 | Batch Acc: 0.1484\n",
      "Epoch 3 | Batch 3400/3649 | Batch Loss: 4.9514 | Batch Acc: 0.2188\n",
      "Epoch 3 | Batch 3450/3649 | Batch Loss: 5.7694 | Batch Acc: 0.1797\n",
      "Epoch 3 | Batch 3500/3649 | Batch Loss: 5.2894 | Batch Acc: 0.1641\n",
      "Epoch 3 | Batch 3550/3649 | Batch Loss: 5.5278 | Batch Acc: 0.1406\n",
      "Epoch 3 | Batch 3600/3649 | Batch Loss: 5.5327 | Batch Acc: 0.1250\n",
      "Epoch 3 | Batch 3649/3649 | Batch Loss: 5.7993 | Batch Acc: 0.1591\n",
      "Epoch 3 completed in 62.30s | Average Loss: 5.3787 | Accuracy: 0.1432\n",
      "  Min batch loss: 4.5016 | Max batch loss: 6.3448\n",
      "\n",
      "Epoch 4 | Batch 50/3649 | Batch Loss: 4.7207 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 100/3649 | Batch Loss: 4.9226 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 150/3649 | Batch Loss: 4.4823 | Batch Acc: 0.2344\n",
      "Epoch 4 | Batch 200/3649 | Batch Loss: 4.5525 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 250/3649 | Batch Loss: 4.9963 | Batch Acc: 0.1719\n",
      "Epoch 4 | Batch 300/3649 | Batch Loss: 4.7990 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 350/3649 | Batch Loss: 4.9731 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 400/3649 | Batch Loss: 4.7253 | Batch Acc: 0.1719\n",
      "Epoch 4 | Batch 450/3649 | Batch Loss: 5.0860 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 500/3649 | Batch Loss: 4.8326 | Batch Acc: 0.2031\n",
      "Epoch 4 | Batch 550/3649 | Batch Loss: 5.0465 | Batch Acc: 0.1328\n",
      "Epoch 4 | Batch 600/3649 | Batch Loss: 4.8676 | Batch Acc: 0.1719\n",
      "Epoch 4 | Batch 650/3649 | Batch Loss: 4.7442 | Batch Acc: 0.2109\n",
      "Epoch 4 | Batch 700/3649 | Batch Loss: 5.0265 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 750/3649 | Batch Loss: 5.0743 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 800/3649 | Batch Loss: 4.8987 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 850/3649 | Batch Loss: 5.2119 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 900/3649 | Batch Loss: 4.5458 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 950/3649 | Batch Loss: 5.4632 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 1000/3649 | Batch Loss: 4.7099 | Batch Acc: 0.2266\n",
      "Epoch 4 | Batch 1050/3649 | Batch Loss: 5.0039 | Batch Acc: 0.2031\n",
      "Epoch 4 | Batch 1100/3649 | Batch Loss: 5.1251 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 1150/3649 | Batch Loss: 5.1731 | Batch Acc: 0.1719\n",
      "Epoch 4 | Batch 1200/3649 | Batch Loss: 4.8643 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 1250/3649 | Batch Loss: 5.3242 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 1300/3649 | Batch Loss: 5.2930 | Batch Acc: 0.0781\n",
      "Epoch 4 | Batch 1350/3649 | Batch Loss: 5.1850 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 1400/3649 | Batch Loss: 5.1143 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 1450/3649 | Batch Loss: 5.2521 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 1500/3649 | Batch Loss: 4.9306 | Batch Acc: 0.2031\n",
      "Epoch 4 | Batch 1550/3649 | Batch Loss: 5.1713 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 1600/3649 | Batch Loss: 4.6929 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 1650/3649 | Batch Loss: 5.0382 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 1700/3649 | Batch Loss: 5.1093 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 1750/3649 | Batch Loss: 4.9700 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 1800/3649 | Batch Loss: 5.1179 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 1850/3649 | Batch Loss: 5.3437 | Batch Acc: 0.1562\n",
      "Epoch 4 | Batch 1900/3649 | Batch Loss: 4.9436 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 1950/3649 | Batch Loss: 4.8252 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 2000/3649 | Batch Loss: 4.7239 | Batch Acc: 0.2188\n",
      "Epoch 4 | Batch 2050/3649 | Batch Loss: 5.0242 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 2100/3649 | Batch Loss: 5.4237 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 2150/3649 | Batch Loss: 4.8520 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 2200/3649 | Batch Loss: 5.2262 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 2250/3649 | Batch Loss: 4.7110 | Batch Acc: 0.1875\n",
      "Epoch 4 | Batch 2300/3649 | Batch Loss: 5.1220 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 2350/3649 | Batch Loss: 5.2389 | Batch Acc: 0.1172\n",
      "Epoch 4 | Batch 2400/3649 | Batch Loss: 5.3237 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 2450/3649 | Batch Loss: 4.9700 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 2500/3649 | Batch Loss: 5.6172 | Batch Acc: 0.1250\n",
      "Epoch 4 | Batch 2550/3649 | Batch Loss: 5.4136 | Batch Acc: 0.1172\n",
      "Epoch 4 | Batch 2600/3649 | Batch Loss: 5.2147 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 2650/3649 | Batch Loss: 5.0065 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 2700/3649 | Batch Loss: 5.5415 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 2750/3649 | Batch Loss: 5.0818 | Batch Acc: 0.1328\n",
      "Epoch 4 | Batch 2800/3649 | Batch Loss: 4.8343 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 2850/3649 | Batch Loss: 4.8722 | Batch Acc: 0.2422\n",
      "Epoch 4 | Batch 2900/3649 | Batch Loss: 5.2102 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 2950/3649 | Batch Loss: 5.5892 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 3000/3649 | Batch Loss: 5.3659 | Batch Acc: 0.2266\n",
      "Epoch 4 | Batch 3050/3649 | Batch Loss: 5.0933 | Batch Acc: 0.1797\n",
      "Epoch 4 | Batch 3100/3649 | Batch Loss: 5.1643 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 3150/3649 | Batch Loss: 5.4828 | Batch Acc: 0.1172\n",
      "Epoch 4 | Batch 3200/3649 | Batch Loss: 5.3164 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 3250/3649 | Batch Loss: 5.3818 | Batch Acc: 0.1641\n",
      "Epoch 4 | Batch 3300/3649 | Batch Loss: 5.1615 | Batch Acc: 0.1406\n",
      "Epoch 4 | Batch 3350/3649 | Batch Loss: 5.4069 | Batch Acc: 0.1719\n",
      "Epoch 4 | Batch 3400/3649 | Batch Loss: 5.4049 | Batch Acc: 0.1484\n",
      "Epoch 4 | Batch 3450/3649 | Batch Loss: 5.8058 | Batch Acc: 0.1250\n",
      "Epoch 4 | Batch 3500/3649 | Batch Loss: 4.8616 | Batch Acc: 0.1953\n",
      "Epoch 4 | Batch 3550/3649 | Batch Loss: 5.2488 | Batch Acc: 0.1172\n",
      "Epoch 4 | Batch 3600/3649 | Batch Loss: 5.4643 | Batch Acc: 0.1328\n",
      "Epoch 4 | Batch 3649/3649 | Batch Loss: 5.3610 | Batch Acc: 0.1136\n",
      "Epoch 4 completed in 64.50s | Average Loss: 5.0652 | Accuracy: 0.1624\n",
      "  Min batch loss: 4.2513 | Max batch loss: 5.9854\n",
      "\n",
      "Epoch 5 | Batch 50/3649 | Batch Loss: 4.6855 | Batch Acc: 0.1953\n",
      "Epoch 5 | Batch 100/3649 | Batch Loss: 5.0333 | Batch Acc: 0.1562\n",
      "Epoch 5 | Batch 150/3649 | Batch Loss: 4.1815 | Batch Acc: 0.2578\n",
      "Epoch 5 | Batch 200/3649 | Batch Loss: 4.6268 | Batch Acc: 0.2109\n",
      "Epoch 5 | Batch 250/3649 | Batch Loss: 4.7442 | Batch Acc: 0.1719\n",
      "Epoch 5 | Batch 300/3649 | Batch Loss: 4.5959 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 350/3649 | Batch Loss: 4.5530 | Batch Acc: 0.1719\n",
      "Epoch 5 | Batch 400/3649 | Batch Loss: 4.7898 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 450/3649 | Batch Loss: 4.5372 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 500/3649 | Batch Loss: 4.7832 | Batch Acc: 0.1953\n",
      "Epoch 5 | Batch 550/3649 | Batch Loss: 4.7821 | Batch Acc: 0.1484\n",
      "Epoch 5 | Batch 600/3649 | Batch Loss: 4.4237 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 650/3649 | Batch Loss: 4.5148 | Batch Acc: 0.2812\n",
      "Epoch 5 | Batch 700/3649 | Batch Loss: 5.0509 | Batch Acc: 0.1250\n",
      "Epoch 5 | Batch 750/3649 | Batch Loss: 4.5983 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 800/3649 | Batch Loss: 5.0031 | Batch Acc: 0.1328\n",
      "Epoch 5 | Batch 850/3649 | Batch Loss: 4.6003 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 900/3649 | Batch Loss: 4.5719 | Batch Acc: 0.1875\n",
      "Epoch 5 | Batch 950/3649 | Batch Loss: 4.7739 | Batch Acc: 0.1953\n",
      "Epoch 5 | Batch 1000/3649 | Batch Loss: 4.3362 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 1050/3649 | Batch Loss: 4.8429 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 1100/3649 | Batch Loss: 4.4136 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 1150/3649 | Batch Loss: 5.2140 | Batch Acc: 0.1719\n",
      "Epoch 5 | Batch 1200/3649 | Batch Loss: 4.6465 | Batch Acc: 0.2344\n",
      "Epoch 5 | Batch 1250/3649 | Batch Loss: 5.3350 | Batch Acc: 0.1250\n",
      "Epoch 5 | Batch 1300/3649 | Batch Loss: 4.9759 | Batch Acc: 0.1328\n",
      "Epoch 5 | Batch 1350/3649 | Batch Loss: 5.0419 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 1400/3649 | Batch Loss: 5.0306 | Batch Acc: 0.1172\n",
      "Epoch 5 | Batch 1450/3649 | Batch Loss: 4.8487 | Batch Acc: 0.1484\n",
      "Epoch 5 | Batch 1500/3649 | Batch Loss: 5.0424 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 1550/3649 | Batch Loss: 4.7065 | Batch Acc: 0.1875\n",
      "Epoch 5 | Batch 1600/3649 | Batch Loss: 4.9916 | Batch Acc: 0.1484\n",
      "Epoch 5 | Batch 1650/3649 | Batch Loss: 5.2059 | Batch Acc: 0.1562\n",
      "Epoch 5 | Batch 1700/3649 | Batch Loss: 4.9545 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 1750/3649 | Batch Loss: 5.0818 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 1800/3649 | Batch Loss: 5.0812 | Batch Acc: 0.1328\n",
      "Epoch 5 | Batch 1850/3649 | Batch Loss: 5.2066 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 1900/3649 | Batch Loss: 4.6299 | Batch Acc: 0.1562\n",
      "Epoch 5 | Batch 1950/3649 | Batch Loss: 5.2385 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 2000/3649 | Batch Loss: 4.4674 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 2050/3649 | Batch Loss: 5.2018 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 2100/3649 | Batch Loss: 4.7921 | Batch Acc: 0.1406\n",
      "Epoch 5 | Batch 2150/3649 | Batch Loss: 5.1968 | Batch Acc: 0.1484\n",
      "Epoch 5 | Batch 2200/3649 | Batch Loss: 4.9836 | Batch Acc: 0.1562\n",
      "Epoch 5 | Batch 2250/3649 | Batch Loss: 4.8679 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 2300/3649 | Batch Loss: 5.0561 | Batch Acc: 0.1328\n",
      "Epoch 5 | Batch 2350/3649 | Batch Loss: 4.7973 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 2400/3649 | Batch Loss: 5.4126 | Batch Acc: 0.1406\n",
      "Epoch 5 | Batch 2450/3649 | Batch Loss: 4.7211 | Batch Acc: 0.1875\n",
      "Epoch 5 | Batch 2500/3649 | Batch Loss: 4.7905 | Batch Acc: 0.1875\n",
      "Epoch 5 | Batch 2550/3649 | Batch Loss: 4.8430 | Batch Acc: 0.1484\n",
      "Epoch 5 | Batch 2600/3649 | Batch Loss: 5.0298 | Batch Acc: 0.1719\n",
      "Epoch 5 | Batch 2650/3649 | Batch Loss: 4.6337 | Batch Acc: 0.2344\n",
      "Epoch 5 | Batch 2700/3649 | Batch Loss: 5.1168 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 2750/3649 | Batch Loss: 5.0224 | Batch Acc: 0.1406\n",
      "Epoch 5 | Batch 2800/3649 | Batch Loss: 5.0889 | Batch Acc: 0.1250\n",
      "Epoch 5 | Batch 2850/3649 | Batch Loss: 5.0434 | Batch Acc: 0.1875\n",
      "Epoch 5 | Batch 2900/3649 | Batch Loss: 4.5540 | Batch Acc: 0.2422\n",
      "Epoch 5 | Batch 2950/3649 | Batch Loss: 5.0019 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 3000/3649 | Batch Loss: 4.7512 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 3050/3649 | Batch Loss: 4.7949 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 3100/3649 | Batch Loss: 5.0915 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 3150/3649 | Batch Loss: 4.7733 | Batch Acc: 0.2109\n",
      "Epoch 5 | Batch 3200/3649 | Batch Loss: 4.6938 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 3250/3649 | Batch Loss: 4.9654 | Batch Acc: 0.2188\n",
      "Epoch 5 | Batch 3300/3649 | Batch Loss: 4.9432 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 3350/3649 | Batch Loss: 4.7596 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 3400/3649 | Batch Loss: 5.3703 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 3450/3649 | Batch Loss: 5.3417 | Batch Acc: 0.1562\n",
      "Epoch 5 | Batch 3500/3649 | Batch Loss: 4.9657 | Batch Acc: 0.1641\n",
      "Epoch 5 | Batch 3550/3649 | Batch Loss: 4.8054 | Batch Acc: 0.2031\n",
      "Epoch 5 | Batch 3600/3649 | Batch Loss: 5.2556 | Batch Acc: 0.1797\n",
      "Epoch 5 | Batch 3649/3649 | Batch Loss: 4.5737 | Batch Acc: 0.2045\n",
      "Epoch 5 completed in 68.84s | Average Loss: 4.8271 | Accuracy: 0.1834\n",
      "  Min batch loss: 4.0512 | Max batch loss: 5.6505\n",
      "\n",
      "Epoch 6 | Batch 50/3649 | Batch Loss: 4.5592 | Batch Acc: 0.1953\n",
      "Epoch 6 | Batch 100/3649 | Batch Loss: 4.5601 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 150/3649 | Batch Loss: 4.2182 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 200/3649 | Batch Loss: 4.4051 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 250/3649 | Batch Loss: 4.6135 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 300/3649 | Batch Loss: 4.5806 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 350/3649 | Batch Loss: 4.5816 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 400/3649 | Batch Loss: 4.3171 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 450/3649 | Batch Loss: 4.4718 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 500/3649 | Batch Loss: 4.3417 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 550/3649 | Batch Loss: 4.2613 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 600/3649 | Batch Loss: 4.6046 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 650/3649 | Batch Loss: 4.5581 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 700/3649 | Batch Loss: 4.7261 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 750/3649 | Batch Loss: 4.3517 | Batch Acc: 0.1953\n",
      "Epoch 6 | Batch 800/3649 | Batch Loss: 4.3367 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 850/3649 | Batch Loss: 4.8743 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 900/3649 | Batch Loss: 4.8792 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 950/3649 | Batch Loss: 4.3947 | Batch Acc: 0.2656\n",
      "Epoch 6 | Batch 1000/3649 | Batch Loss: 4.6606 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 1050/3649 | Batch Loss: 4.1687 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 1100/3649 | Batch Loss: 4.2962 | Batch Acc: 0.2812\n",
      "Epoch 6 | Batch 1150/3649 | Batch Loss: 4.8938 | Batch Acc: 0.1406\n",
      "Epoch 6 | Batch 1200/3649 | Batch Loss: 4.8102 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 1250/3649 | Batch Loss: 5.0867 | Batch Acc: 0.1328\n",
      "Epoch 6 | Batch 1300/3649 | Batch Loss: 4.4363 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 1350/3649 | Batch Loss: 5.0211 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 1400/3649 | Batch Loss: 4.4434 | Batch Acc: 0.2500\n",
      "Epoch 6 | Batch 1450/3649 | Batch Loss: 4.5865 | Batch Acc: 0.3047\n",
      "Epoch 6 | Batch 1500/3649 | Batch Loss: 4.5816 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 1550/3649 | Batch Loss: 4.8698 | Batch Acc: 0.1953\n",
      "Epoch 6 | Batch 1600/3649 | Batch Loss: 4.4657 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 1650/3649 | Batch Loss: 4.8615 | Batch Acc: 0.1953\n",
      "Epoch 6 | Batch 1700/3649 | Batch Loss: 4.8580 | Batch Acc: 0.1562\n",
      "Epoch 6 | Batch 1750/3649 | Batch Loss: 4.8294 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 1800/3649 | Batch Loss: 4.7799 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 1850/3649 | Batch Loss: 4.6514 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 1900/3649 | Batch Loss: 4.6116 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 1950/3649 | Batch Loss: 4.3202 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 2000/3649 | Batch Loss: 4.5883 | Batch Acc: 0.2812\n",
      "Epoch 6 | Batch 2050/3649 | Batch Loss: 4.4134 | Batch Acc: 0.2578\n",
      "Epoch 6 | Batch 2100/3649 | Batch Loss: 4.4743 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 2150/3649 | Batch Loss: 5.0761 | Batch Acc: 0.1250\n",
      "Epoch 6 | Batch 2200/3649 | Batch Loss: 4.5403 | Batch Acc: 0.2891\n",
      "Epoch 6 | Batch 2250/3649 | Batch Loss: 4.6325 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 2300/3649 | Batch Loss: 4.8212 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 2350/3649 | Batch Loss: 4.7051 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 2400/3649 | Batch Loss: 4.6872 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 2450/3649 | Batch Loss: 4.6518 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 2500/3649 | Batch Loss: 4.7573 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 2550/3649 | Batch Loss: 4.8357 | Batch Acc: 0.1875\n",
      "Epoch 6 | Batch 2600/3649 | Batch Loss: 4.9274 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 2650/3649 | Batch Loss: 4.7467 | Batch Acc: 0.1641\n",
      "Epoch 6 | Batch 2700/3649 | Batch Loss: 4.9891 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 2750/3649 | Batch Loss: 4.8692 | Batch Acc: 0.2188\n",
      "Epoch 6 | Batch 2800/3649 | Batch Loss: 4.5230 | Batch Acc: 0.2266\n",
      "Epoch 6 | Batch 2850/3649 | Batch Loss: 4.9909 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 2900/3649 | Batch Loss: 5.2489 | Batch Acc: 0.1562\n",
      "Epoch 6 | Batch 2950/3649 | Batch Loss: 4.8479 | Batch Acc: 0.1953\n",
      "Epoch 6 | Batch 3000/3649 | Batch Loss: 4.5159 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 3050/3649 | Batch Loss: 4.7167 | Batch Acc: 0.2031\n",
      "Epoch 6 | Batch 3100/3649 | Batch Loss: 4.5707 | Batch Acc: 0.2109\n",
      "Epoch 6 | Batch 3150/3649 | Batch Loss: 4.7099 | Batch Acc: 0.2344\n",
      "Epoch 6 | Batch 3200/3649 | Batch Loss: 4.8579 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 3250/3649 | Batch Loss: 4.8438 | Batch Acc: 0.2500\n",
      "Epoch 6 | Batch 3300/3649 | Batch Loss: 4.8097 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 3350/3649 | Batch Loss: 4.9443 | Batch Acc: 0.1797\n",
      "Epoch 6 | Batch 3400/3649 | Batch Loss: 4.8709 | Batch Acc: 0.1562\n",
      "Epoch 6 | Batch 3450/3649 | Batch Loss: 5.1510 | Batch Acc: 0.1562\n",
      "Epoch 6 | Batch 3500/3649 | Batch Loss: 4.8251 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 3550/3649 | Batch Loss: 5.2314 | Batch Acc: 0.1328\n",
      "Epoch 6 | Batch 3600/3649 | Batch Loss: 5.1846 | Batch Acc: 0.1719\n",
      "Epoch 6 | Batch 3649/3649 | Batch Loss: 4.1267 | Batch Acc: 0.2955\n",
      "Epoch 6 completed in 68.30s | Average Loss: 4.6423 | Accuracy: 0.2012\n",
      "  Min batch loss: 3.6904 | Max batch loss: 5.7765\n",
      "\n",
      "Epoch 7 | Batch 50/3649 | Batch Loss: 4.4297 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 100/3649 | Batch Loss: 4.0965 | Batch Acc: 0.2734\n",
      "Epoch 7 | Batch 150/3649 | Batch Loss: 4.0466 | Batch Acc: 0.2891\n",
      "Epoch 7 | Batch 200/3649 | Batch Loss: 3.9148 | Batch Acc: 0.2891\n",
      "Epoch 7 | Batch 250/3649 | Batch Loss: 4.2541 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 300/3649 | Batch Loss: 4.1074 | Batch Acc: 0.2969\n",
      "Epoch 7 | Batch 350/3649 | Batch Loss: 4.2435 | Batch Acc: 0.2891\n",
      "Epoch 7 | Batch 400/3649 | Batch Loss: 4.5792 | Batch Acc: 0.2266\n",
      "Epoch 7 | Batch 450/3649 | Batch Loss: 4.5866 | Batch Acc: 0.2656\n",
      "Epoch 7 | Batch 500/3649 | Batch Loss: 4.0722 | Batch Acc: 0.2656\n",
      "Epoch 7 | Batch 550/3649 | Batch Loss: 4.3820 | Batch Acc: 0.2422\n",
      "Epoch 7 | Batch 600/3649 | Batch Loss: 4.5023 | Batch Acc: 0.1641\n",
      "Epoch 7 | Batch 650/3649 | Batch Loss: 4.4214 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 700/3649 | Batch Loss: 4.4058 | Batch Acc: 0.2188\n",
      "Epoch 7 | Batch 750/3649 | Batch Loss: 4.3915 | Batch Acc: 0.2266\n",
      "Epoch 7 | Batch 800/3649 | Batch Loss: 4.3472 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 850/3649 | Batch Loss: 4.2698 | Batch Acc: 0.2734\n",
      "Epoch 7 | Batch 900/3649 | Batch Loss: 4.2653 | Batch Acc: 0.2500\n",
      "Epoch 7 | Batch 950/3649 | Batch Loss: 4.0432 | Batch Acc: 0.2734\n",
      "Epoch 7 | Batch 1000/3649 | Batch Loss: 4.5857 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 1050/3649 | Batch Loss: 4.5064 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 1100/3649 | Batch Loss: 4.4803 | Batch Acc: 0.2578\n",
      "Epoch 7 | Batch 1150/3649 | Batch Loss: 4.3774 | Batch Acc: 0.2422\n",
      "Epoch 7 | Batch 1200/3649 | Batch Loss: 4.4613 | Batch Acc: 0.2422\n",
      "Epoch 7 | Batch 1250/3649 | Batch Loss: 4.3465 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 1300/3649 | Batch Loss: 4.9068 | Batch Acc: 0.1562\n",
      "Epoch 7 | Batch 1350/3649 | Batch Loss: 4.2020 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 1400/3649 | Batch Loss: 4.2916 | Batch Acc: 0.2188\n",
      "Epoch 7 | Batch 1450/3649 | Batch Loss: 4.2906 | Batch Acc: 0.2578\n",
      "Epoch 7 | Batch 1500/3649 | Batch Loss: 4.5559 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 1550/3649 | Batch Loss: 4.6354 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 1600/3649 | Batch Loss: 4.3466 | Batch Acc: 0.2266\n",
      "Epoch 7 | Batch 1650/3649 | Batch Loss: 4.8810 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 1700/3649 | Batch Loss: 4.4810 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 1750/3649 | Batch Loss: 4.8836 | Batch Acc: 0.1797\n",
      "Epoch 7 | Batch 1800/3649 | Batch Loss: 4.7387 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 1850/3649 | Batch Loss: 4.6333 | Batch Acc: 0.2422\n",
      "Epoch 7 | Batch 1900/3649 | Batch Loss: 4.6859 | Batch Acc: 0.2422\n",
      "Epoch 7 | Batch 1950/3649 | Batch Loss: 4.4283 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2000/3649 | Batch Loss: 4.6841 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 2050/3649 | Batch Loss: 4.4764 | Batch Acc: 0.2188\n",
      "Epoch 7 | Batch 2100/3649 | Batch Loss: 4.2959 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 2150/3649 | Batch Loss: 4.6701 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 2200/3649 | Batch Loss: 4.4899 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 2250/3649 | Batch Loss: 4.2113 | Batch Acc: 0.2891\n",
      "Epoch 7 | Batch 2300/3649 | Batch Loss: 4.7659 | Batch Acc: 0.1719\n",
      "Epoch 7 | Batch 2350/3649 | Batch Loss: 4.5183 | Batch Acc: 0.1797\n",
      "Epoch 7 | Batch 2400/3649 | Batch Loss: 4.4750 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 2450/3649 | Batch Loss: 4.0994 | Batch Acc: 0.2969\n",
      "Epoch 7 | Batch 2500/3649 | Batch Loss: 4.5204 | Batch Acc: 0.1641\n",
      "Epoch 7 | Batch 2550/3649 | Batch Loss: 4.4073 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2600/3649 | Batch Loss: 4.5807 | Batch Acc: 0.2344\n",
      "Epoch 7 | Batch 2650/3649 | Batch Loss: 5.0404 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2700/3649 | Batch Loss: 4.4803 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2750/3649 | Batch Loss: 4.2738 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2800/3649 | Batch Loss: 4.8514 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 2850/3649 | Batch Loss: 4.7737 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 2900/3649 | Batch Loss: 4.3981 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 2950/3649 | Batch Loss: 4.7447 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 3000/3649 | Batch Loss: 4.4543 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 3050/3649 | Batch Loss: 5.0726 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 3100/3649 | Batch Loss: 4.3810 | Batch Acc: 0.1875\n",
      "Epoch 7 | Batch 3150/3649 | Batch Loss: 4.6187 | Batch Acc: 0.2266\n",
      "Epoch 7 | Batch 3200/3649 | Batch Loss: 4.5295 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 3250/3649 | Batch Loss: 5.1359 | Batch Acc: 0.1797\n",
      "Epoch 7 | Batch 3300/3649 | Batch Loss: 4.5727 | Batch Acc: 0.1719\n",
      "Epoch 7 | Batch 3350/3649 | Batch Loss: 4.8382 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 3400/3649 | Batch Loss: 4.6909 | Batch Acc: 0.2188\n",
      "Epoch 7 | Batch 3450/3649 | Batch Loss: 4.5829 | Batch Acc: 0.2109\n",
      "Epoch 7 | Batch 3500/3649 | Batch Loss: 4.5271 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 3550/3649 | Batch Loss: 4.3620 | Batch Acc: 0.2031\n",
      "Epoch 7 | Batch 3600/3649 | Batch Loss: 4.7951 | Batch Acc: 0.1953\n",
      "Epoch 7 | Batch 3649/3649 | Batch Loss: 4.3142 | Batch Acc: 0.1364\n",
      "Epoch 7 completed in 67.63s | Average Loss: 4.4918 | Accuracy: 0.2157\n",
      "  Min batch loss: 3.5718 | Max batch loss: 5.5136\n",
      "\n",
      "Epoch 8 | Batch 50/3649 | Batch Loss: 3.7759 | Batch Acc: 0.3203\n",
      "Epoch 8 | Batch 100/3649 | Batch Loss: 4.1136 | Batch Acc: 0.2891\n",
      "Epoch 8 | Batch 150/3649 | Batch Loss: 4.2680 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 200/3649 | Batch Loss: 4.4102 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 250/3649 | Batch Loss: 4.1365 | Batch Acc: 0.2188\n",
      "Epoch 8 | Batch 300/3649 | Batch Loss: 4.4218 | Batch Acc: 0.2891\n",
      "Epoch 8 | Batch 350/3649 | Batch Loss: 4.1796 | Batch Acc: 0.2422\n",
      "Epoch 8 | Batch 400/3649 | Batch Loss: 4.1167 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 450/3649 | Batch Loss: 4.4245 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 500/3649 | Batch Loss: 4.1289 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 550/3649 | Batch Loss: 4.0402 | Batch Acc: 0.2578\n",
      "Epoch 8 | Batch 600/3649 | Batch Loss: 4.1910 | Batch Acc: 0.2734\n",
      "Epoch 8 | Batch 650/3649 | Batch Loss: 3.7986 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 700/3649 | Batch Loss: 4.3383 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 750/3649 | Batch Loss: 4.2801 | Batch Acc: 0.3125\n",
      "Epoch 8 | Batch 800/3649 | Batch Loss: 4.2643 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 850/3649 | Batch Loss: 4.1099 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 900/3649 | Batch Loss: 4.2077 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 950/3649 | Batch Loss: 3.9498 | Batch Acc: 0.2969\n",
      "Epoch 8 | Batch 1000/3649 | Batch Loss: 4.2860 | Batch Acc: 0.2578\n",
      "Epoch 8 | Batch 1050/3649 | Batch Loss: 4.1940 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 1100/3649 | Batch Loss: 4.2591 | Batch Acc: 0.2969\n",
      "Epoch 8 | Batch 1150/3649 | Batch Loss: 3.9989 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 1200/3649 | Batch Loss: 3.9659 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 1250/3649 | Batch Loss: 4.3473 | Batch Acc: 0.2578\n",
      "Epoch 8 | Batch 1300/3649 | Batch Loss: 4.6573 | Batch Acc: 0.2266\n",
      "Epoch 8 | Batch 1350/3649 | Batch Loss: 4.3723 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 1400/3649 | Batch Loss: 4.7526 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 1450/3649 | Batch Loss: 4.0174 | Batch Acc: 0.2422\n",
      "Epoch 8 | Batch 1500/3649 | Batch Loss: 4.3847 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 1550/3649 | Batch Loss: 4.0862 | Batch Acc: 0.2578\n",
      "Epoch 8 | Batch 1600/3649 | Batch Loss: 4.5279 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 1650/3649 | Batch Loss: 4.5306 | Batch Acc: 0.1406\n",
      "Epoch 8 | Batch 1700/3649 | Batch Loss: 4.5919 | Batch Acc: 0.2188\n",
      "Epoch 8 | Batch 1750/3649 | Batch Loss: 4.5018 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 1800/3649 | Batch Loss: 4.3447 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 1850/3649 | Batch Loss: 4.4124 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 1900/3649 | Batch Loss: 4.3303 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 1950/3649 | Batch Loss: 4.7327 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 2000/3649 | Batch Loss: 4.2978 | Batch Acc: 0.2188\n",
      "Epoch 8 | Batch 2050/3649 | Batch Loss: 4.6764 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 2100/3649 | Batch Loss: 3.9577 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 2150/3649 | Batch Loss: 4.5076 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 2200/3649 | Batch Loss: 4.6616 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 2250/3649 | Batch Loss: 4.3146 | Batch Acc: 0.2031\n",
      "Epoch 8 | Batch 2300/3649 | Batch Loss: 4.2483 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 2350/3649 | Batch Loss: 4.3253 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 2400/3649 | Batch Loss: 4.3000 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 2450/3649 | Batch Loss: 4.4154 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 2500/3649 | Batch Loss: 4.3262 | Batch Acc: 0.1719\n",
      "Epoch 8 | Batch 2550/3649 | Batch Loss: 4.6270 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 2600/3649 | Batch Loss: 4.3896 | Batch Acc: 0.2031\n",
      "Epoch 8 | Batch 2650/3649 | Batch Loss: 4.5435 | Batch Acc: 0.2188\n",
      "Epoch 8 | Batch 2700/3649 | Batch Loss: 4.4697 | Batch Acc: 0.2109\n",
      "Epoch 8 | Batch 2750/3649 | Batch Loss: 4.4175 | Batch Acc: 0.2422\n",
      "Epoch 8 | Batch 2800/3649 | Batch Loss: 4.5300 | Batch Acc: 0.2656\n",
      "Epoch 8 | Batch 2850/3649 | Batch Loss: 4.2942 | Batch Acc: 0.2578\n",
      "Epoch 8 | Batch 2900/3649 | Batch Loss: 4.2042 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 2950/3649 | Batch Loss: 4.3590 | Batch Acc: 0.2344\n",
      "Epoch 8 | Batch 3000/3649 | Batch Loss: 4.5926 | Batch Acc: 0.2031\n",
      "Epoch 8 | Batch 3050/3649 | Batch Loss: 4.5781 | Batch Acc: 0.2266\n",
      "Epoch 8 | Batch 3100/3649 | Batch Loss: 4.4178 | Batch Acc: 0.2422\n",
      "Epoch 8 | Batch 3150/3649 | Batch Loss: 4.4046 | Batch Acc: 0.1797\n",
      "Epoch 8 | Batch 3200/3649 | Batch Loss: 4.5695 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 3250/3649 | Batch Loss: 4.5163 | Batch Acc: 0.1875\n",
      "Epoch 8 | Batch 3300/3649 | Batch Loss: 4.6511 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 3350/3649 | Batch Loss: 4.4983 | Batch Acc: 0.2266\n",
      "Epoch 8 | Batch 3400/3649 | Batch Loss: 4.5208 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 3450/3649 | Batch Loss: 4.9140 | Batch Acc: 0.1641\n",
      "Epoch 8 | Batch 3500/3649 | Batch Loss: 4.4141 | Batch Acc: 0.2500\n",
      "Epoch 8 | Batch 3550/3649 | Batch Loss: 4.5039 | Batch Acc: 0.1953\n",
      "Epoch 8 | Batch 3600/3649 | Batch Loss: 4.6457 | Batch Acc: 0.1797\n",
      "Epoch 8 | Batch 3649/3649 | Batch Loss: 4.8968 | Batch Acc: 0.2045\n",
      "Epoch 8 completed in 67.49s | Average Loss: 4.3647 | Accuracy: 0.2291\n",
      "  Min batch loss: 3.2742 | Max batch loss: 5.4180\n",
      "\n",
      "Epoch 9 | Batch 50/3649 | Batch Loss: 3.8861 | Batch Acc: 0.2891\n",
      "Epoch 9 | Batch 100/3649 | Batch Loss: 3.9779 | Batch Acc: 0.3125\n",
      "Epoch 9 | Batch 150/3649 | Batch Loss: 4.2175 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 200/3649 | Batch Loss: 3.9345 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 250/3649 | Batch Loss: 3.8229 | Batch Acc: 0.2969\n",
      "Epoch 9 | Batch 300/3649 | Batch Loss: 3.8454 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 350/3649 | Batch Loss: 3.9626 | Batch Acc: 0.2812\n",
      "Epoch 9 | Batch 400/3649 | Batch Loss: 4.2107 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 450/3649 | Batch Loss: 4.1039 | Batch Acc: 0.2266\n",
      "Epoch 9 | Batch 500/3649 | Batch Loss: 4.1579 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 550/3649 | Batch Loss: 4.3331 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 600/3649 | Batch Loss: 4.2035 | Batch Acc: 0.2578\n",
      "Epoch 9 | Batch 650/3649 | Batch Loss: 4.1585 | Batch Acc: 0.2188\n",
      "Epoch 9 | Batch 700/3649 | Batch Loss: 4.0538 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 750/3649 | Batch Loss: 4.0313 | Batch Acc: 0.2578\n",
      "Epoch 9 | Batch 800/3649 | Batch Loss: 4.1191 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 850/3649 | Batch Loss: 4.5109 | Batch Acc: 0.2031\n",
      "Epoch 9 | Batch 900/3649 | Batch Loss: 4.2696 | Batch Acc: 0.2812\n",
      "Epoch 9 | Batch 950/3649 | Batch Loss: 3.8024 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 1000/3649 | Batch Loss: 3.9266 | Batch Acc: 0.2578\n",
      "Epoch 9 | Batch 1050/3649 | Batch Loss: 3.8607 | Batch Acc: 0.2969\n",
      "Epoch 9 | Batch 1100/3649 | Batch Loss: 4.3774 | Batch Acc: 0.2188\n",
      "Epoch 9 | Batch 1150/3649 | Batch Loss: 3.9040 | Batch Acc: 0.3359\n",
      "Epoch 9 | Batch 1200/3649 | Batch Loss: 4.3870 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 1250/3649 | Batch Loss: 4.6095 | Batch Acc: 0.1953\n",
      "Epoch 9 | Batch 1300/3649 | Batch Loss: 4.0982 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 1350/3649 | Batch Loss: 4.2903 | Batch Acc: 0.1797\n",
      "Epoch 9 | Batch 1400/3649 | Batch Loss: 4.0105 | Batch Acc: 0.2734\n",
      "Epoch 9 | Batch 1450/3649 | Batch Loss: 4.1006 | Batch Acc: 0.2891\n",
      "Epoch 9 | Batch 1500/3649 | Batch Loss: 4.4041 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 1550/3649 | Batch Loss: 4.2776 | Batch Acc: 0.2578\n",
      "Epoch 9 | Batch 1600/3649 | Batch Loss: 4.2415 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 1650/3649 | Batch Loss: 4.3024 | Batch Acc: 0.2500\n",
      "Epoch 9 | Batch 1700/3649 | Batch Loss: 4.3700 | Batch Acc: 0.2266\n",
      "Epoch 9 | Batch 1750/3649 | Batch Loss: 4.7261 | Batch Acc: 0.2188\n",
      "Epoch 9 | Batch 1800/3649 | Batch Loss: 4.7362 | Batch Acc: 0.1797\n",
      "Epoch 9 | Batch 1850/3649 | Batch Loss: 4.0534 | Batch Acc: 0.2734\n",
      "Epoch 9 | Batch 1900/3649 | Batch Loss: 4.2408 | Batch Acc: 0.2734\n",
      "Epoch 9 | Batch 1950/3649 | Batch Loss: 4.4134 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 2000/3649 | Batch Loss: 4.4735 | Batch Acc: 0.2031\n",
      "Epoch 9 | Batch 2050/3649 | Batch Loss: 4.1447 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 2100/3649 | Batch Loss: 4.4582 | Batch Acc: 0.2188\n",
      "Epoch 9 | Batch 2150/3649 | Batch Loss: 4.0999 | Batch Acc: 0.2734\n",
      "Epoch 9 | Batch 2200/3649 | Batch Loss: 4.1093 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 2250/3649 | Batch Loss: 4.3760 | Batch Acc: 0.2109\n",
      "Epoch 9 | Batch 2300/3649 | Batch Loss: 4.8530 | Batch Acc: 0.1562\n",
      "Epoch 9 | Batch 2350/3649 | Batch Loss: 4.0219 | Batch Acc: 0.2578\n",
      "Epoch 9 | Batch 2400/3649 | Batch Loss: 3.9216 | Batch Acc: 0.3125\n",
      "Epoch 9 | Batch 2450/3649 | Batch Loss: 4.5174 | Batch Acc: 0.1797\n",
      "Epoch 9 | Batch 2500/3649 | Batch Loss: 4.3672 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 2550/3649 | Batch Loss: 4.9155 | Batch Acc: 0.1250\n",
      "Epoch 9 | Batch 2600/3649 | Batch Loss: 4.4809 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 2650/3649 | Batch Loss: 4.2734 | Batch Acc: 0.2031\n",
      "Epoch 9 | Batch 2700/3649 | Batch Loss: 4.5645 | Batch Acc: 0.2266\n",
      "Epoch 9 | Batch 2750/3649 | Batch Loss: 3.8621 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 2800/3649 | Batch Loss: 4.1483 | Batch Acc: 0.2734\n",
      "Epoch 9 | Batch 2850/3649 | Batch Loss: 4.5917 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 2900/3649 | Batch Loss: 4.5387 | Batch Acc: 0.2109\n",
      "Epoch 9 | Batch 2950/3649 | Batch Loss: 4.2390 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 3000/3649 | Batch Loss: 4.2657 | Batch Acc: 0.2031\n",
      "Epoch 9 | Batch 3050/3649 | Batch Loss: 4.4338 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 3100/3649 | Batch Loss: 4.2716 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 3150/3649 | Batch Loss: 4.5871 | Batch Acc: 0.1641\n",
      "Epoch 9 | Batch 3200/3649 | Batch Loss: 4.3938 | Batch Acc: 0.1953\n",
      "Epoch 9 | Batch 3250/3649 | Batch Loss: 4.1269 | Batch Acc: 0.2812\n",
      "Epoch 9 | Batch 3300/3649 | Batch Loss: 4.3948 | Batch Acc: 0.2344\n",
      "Epoch 9 | Batch 3350/3649 | Batch Loss: 4.9458 | Batch Acc: 0.1328\n",
      "Epoch 9 | Batch 3400/3649 | Batch Loss: 4.5867 | Batch Acc: 0.2656\n",
      "Epoch 9 | Batch 3450/3649 | Batch Loss: 4.5013 | Batch Acc: 0.2031\n",
      "Epoch 9 | Batch 3500/3649 | Batch Loss: 4.6138 | Batch Acc: 0.2266\n",
      "Epoch 9 | Batch 3550/3649 | Batch Loss: 4.1952 | Batch Acc: 0.2422\n",
      "Epoch 9 | Batch 3600/3649 | Batch Loss: 4.7365 | Batch Acc: 0.1719\n",
      "Epoch 9 | Batch 3649/3649 | Batch Loss: 4.5094 | Batch Acc: 0.2500\n",
      "Epoch 9 completed in 67.70s | Average Loss: 4.2504 | Accuracy: 0.2409\n",
      "  Min batch loss: 3.2927 | Max batch loss: 5.2378\n",
      "\n",
      "Epoch 10 | Batch 50/3649 | Batch Loss: 3.8871 | Batch Acc: 0.2578\n",
      "Epoch 10 | Batch 100/3649 | Batch Loss: 3.6914 | Batch Acc: 0.2344\n",
      "Epoch 10 | Batch 150/3649 | Batch Loss: 4.1682 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 200/3649 | Batch Loss: 3.5800 | Batch Acc: 0.2969\n",
      "Epoch 10 | Batch 250/3649 | Batch Loss: 3.5614 | Batch Acc: 0.3438\n",
      "Epoch 10 | Batch 300/3649 | Batch Loss: 3.8167 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 350/3649 | Batch Loss: 4.1108 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 400/3649 | Batch Loss: 4.2863 | Batch Acc: 0.1875\n",
      "Epoch 10 | Batch 450/3649 | Batch Loss: 3.9912 | Batch Acc: 0.2500\n",
      "Epoch 10 | Batch 500/3649 | Batch Loss: 3.9096 | Batch Acc: 0.2578\n",
      "Epoch 10 | Batch 550/3649 | Batch Loss: 4.2102 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 600/3649 | Batch Loss: 3.7580 | Batch Acc: 0.2500\n",
      "Epoch 10 | Batch 650/3649 | Batch Loss: 3.9266 | Batch Acc: 0.2578\n",
      "Epoch 10 | Batch 700/3649 | Batch Loss: 4.1279 | Batch Acc: 0.2734\n",
      "Epoch 10 | Batch 750/3649 | Batch Loss: 3.7293 | Batch Acc: 0.3047\n",
      "Epoch 10 | Batch 800/3649 | Batch Loss: 4.3878 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 850/3649 | Batch Loss: 4.4236 | Batch Acc: 0.1953\n",
      "Epoch 10 | Batch 900/3649 | Batch Loss: 4.0863 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 950/3649 | Batch Loss: 3.8203 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 1000/3649 | Batch Loss: 4.4070 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 1050/3649 | Batch Loss: 4.2236 | Batch Acc: 0.2266\n",
      "Epoch 10 | Batch 1100/3649 | Batch Loss: 4.5546 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 1150/3649 | Batch Loss: 3.9122 | Batch Acc: 0.2969\n",
      "Epoch 10 | Batch 1200/3649 | Batch Loss: 3.9783 | Batch Acc: 0.1797\n",
      "Epoch 10 | Batch 1250/3649 | Batch Loss: 4.2037 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 1300/3649 | Batch Loss: 3.9561 | Batch Acc: 0.3047\n",
      "Epoch 10 | Batch 1350/3649 | Batch Loss: 3.9305 | Batch Acc: 0.2734\n",
      "Epoch 10 | Batch 1400/3649 | Batch Loss: 3.9485 | Batch Acc: 0.2734\n",
      "Epoch 10 | Batch 1450/3649 | Batch Loss: 3.8876 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 1500/3649 | Batch Loss: 4.3309 | Batch Acc: 0.2344\n",
      "Epoch 10 | Batch 1550/3649 | Batch Loss: 4.2329 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 1600/3649 | Batch Loss: 4.0390 | Batch Acc: 0.2344\n",
      "Epoch 10 | Batch 1650/3649 | Batch Loss: 3.8828 | Batch Acc: 0.3125\n",
      "Epoch 10 | Batch 1700/3649 | Batch Loss: 4.3908 | Batch Acc: 0.1953\n",
      "Epoch 10 | Batch 1750/3649 | Batch Loss: 4.3139 | Batch Acc: 0.2266\n",
      "Epoch 10 | Batch 1800/3649 | Batch Loss: 3.9463 | Batch Acc: 0.2500\n",
      "Epoch 10 | Batch 1850/3649 | Batch Loss: 3.6870 | Batch Acc: 0.3203\n",
      "Epoch 10 | Batch 1900/3649 | Batch Loss: 3.7871 | Batch Acc: 0.2734\n",
      "Epoch 10 | Batch 1950/3649 | Batch Loss: 4.5468 | Batch Acc: 0.2031\n",
      "Epoch 10 | Batch 2000/3649 | Batch Loss: 4.5440 | Batch Acc: 0.1641\n",
      "Epoch 10 | Batch 2050/3649 | Batch Loss: 3.9584 | Batch Acc: 0.2578\n",
      "Epoch 10 | Batch 2100/3649 | Batch Loss: 4.3913 | Batch Acc: 0.1719\n",
      "Epoch 10 | Batch 2150/3649 | Batch Loss: 4.1732 | Batch Acc: 0.3047\n",
      "Epoch 10 | Batch 2200/3649 | Batch Loss: 4.3365 | Batch Acc: 0.2109\n",
      "Epoch 10 | Batch 2250/3649 | Batch Loss: 4.2433 | Batch Acc: 0.2109\n",
      "Epoch 10 | Batch 2300/3649 | Batch Loss: 3.8023 | Batch Acc: 0.2734\n",
      "Epoch 10 | Batch 2350/3649 | Batch Loss: 4.0865 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 2400/3649 | Batch Loss: 3.9438 | Batch Acc: 0.2656\n",
      "Epoch 10 | Batch 2450/3649 | Batch Loss: 4.3221 | Batch Acc: 0.2344\n",
      "Epoch 10 | Batch 2500/3649 | Batch Loss: 4.2823 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 2550/3649 | Batch Loss: 4.0293 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 2600/3649 | Batch Loss: 4.7299 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 2650/3649 | Batch Loss: 4.7613 | Batch Acc: 0.1406\n",
      "Epoch 10 | Batch 2700/3649 | Batch Loss: 4.2057 | Batch Acc: 0.2656\n",
      "Epoch 10 | Batch 2750/3649 | Batch Loss: 4.3846 | Batch Acc: 0.2422\n",
      "Epoch 10 | Batch 2800/3649 | Batch Loss: 3.9188 | Batch Acc: 0.2344\n",
      "Epoch 10 | Batch 2850/3649 | Batch Loss: 4.0530 | Batch Acc: 0.2266\n",
      "Epoch 10 | Batch 2900/3649 | Batch Loss: 4.4135 | Batch Acc: 0.2266\n",
      "Epoch 10 | Batch 2950/3649 | Batch Loss: 4.5322 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 3000/3649 | Batch Loss: 4.0411 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 3050/3649 | Batch Loss: 4.0539 | Batch Acc: 0.2812\n",
      "Epoch 10 | Batch 3100/3649 | Batch Loss: 4.2236 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 3150/3649 | Batch Loss: 4.6193 | Batch Acc: 0.1562\n",
      "Epoch 10 | Batch 3200/3649 | Batch Loss: 4.1526 | Batch Acc: 0.2266\n",
      "Epoch 10 | Batch 3250/3649 | Batch Loss: 5.0532 | Batch Acc: 0.1406\n",
      "Epoch 10 | Batch 3300/3649 | Batch Loss: 4.5177 | Batch Acc: 0.1875\n",
      "Epoch 10 | Batch 3350/3649 | Batch Loss: 4.5885 | Batch Acc: 0.1797\n",
      "Epoch 10 | Batch 3400/3649 | Batch Loss: 4.0435 | Batch Acc: 0.2188\n",
      "Epoch 10 | Batch 3450/3649 | Batch Loss: 4.3149 | Batch Acc: 0.2812\n",
      "Epoch 10 | Batch 3500/3649 | Batch Loss: 3.9459 | Batch Acc: 0.2891\n",
      "Epoch 10 | Batch 3550/3649 | Batch Loss: 4.6620 | Batch Acc: 0.1875\n",
      "Epoch 10 | Batch 3600/3649 | Batch Loss: 4.4172 | Batch Acc: 0.2031\n",
      "Epoch 10 | Batch 3649/3649 | Batch Loss: 4.3471 | Batch Acc: 0.1591\n",
      "Epoch 10 completed in 71.62s | Average Loss: 4.1474 | Accuracy: 0.2514\n",
      "  Min batch loss: 3.3447 | Max batch loss: 5.0791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Use a small number for a quick test, increase for better results\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "cbow_dataloader = DataLoader(\n",
    "    dataset=cbow_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "PRINT_MODULO = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # The DataLoader provides batches of data automatically\n",
    "    for batch_idx, (context_batch, target_batch) in enumerate(cbow_dataloader):\n",
    "        # The context_batch is already the correct shape [BATCH_SIZE, 2 * CONTEXT_SIZE]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_batch)\n",
    "\n",
    "        # CrossEntropyLoss expects the target to be a 1D tensor of shape [BATCH_SIZE].\n",
    "        # Our target_batch is [BATCH_SIZE, 1], so we use .squeeze() to remove the extra dimension.\n",
    "        targets = target_batch.squeeze()\n",
    "        loss = loss_function(log_probs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted = log_probs.argmax(dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_MODULO == 0 or (batch_idx + 1) == len(cbow_dataloader):\n",
    "            batch_acc = (predicted == targets).float().mean().item()\n",
    "            print(\n",
    "                f\"Epoch {epoch+1} | Batch {batch_idx+1}/{len(cbow_dataloader)} | \"\n",
    "                f\"Batch Loss: {loss.item():.4f} | Batch Acc: {batch_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Calculate average loss and accuracy over all batches\n",
    "    avg_loss = total_loss / len(cbow_dataloader)\n",
    "    epoch_acc = correct / total if total > 0 else 0.0\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s | Average Loss: {avg_loss:.4f} | Accuracy: {epoch_acc:.4f}\")\n",
    "    print(f\"  Min batch loss: {min(batch_losses):.4f} | Max batch loss: {max(batch_losses):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "55cbd175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['i', 'didn', 'know', 'this']\n",
      "Top predictions for the center word:\n",
      "  t: 0.9621\n",
      "  why: 0.0035\n",
      "  movie: 0.0031\n",
      "  but: 0.0025\n",
      "  that: 0.0023\n",
      "------------------------------\n",
      "Context: ['this', 'movie', 'is', 'good']\n",
      "Top predictions for the center word:\n",
      "  a: 0.2687\n",
      "  but: 0.0553\n",
      "  very: 0.0316\n",
      "  one: 0.0274\n",
      "  not: 0.0270\n"
     ]
    }
   ],
   "source": [
    "def evaluate_cbow(model, context_words):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert context words to a tensor of indices\n",
    "        try:\n",
    "            context_idxs = torch.tensor([word_to_idx[w] for w in context_words], dtype=torch.long)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: One of the context words is not in the vocabulary: {e}\")\n",
    "            return\n",
    "\n",
    "        # =================================================================\n",
    "        # THE FIX: Add a batch dimension before passing to the model\n",
    "        # =================================================================\n",
    "        context_idxs = context_idxs.unsqueeze(0)  # Shape changes from [4] to [1, 4]\n",
    "        # =================================================================\n",
    "        \n",
    "        # Now the model receives the correct input shape\n",
    "        output = model(context_idxs)\n",
    "        \n",
    "        # The rest of the function is correct\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        top_prob, top_idx = torch.topk(probs, 5)  # top 5 predictions\n",
    "\n",
    "        print(f\"Context: {context_words}\")\n",
    "        print(\"Top predictions for the center word:\")\n",
    "        for prob, idx in zip(top_prob[0], top_idx[0]):\n",
    "            print(f\"  {idx_to_word[idx.item()]}: {prob.item():.4f}\")\n",
    "\n",
    "# Now, when you call it, it will work\n",
    "context_example = ['i', 'didn', 'know', 'this']\n",
    "evaluate_cbow(model, context_example)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "context_example_2 = ['this', 'movie', 'is', 'good']\n",
    "evaluate_cbow(model, context_example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c18f7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'movie' and 'film': 0.5160\n"
     ]
    }
   ],
   "source": [
    "movie_embedding = model.embedding.weight[word_to_idx[\"movie\"]]\n",
    "film_embedding = model.embedding.weight[word_to_idx[\"film\"]]\n",
    "\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(movie_embedding, film_embedding, dim=0)\n",
    "print(f\"Cosine similarity between 'movie' and 'film': {cosine_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5b15384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'dog' and 'chair': 0.0956\n"
     ]
    }
   ],
   "source": [
    "dog_embedding = model.embedding.weight[word_to_idx[\"dog\"]]\n",
    "chair_embedding = model.embedding.weight[word_to_idx[\"chair\"]]\n",
    "\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(dog_embedding, chair_embedding, dim=0)\n",
    "print(f\"Cosine similarity between 'dog' and 'chair': {cosine_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "258b3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'dog' and 'cat': -0.0769\n"
     ]
    }
   ],
   "source": [
    "dog_embedding = model.embedding.weight[word_to_idx[\"dog\"]]\n",
    "cat_embedding = model.embedding.weight[word_to_idx[\"cat\"]]\n",
    "\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(dog_embedding, cat_embedding, dim=0)\n",
    "print(f\"Cosine similarity between 'dog' and 'cat': {cosine_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "54ded2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'movie':\n",
      "  - film            (Similarity: 0.5160)\n",
      "  - liver           (Similarity: 0.3955)\n",
      "  - misfire         (Similarity: 0.3730)\n",
      "  - cinematographic (Similarity: 0.3606)\n",
      "  - adaption        (Similarity: 0.3528)\n",
      "\n",
      "========================================\n",
      "\n",
      "Most similar words to 'dog':\n",
      "  - doulittle       (Similarity: 0.3887)\n",
      "  - inked           (Similarity: 0.3841)\n",
      "  - kelley          (Similarity: 0.3630)\n",
      "  - histories       (Similarity: 0.3586)\n",
      "  - stanza          (Similarity: 0.3533)\n",
      "\n",
      "========================================\n",
      "\n",
      "Most similar words to 'good':\n",
      "  - decent          (Similarity: 0.4362)\n",
      "  - great           (Similarity: 0.4184)\n",
      "  - prepubescent    (Similarity: 0.4182)\n",
      "  - earnestness     (Similarity: 0.4085)\n",
      "  - novelty         (Similarity: 0.4051)\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(word, model, word_to_idx, idx_to_word, top_k=10):\n",
    "    \"\"\"\n",
    "    Finds the top_k most similar words to a given word based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to find similar words for.\n",
    "        model (nn.Module): The trained CBOW or Skip-gram model.\n",
    "        word_to_idx (dict): Mapping from word to its index.\n",
    "        idx_to_word (dict): Mapping from index to its word.\n",
    "        top_k (int): The number of similar words to return.\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"Error: '{word}' not in vocabulary.\")\n",
    "        return\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Get the full embedding matrix\n",
    "        all_embeddings = model.embedding.weight\n",
    "\n",
    "        # 2. Get the query vector for the input word\n",
    "        query_idx = torch.tensor([word_to_idx[word]], dtype=torch.long)\n",
    "        query_embedding = model.embedding(query_idx) # Shape: [1, EMBEDDING_DIM]\n",
    "\n",
    "        # 3. Compute cosine similarity between the query vector and all other vectors\n",
    "        # The query_embedding is [1, D] and all_embeddings is [V, D].\n",
    "        # F.cosine_similarity will broadcast the query vector to compare against all V vectors.\n",
    "        # dim=1 specifies that the similarity should be computed along the embedding dimension.\n",
    "        cos_similarities = torch.nn.functional.cosine_similarity(\n",
    "            query_embedding, \n",
    "            all_embeddings, \n",
    "            dim=1\n",
    "        )\n",
    "        # The result is a tensor of shape [VOCAB_SIZE]\n",
    "\n",
    "        # 4. Find the top K most similar words\n",
    "        # We ask for top_k + 1 because the most similar word will always be the word itself.\n",
    "        top_results = torch.topk(cos_similarities, k=top_k + 1)\n",
    "        \n",
    "        top_indices = top_results.indices\n",
    "        top_scores = top_results.values\n",
    "\n",
    "        print(f\"Most similar words to '{word}':\")\n",
    "        # 5. Print the results, skipping the first one (which is the word itself)\n",
    "        for i in range(1, top_k + 1):\n",
    "            idx = top_indices[i].item()\n",
    "            score = top_scores[i].item()\n",
    "            similar_word = idx_to_word[idx]\n",
    "            print(f\"  - {similar_word:<15} (Similarity: {score:.4f})\")\n",
    "\n",
    "# Find words similar to 'movie'\n",
    "find_most_similar('movie', model, word_to_idx, idx_to_word, top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# Find words similar to 'dog'\n",
    "find_most_similar('dog', model, word_to_idx, idx_to_word, top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# Find words similar to 'good'\n",
    "find_most_similar('good', model, word_to_idx, idx_to_word, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d22c8e4bd8c72",
   "metadata": {},
   "source": [
    "## Task 2: Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7f73c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sentence: ['this', 'is', 'one', 'of', 'those', 'movies', 'that', 'you', 'and', 'a']\n",
      "First 4 samples: [('this', '<PAD>'), ('this', '<PAD>'), ('this', 'is'), ('this', 'one')]\n",
      "\n",
      "Last sentence: ['or', 'native', 'inability', 'ever', 'to', 'give', 'in', 'to', 'being', 'dead']\n",
      "Last 4 samples: [('dead', 'being'), ('dead', 'to'), ('dead', '<PAD>'), ('dead', '<PAD>')]\n"
     ]
    }
   ],
   "source": [
    "skip_gram_training : list[tuple[str, str]]= [] # list of (center_word -> word in context)\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    padded_sentence = [PAD_TOKEN] * CONTEXT_SIZE + sentence + [PAD_TOKEN] * CONTEXT_SIZE\n",
    "    for i in range(len(padded_sentence)):\n",
    "        if i < CONTEXT_SIZE or i >= len(sentence)+CONTEXT_SIZE:\n",
    "            continue\n",
    "       \n",
    "        center_word = padded_sentence[i]\n",
    "\n",
    "        for k in range(CONTEXT_SIZE):\n",
    "            skip_gram_training.append((center_word, padded_sentence[i-(k+1)]))\n",
    "        for k in range(CONTEXT_SIZE):\n",
    "            skip_gram_training.append((center_word, padded_sentence[i+(k+1)]))\n",
    "\n",
    "\n",
    "\n",
    "# show first 5 samples\n",
    "\n",
    "print(f\"first sentence: {tokenized_corpus[0][:10]}\")\n",
    "print(f\"First 4 samples: {skip_gram_training[:4]}\")\n",
    "print()\n",
    "print(f\"Last sentence: {tokenized_corpus[-1][-10:]}\")\n",
    "print(f\"Last 4 samples: {skip_gram_training[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4e52461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: (tensor([6361]), tensor([24876]))\n",
      "Last sample: (tensor([15946]), tensor([24876]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Skip-Gram training data.\"\"\"\n",
    "    def __init__(self, data_tensors: list[tuple[str, str]], word_to_idx: dict[str, int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_tensors (list of tuples): A list where each element is a tuple (center_word, word in context).\n",
    "        \"\"\"\n",
    "        self.data = data_tensors\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "        \"\"\"\n",
    "        center_word, context_word = self.data[idx]\n",
    "\n",
    "        return torch.tensor([word_to_idx[center_word]]), torch.tensor([word_to_idx[context_word]])\n",
    "\n",
    "\n",
    "skip_gram_dataset = SkipGramDataset(skip_gram_training, word_to_idx=word_to_idx)\n",
    "\n",
    "print(f\"First sample: {skip_gram_dataset[0]}\")\n",
    "print(f\"Last sample: {skip_gram_dataset[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a3768bb5eaf7750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, center_word_idx:torch.Tensor):\n",
    "        center_word_emb = self.embeddings(center_word_idx)\n",
    "        return self.linear(center_word_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a08d926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2d90fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: runs/skipgram_20250715-170035\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Create a unique directory for this training run\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"runs/skipgram_{timestamp}\"\n",
    "\n",
    "# Instantiate the writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d727026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 267.32s | Avg Loss: 7.0563 | Accuracy: 0.0638\n",
      "Epoch 2 completed in 317.91s | Avg Loss: 6.6207 | Accuracy: 0.0729\n",
      "Epoch 3 completed in 304.20s | Avg Loss: 6.5146 | Accuracy: 0.0750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[229]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m loss = loss_function(log_probs, context)\n\u001b[32m     36\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# --- TensorBoard Logging ---\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Log the loss for this specific batch\u001b[39;00m\n\u001b[32m     41\u001b[39m writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mLoss/batch\u001b[39m\u001b[33m'\u001b[39m, loss.item(), global_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/torch/optim/adam.py:527\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "skip_gram_dataset = SkipGramDataset(skip_gram_training, word_to_idx=word_to_idx)\n",
    "skip_gram_dataloader = DataLoader(\n",
    "    dataset=skip_gram_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "model = model = SkipGram(vocab_size, EMBEDDING_DIM)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Use a small number for a quick test, increase for better results\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (center, context) in enumerate(skip_gram_dataloader):\n",
    "        # Squeeze the tensors right after getting them from the dataloader\n",
    "        center = center.squeeze(1)   # Shape: [BATCH_SIZE]\n",
    "        context = context.squeeze(1) # Shape: [BATCH_SIZE]\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(center) # Model now takes [BATCH_SIZE] and returns [BATCH_SIZE, VOCAB_SIZE]\n",
    "        loss = loss_function(log_probs, context)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- TensorBoard Logging ---\n",
    "        # Log the loss for this specific batch\n",
    "        writer.add_scalar('Loss/batch', loss.item(), global_step)\n",
    "        \n",
    "        # Log accuracy for this batch\n",
    "        with torch.no_grad():\n",
    "            predicted = log_probs.argmax(dim=1)\n",
    "            batch_acc = (predicted == context).float().mean().item()\n",
    "            writer.add_scalar('Accuracy/batch', batch_acc, global_step)\n",
    "        \n",
    "        global_step += 1\n",
    "        # -------------------------\n",
    "\n",
    "        # Accumulate for epoch-level stats\n",
    "        total_loss += loss.item()\n",
    "        correct += (predicted == context).sum().item()\n",
    "        total += context.size(0)\n",
    "\n",
    "    # --- Log epoch-level metrics ---\n",
    "    avg_loss = total_loss / len(skip_gram_dataloader)\n",
    "    epoch_acc = correct / total\n",
    "    writer.add_scalar('Loss/epoch', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/epoch', epoch_acc, epoch)\n",
    "    # -----------------------------\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s | Avg Loss: {avg_loss:.4f} | Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "41990e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embeddings for TensorBoard visualization...\n",
      "Embeddings saved. Launch TensorBoard to view.\n",
      "Command: tensorboard --logdir runs/skipgram_20250715-170035\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing embeddings for TensorBoard visualization...\")\n",
    "\n",
    "# 1. Get the trained embedding matrix from the model\n",
    "final_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# 2. Get the corresponding labels for each vector (our vocabulary)\n",
    "# We need a list of strings, so we'll create it from our idx_to_word mapping.\n",
    "# Make sure the order is correct!\n",
    "metadata = [idx_to_word[i] for i in range(vocab_size)]\n",
    "\n",
    "# 3. Add the embedding data to the writer\n",
    "# This function will save the data needed for the visualization.\n",
    "writer.add_embedding(\n",
    "    mat=final_embeddings,\n",
    "    metadata=metadata,\n",
    "    global_step=epochs  # Log it at the final epoch\n",
    ")\n",
    "\n",
    "# 4. Close the writer to make sure everything is saved\n",
    "writer.close()\n",
    "\n",
    "print(\"Embeddings saved. Launch TensorBoard to view.\")\n",
    "print(f\"Command: tensorboard --logdir {writer.log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "12a2de3d23bd45fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center word: 'can'\n",
      "Top predicted context words per context position:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m     22\u001b[39m center_word_example = \u001b[33m'\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mevaluate_skipgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_word_example\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mevaluate_skipgram\u001b[39m\u001b[34m(model, center_word)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pos, preds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(context_preds):\n\u001b[32m     14\u001b[39m     probs = torch.softmax(preds, dim=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# softmax over vocab dimension\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     top_prob, top_idx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Context position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prob, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(top_prob, top_idx):\n",
      "\u001b[31mRuntimeError\u001b[39m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "def evaluate_skipgram(model, center_word):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_idx = torch.tensor([word_to_idx[center_word]], dtype=torch.long)  # (1,)\n",
    "        output = model(input_idx)  # (1, context_size*2, vocab_size)\n",
    "        \n",
    "        # For each context position, get top predictions\n",
    "        context_preds = output.squeeze(0)  # (context_size*2, vocab_size)\n",
    "        \n",
    "        print(f\"Center word: '{center_word}'\")\n",
    "        print(\"Top predicted context words per context position:\")\n",
    "        \n",
    "        for pos, preds in enumerate(context_preds):\n",
    "            probs = torch.softmax(preds, dim=0)  # softmax over vocab dimension\n",
    "            top_prob, top_idx = torch.topk(probs, 5)\n",
    "            print(f\" Context position {pos+1}:\")\n",
    "            for prob, idx in zip(top_prob, top_idx):\n",
    "                print(f\"   {idx_to_word[int(idx.item())]}: {prob.item():.4f}\")\n",
    "            print()\n",
    "\n",
    "# Example usage\n",
    "center_word_example = 'can'\n",
    "evaluate_skipgram(model, center_word_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "abfbef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embedding(word, model, word_to_idx):\n",
    "    \"\"\"Helper function to get the embedding vector for a single word.\"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"Error: '{word}' not in vocabulary.\")\n",
    "        return None\n",
    "    return model.embeddings.weight[word_to_idx[word]]\n",
    "\n",
    "def find_most_similar(query, model, word_to_idx, idx_to_word, top_k=10):\n",
    "    \"\"\"\n",
    "    Finds the top_k most similar words to a given query.\n",
    "    The query can be a word (str) or a vector (torch.Tensor).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Get the query embedding\n",
    "        if isinstance(query, str):\n",
    "            if query not in word_to_idx:\n",
    "                print(f\"Error: '{query}' not in vocabulary.\")\n",
    "                return\n",
    "            query_embedding = get_embedding(query, model, word_to_idx).unsqueeze(0)\n",
    "        elif isinstance(query, torch.Tensor):\n",
    "            query_embedding = query.unsqueeze(0)\n",
    "        else:\n",
    "            print(\"Error: Query must be a string or a tensor.\")\n",
    "            return\n",
    "\n",
    "        # 2. Get the full embedding matrix\n",
    "        all_embeddings = model.embeddings.weight\n",
    "\n",
    "        # 3. Compute cosine similarities\n",
    "        cos_similarities = F.cosine_similarity(query_embedding, all_embeddings, dim=1)\n",
    "\n",
    "        # 4. Find the top K results\n",
    "        top_results = torch.topk(cos_similarities, k=top_k + 1)\n",
    "        \n",
    "        # 5. Print the results\n",
    "        # If the query was a word, we skip the first result (the word itself)\n",
    "        start_index = 1 if isinstance(query, str) else 0\n",
    "        for i in range(start_index, top_k + start_index):\n",
    "            idx = top_results.indices[i].item()\n",
    "            score = top_results.values[i].item()\n",
    "            print(f\"  - {idx_to_word[idx]:<15} (Similarity: {score:.4f})\")\n",
    "\n",
    "def find_analogy(pos, neg, model, word_to_idx, idx_to_word, top_k=5):\n",
    "    \"\"\"\n",
    "    Finds analogies like \"king is to man as [?] is to woman\".\n",
    "    Usage: find_analogy(pos=['king', 'woman'], neg=['man'], ...)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pos_vecs = [get_embedding(word, model, word_to_idx) for word in pos]\n",
    "        neg_vecs = [get_embedding(word, model, word_to_idx) for word in neg]\n",
    "\n",
    "        if any(v is None for v in pos_vecs) or any(v is None for v in neg_vecs):\n",
    "            return\n",
    "\n",
    "        # Perform the vector arithmetic\n",
    "        result_vec = sum(pos_vecs) - sum(neg_vecs)\n",
    "        \n",
    "        print(f\"Finding analogy for: {' + '.join(pos)} - {' - '.join(neg)}\")\n",
    "        find_most_similar(result_vec, model, word_to_idx, idx_to_word, top_k=top_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "83fd2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Section 1: Sanity Checks ---\n",
      "\n",
      "Similarity between 'movie' and 'film': 0.3148\n",
      "Similarity between 'actor' and 'actress': 0.0758\n",
      "Similarity between 'movie' and 'chair': 0.0026\n",
      "\n",
      "--- Section 2: Finding Nearest Neighbors ---\n",
      "\n",
      "Most similar to 'good':\n",
      "  - thin            (Similarity: 0.3959)\n",
      "  - pitiful         (Similarity: 0.3845)\n",
      "  - slightly        (Similarity: 0.3587)\n",
      "  - bathing         (Similarity: 0.3364)\n",
      "  - torturers       (Similarity: 0.3363)\n",
      "--------------------\n",
      "Most similar to 'bad':\n",
      "  - amadeus         (Similarity: 0.3828)\n",
      "  - delve           (Similarity: 0.3614)\n",
      "  - retirony        (Similarity: 0.3480)\n",
      "  - rosenstrasse    (Similarity: 0.3459)\n",
      "  - purse           (Similarity: 0.3434)\n",
      "--------------------\n",
      "Most similar to 'woman':\n",
      "  - analysing       (Similarity: 0.4166)\n",
      "  - characteratures (Similarity: 0.3989)\n",
      "  - fornication     (Similarity: 0.3931)\n",
      "  - elite           (Similarity: 0.3898)\n",
      "  - blacks          (Similarity: 0.3749)\n",
      "--------------------\n",
      "\n",
      "--- Section 3: Word Analogies ---\n",
      "\n",
      "Finding analogy for: woman + actor - man\n",
      "  - actor           (Similarity: 0.6455)\n",
      "  - woman           (Similarity: 0.6262)\n",
      "  - kids            (Similarity: 0.3527)\n",
      "  - glitches        (Similarity: 0.3480)\n",
      "  - checks          (Similarity: 0.3358)\n",
      "--------------------\n",
      "Finding analogy for: director + book - movie\n",
      "  - director        (Similarity: 0.6539)\n",
      "  - book            (Similarity: 0.5679)\n",
      "  - gigolo          (Similarity: 0.3892)\n",
      "  - olivia          (Similarity: 0.3867)\n",
      "  - whirlwind       (Similarity: 0.3614)\n",
      "--------------------\n",
      "Finding analogy for: best + bad - good\n",
      "  - best            (Similarity: 0.5809)\n",
      "  - bad             (Similarity: 0.5485)\n",
      "  - dismantling     (Similarity: 0.3826)\n",
      "  - strangers       (Similarity: 0.3764)\n",
      "  - craze           (Similarity: 0.3715)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# == Section 1: Sanity Checks (Basic Similarities)    ==\n",
    "# ======================================================\n",
    "print(\"--- Section 1: Sanity Checks ---\\n\")\n",
    "\n",
    "# Test 1: Synonyms. 'movie' and 'film' should be very similar.\n",
    "vec1 = get_embedding('movie', model, word_to_idx)\n",
    "vec2 = get_embedding('film', model, word_to_idx)\n",
    "if vec1 is not None and vec2 is not None:\n",
    "    sim = F.cosine_similarity(vec1, vec2, dim=0).item()\n",
    "    print(f\"Similarity between 'movie' and 'film': {sim:.4f}\")\n",
    "\n",
    "# Test 2: Related concepts. 'actor' and 'actress' should be similar.\n",
    "vec1 = get_embedding('actor', model, word_to_idx)\n",
    "vec2 = get_embedding('actress', model, word_to_idx)\n",
    "if vec1 is not None and vec2 is not None:\n",
    "    sim = F.cosine_similarity(vec1, vec2, dim=0).item()\n",
    "    print(f\"Similarity between 'actor' and 'actress': {sim:.4f}\")\n",
    "\n",
    "# Test 3: Unrelated concepts. 'movie' and 'chair' should have low similarity.\n",
    "vec1 = get_embedding('movie', model, word_to_idx)\n",
    "vec2 = get_embedding('chair', model, word_to_idx)\n",
    "if vec1 is not None and vec2 is not None:\n",
    "    sim = F.cosine_similarity(vec1, vec2, dim=0).item()\n",
    "    print(f\"Similarity between 'movie' and 'chair': {sim:.4f}\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# == Section 2: Finding Nearest Neighbors             ==\n",
    "# ======================================================\n",
    "print(\"--- Section 2: Finding Nearest Neighbors ---\\n\")\n",
    "\n",
    "print(\"Most similar to 'good':\")\n",
    "find_most_similar('good', model, word_to_idx, idx_to_word, top_k=5)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"Most similar to 'bad':\")\n",
    "find_most_similar('bad', model, word_to_idx, idx_to_word, top_k=5)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"Most similar to 'woman':\")\n",
    "find_most_similar('woman', model, word_to_idx, idx_to_word, top_k=5)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# == Section 3: Word Analogies                        ==\n",
    "# ======================================================\n",
    "# This is a very hard task and may not work perfectly with our small model,\n",
    "# but it's fun to try!\n",
    "\n",
    "print(\"\\n--- Section 3: Word Analogies ---\\n\")\n",
    "\n",
    "# Analogy 1: man -> woman :: actor -> ? (expecting 'actress')\n",
    "find_analogy(pos=['woman', 'actor'], neg=['man'], model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Analogy 2: movie -> director :: book -> ? (expecting 'author' or 'writer')\n",
    "find_analogy(pos=['director', 'book'], neg=['movie'], model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Analogy 3: good -> best :: bad -> ? (expecting 'worst')\n",
    "find_analogy(pos=['best', 'bad'], neg=['good'], model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64c60906e47f34",
   "metadata": {},
   "source": [
    "## Task 3: Cosine Similarity\n",
    "Make sure that you have installed the package gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b9a60381",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e5a0ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at: /Users/s.mallet/passau/dlnlp/.venv\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 91ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 0.71ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 96ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 70ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.3 environment at: /Users/s.mallet/passau/dlnlp/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 232ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/4.88 MiB            \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 16.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 32.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 48.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 64.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 80.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 96.00 KiB/4.88 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 112.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 128.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 144.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 160.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 176.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 192.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 208.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 224.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 240.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 256.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 272.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 288.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 304.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 603.00 KiB/4.88 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.53 MiB/4.88 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 2.32 MiB/4.88 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)2m------------\u001b[0m\u001b[0m 2.80 MiB/4.88 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 443ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.3 environment at: /Users/s.mallet/passau/dlnlp/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 365ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m     0 B/60.49 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 16.00 KiB/60.49 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 32.00 KiB/60.49 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 32.00 KiB/60.49 KiB         \u001b[1A\n",
      "\u001b[2msmart-open          \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/60.49 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m     0 B/22.92 MiB           \u001b[2A\n",
      "\u001b[2msmart-open          \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 48.00 KiB/60.49 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m     0 B/22.92 MiB           \u001b[2A\n",
      "\u001b[2msmart-open          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 60.49 KiB/60.49 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m     0 B/22.92 MiB           \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m     0 B/22.92 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 16.00 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 32.00 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 48.00 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2mwrapt               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/38.01 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[2A\n",
      "\u001b[2mwrapt               \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/38.01 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[2A\n",
      "\u001b[2mwrapt               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 32.00 KiB/38.01 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[2A\n",
      "\u001b[2mwrapt               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 38.01 KiB/38.01 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 61.99 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 77.99 KiB/22.92 MiB         \u001b[1A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 64.00 KiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 157.99 KiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)--------------\u001b[0m\u001b[0m 32.00 KiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 377.81 KiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 461.99 KiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 399.89 KiB/28.96 MiB        \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 649.81 KiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 717.99 KiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 656.00 KiB/28.96 MiB        \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 649.81 KiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 717.99 KiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 672.00 KiB/28.96 MiB        \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 836.81 KiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 909.99 KiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 864.00 KiB/28.96 MiB        \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.19 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.34 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 1.12 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.57 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.71 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 1.50 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.86 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.95 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 1.84 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.33 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.48 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 2.26 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.72 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 2.88 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 2.66 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 3.05 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 3.18 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 2.97 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 3.40 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 3.57 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 3.36 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.79 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 3.93 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 3.72 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.82 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 3.98 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 3.76 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.88 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 4.04 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 3.83 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 4.30 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 4.46 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 4.26 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.60 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 4.76 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 4.56 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 4.82 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 4.99 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 4.78 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 4.96 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 5.13 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 4.94 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 5.40 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 5.57 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 5.37 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.85 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 6.01 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 5.83 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 6.32 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 6.49 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 6.31 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.77 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 6.92 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 6.75 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 7.14 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 7.31 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 7.08 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 7.50 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 7.61 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 7.42 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 7.52 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 7.61 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 7.42 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 8.36 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 8.78 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 8.14 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 8.84 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 9.28 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 8.84 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 9.42 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 9.58 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 9.37 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 9.97 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 10.19 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 9.64 MiB/28.96 MiB          \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 10.21 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 10.53 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 10.24 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 10.53 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 10.80 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 10.50 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 10.73 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 11.00 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 10.64 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 11.00 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 11.37 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 10.77 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 11.24 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 11.84 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 11.20 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 11.80 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 12.11 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 11.89 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 12.08 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 12.48 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 12.56 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mnumpy               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.86 MiB/13.05 MiB\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 13.09 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 12.64 MiB/28.96 MiB         \u001b[3A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 13.83 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 13.08 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 13.83 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)--------------\u001b[0m\u001b[0m 13.09 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.52 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)--------------\u001b[0m\u001b[0m 14.00 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 15.19 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)--------------\u001b[0m\u001b[0m 14.55 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 15.91 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)--------------\u001b[0m\u001b[0m 15.34 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.55 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)m-------------\u001b[0m\u001b[0m 16.12 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 17.48 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)2m------------\u001b[0m\u001b[0m 16.69 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 17.86 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)[2m-----------\u001b[0m\u001b[0m 17.71 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 19.09 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)[2m-----------\u001b[0m\u001b[0m 18.09 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 19.29 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\u001b[2m----------\u001b[0m\u001b[0m 19.14 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 20.33 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)-\u001b[2m---------\u001b[0m\u001b[0m 19.43 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 20.67 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)--\u001b[2m--------\u001b[0m\u001b[0m 20.43 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 21.64 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)--\u001b[2m--------\u001b[0m\u001b[0m 20.79 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 22.08 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)---\u001b[2m-------\u001b[0m\u001b[0m 21.80 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2mgensim              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.75 MiB/22.92 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)----\u001b[2m------\u001b[0m\u001b[0m 22.45 MiB/28.96 MiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)----\u001b[2m------\u001b[0m\u001b[0m 22.84 MiB/28.96 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)-----\u001b[2m-----\u001b[0m\u001b[0m 23.69 MiB/28.96 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)------\u001b[2m----\u001b[0m\u001b[0m 24.92 MiB/28.96 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (4/5)--------\u001b[2m--\u001b[0m\u001b[0m 26.29 MiB/28.96 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 2.85s\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m5 packages\u001b[0m \u001b[2min 525ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mgensim\u001b[0m\u001b[2m==4.3.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1msmart-open\u001b[0m\u001b[2m==7.3.0.post1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.17.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Try to fix numpy if needed, then install gensim\n",
    "!uv pip install --upgrade numpy\n",
    "!uv pip install --force-reinstall --no-cache-dir numpy\n",
    "!uv pip install --upgrade --force-reinstall --no-cache-dir gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dae65ed8b9b4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04ce26e1618203",
   "metadata": {},
   "source": [
    "### Task 3 (a): Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f0c2f644d52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    \n",
    "    Formula: cos(theta) = (x . y) / (||x|| * ||y||)\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): The first vector.\n",
    "        y (np.ndarray): The second vector.\n",
    "        \n",
    "    Returns:\n",
    "        float: The cosine similarity, a value between -1 and 1.\n",
    "    \"\"\"\n",
    "    # Ensure the vectors are numpy arrays\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Compute the dot product\n",
    "    dot_product = dot(x, y)\n",
    "    \n",
    "    # Compute the L2 norms (magnitudes)\n",
    "    norm_x = norm(x)\n",
    "    norm_y = norm(y)\n",
    "    \n",
    "    # Handle the case of zero-vectors to avoid division by zero\n",
    "    if norm_x == 0 or norm_y == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product / (norm_x * norm_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dd143b9fb14c8",
   "metadata": {},
   "source": [
    "### Task 3 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b85a015e73c0e2",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d61bef439acf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bbb0bb5095b684",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'queen' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m king_vector_m1 = model1.get_vector(\u001b[33m'\u001b[39m\u001b[33mking\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m queen_vector_m1 = \u001b[43mmodel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqueen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m man_vector_m1 = model1.get_vector(\u001b[33m'\u001b[39m\u001b[33mman\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m woman_vector_m1 = model1.get_vector(\u001b[33m'\u001b[39m\u001b[33mwoman\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:446\u001b[39m, in \u001b[36mKeyedVectors.get_vector\u001b[39m\u001b[34m(self, key, norm)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    423\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[32m    424\u001b[39m \n\u001b[32m    425\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m \n\u001b[32m    445\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[32m    448\u001b[39m         \u001b[38;5;28mself\u001b[39m.fill_norms()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:420\u001b[39m, in \u001b[36mKeyedVectors.get_index\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'queen' not present\""
     ]
    }
   ],
   "source": [
    "king_vector_m1 = model1.get_vector('king')\n",
    "queen_vector_m1 = model1.get_vector('queen')\n",
    "man_vector_m1 = model1.get_vector('man')\n",
    "woman_vector_m1 = model1.get_vector('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc59d8b29bfa794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'to': 1,\n",
       " 'of': 2,\n",
       " 'in': 3,\n",
       " 'and': 4,\n",
       " 'he': 5,\n",
       " 'is': 6,\n",
       " 'for': 7,\n",
       " 'on': 8,\n",
       " 'said': 9,\n",
       " 'that': 10,\n",
       " 'has': 11,\n",
       " 'says': 12,\n",
       " 'was': 13,\n",
       " 'have': 14,\n",
       " 'it': 15,\n",
       " 'be': 16,\n",
       " 'are': 17,\n",
       " 'with': 18,\n",
       " 'will': 19,\n",
       " 'at': 20,\n",
       " 'mr': 21,\n",
       " 'from': 22,\n",
       " 'by': 23,\n",
       " 'we': 24,\n",
       " 'been': 25,\n",
       " 'as': 26,\n",
       " 'an': 27,\n",
       " 'not': 28,\n",
       " 'his': 29,\n",
       " 'but': 30,\n",
       " 'they': 31,\n",
       " 'after': 32,\n",
       " 'were': 33,\n",
       " 'had': 34,\n",
       " 'there': 35,\n",
       " 'new': 36,\n",
       " 'this': 37,\n",
       " 'australia': 38,\n",
       " 'australian': 39,\n",
       " 'who': 40,\n",
       " 'people': 41,\n",
       " 'palestinian': 42,\n",
       " 'their': 43,\n",
       " 'two': 44,\n",
       " 'government': 45,\n",
       " 'up': 46,\n",
       " 'south': 47,\n",
       " 'us': 48,\n",
       " 'which': 49,\n",
       " 'year': 50,\n",
       " 'one': 51,\n",
       " 'about': 52,\n",
       " 'out': 53,\n",
       " 'if': 54,\n",
       " 'also': 55,\n",
       " 'more': 56,\n",
       " 'when': 57,\n",
       " 'its': 58,\n",
       " 'would': 59,\n",
       " 'into': 60,\n",
       " 'first': 61,\n",
       " 'last': 62,\n",
       " 'against': 63,\n",
       " 'israeli': 64,\n",
       " 'minister': 65,\n",
       " 'arafat': 66,\n",
       " 'all': 67,\n",
       " 'over': 68,\n",
       " 'afghanistan': 69,\n",
       " 'three': 70,\n",
       " 'united': 71,\n",
       " 'no': 72,\n",
       " 'world': 73,\n",
       " 'or': 74,\n",
       " 'police': 75,\n",
       " 'than': 76,\n",
       " 'before': 77,\n",
       " 'attacks': 78,\n",
       " 'fire': 79,\n",
       " 'day': 80,\n",
       " 'security': 81,\n",
       " 'some': 82,\n",
       " 'states': 83,\n",
       " 'you': 84,\n",
       " 'them': 85,\n",
       " 'could': 86,\n",
       " 'today': 87,\n",
       " 'say': 88,\n",
       " 'told': 89,\n",
       " 'now': 90,\n",
       " 'time': 91,\n",
       " 'any': 92,\n",
       " 'very': 93,\n",
       " 'laden': 94,\n",
       " 'just': 95,\n",
       " 'bin': 96,\n",
       " 'can': 97,\n",
       " 'what': 98,\n",
       " 'sydney': 99,\n",
       " 'still': 100,\n",
       " 'president': 101,\n",
       " 'company': 102,\n",
       " 'four': 103,\n",
       " 'man': 104,\n",
       " 'killed': 105,\n",
       " 'taliban': 106,\n",
       " 'al': 107,\n",
       " 'our': 108,\n",
       " 'forces': 109,\n",
       " 'around': 110,\n",
       " 'days': 111,\n",
       " 'being': 112,\n",
       " 'west': 113,\n",
       " 'old': 114,\n",
       " 'other': 115,\n",
       " 'where': 116,\n",
       " 'so': 117,\n",
       " 'officials': 118,\n",
       " 'test': 119,\n",
       " 'israel': 120,\n",
       " 'qaeda': 121,\n",
       " 'per': 122,\n",
       " 'think': 123,\n",
       " 'federal': 124,\n",
       " 'next': 125,\n",
       " 'general': 126,\n",
       " 'force': 127,\n",
       " 'she': 128,\n",
       " 'cent': 129,\n",
       " 'yesterday': 130,\n",
       " 'leader': 131,\n",
       " 'workers': 132,\n",
       " 'hamas': 133,\n",
       " 'take': 134,\n",
       " 'under': 135,\n",
       " 'him': 136,\n",
       " 'state': 137,\n",
       " 'meeting': 138,\n",
       " 'back': 139,\n",
       " 'bank': 140,\n",
       " 'years': 141,\n",
       " 'suicide': 142,\n",
       " 'those': 143,\n",
       " 'made': 144,\n",
       " 'action': 145,\n",
       " 'down': 146,\n",
       " 'morning': 147,\n",
       " 'commission': 148,\n",
       " 'pakistan': 149,\n",
       " 're': 150,\n",
       " 'international': 151,\n",
       " 'attack': 152,\n",
       " 'afghan': 153,\n",
       " 'group': 154,\n",
       " 'city': 155,\n",
       " 'centre': 156,\n",
       " 'military': 157,\n",
       " 'well': 158,\n",
       " 'number': 159,\n",
       " 'members': 160,\n",
       " 'through': 161,\n",
       " 'while': 162,\n",
       " 'qantas': 163,\n",
       " 'area': 164,\n",
       " 'called': 165,\n",
       " 'five': 166,\n",
       " 'local': 167,\n",
       " 'national': 168,\n",
       " 'union': 169,\n",
       " 'gaza': 170,\n",
       " 'week': 171,\n",
       " 'since': 172,\n",
       " 'september': 173,\n",
       " 'including': 174,\n",
       " 'wales': 175,\n",
       " 'hours': 176,\n",
       " 'east': 177,\n",
       " 'another': 178,\n",
       " 'should': 179,\n",
       " 'report': 180,\n",
       " 'night': 181,\n",
       " 'north': 182,\n",
       " 'off': 183,\n",
       " 'these': 184,\n",
       " 'six': 185,\n",
       " 'war': 186,\n",
       " 'second': 187,\n",
       " 'between': 188,\n",
       " 'staff': 189,\n",
       " 'get': 190,\n",
       " 'earlier': 191,\n",
       " 'go': 192,\n",
       " 'months': 193,\n",
       " 'further': 194,\n",
       " 'islamic': 195,\n",
       " 'defence': 196,\n",
       " 'end': 197,\n",
       " 'do': 198,\n",
       " 'because': 199,\n",
       " 'power': 200,\n",
       " 'team': 201,\n",
       " 'authority': 202,\n",
       " 'near': 203,\n",
       " 'foreign': 204,\n",
       " 'going': 205,\n",
       " 'areas': 206,\n",
       " 'sharon': 207,\n",
       " 'work': 208,\n",
       " 'during': 209,\n",
       " 'india': 210,\n",
       " 'eight': 211,\n",
       " 'only': 212,\n",
       " 'died': 213,\n",
       " 'many': 214,\n",
       " 'way': 215,\n",
       " 'month': 216,\n",
       " 'know': 217,\n",
       " 'former': 218,\n",
       " 'air': 219,\n",
       " 'claims': 220,\n",
       " 'match': 221,\n",
       " 'melbourne': 222,\n",
       " 'good': 223,\n",
       " 'left': 224,\n",
       " 'metres': 225,\n",
       " 've': 226,\n",
       " 'northern': 227,\n",
       " 'spokesman': 228,\n",
       " 'make': 229,\n",
       " 'prime': 230,\n",
       " 'most': 231,\n",
       " 'like': 232,\n",
       " 'support': 233,\n",
       " 'osama': 234,\n",
       " 'peace': 235,\n",
       " 'authorities': 236,\n",
       " 'set': 237,\n",
       " 'ago': 238,\n",
       " 'am': 239,\n",
       " 'given': 240,\n",
       " 'expected': 241,\n",
       " 'saying': 242,\n",
       " 'come': 243,\n",
       " 'place': 244,\n",
       " 'militants': 245,\n",
       " 'put': 246,\n",
       " 'tora': 247,\n",
       " 'bora': 248,\n",
       " 'looking': 249,\n",
       " 'several': 250,\n",
       " 'fighters': 251,\n",
       " 'children': 252,\n",
       " 'troops': 253,\n",
       " 'meanwhile': 254,\n",
       " 'indian': 255,\n",
       " 'unions': 256,\n",
       " 'groups': 257,\n",
       " 'africa': 258,\n",
       " 'child': 259,\n",
       " 'christmas': 260,\n",
       " 'arrested': 261,\n",
       " 'river': 262,\n",
       " 'royal': 263,\n",
       " 'injured': 264,\n",
       " 'found': 265,\n",
       " 'then': 266,\n",
       " 'talks': 267,\n",
       " 'yasser': 268,\n",
       " 'hospital': 269,\n",
       " 'terrorist': 270,\n",
       " 'interim': 271,\n",
       " 'official': 272,\n",
       " 'part': 273,\n",
       " 'whether': 274,\n",
       " 'reports': 275,\n",
       " 'economy': 276,\n",
       " 'early': 277,\n",
       " 'don': 278,\n",
       " 'third': 279,\n",
       " 'leaders': 280,\n",
       " 'statement': 281,\n",
       " 'industrial': 282,\n",
       " 'how': 283,\n",
       " 'terrorism': 284,\n",
       " 'mountains': 285,\n",
       " 'start': 286,\n",
       " 'senior': 287,\n",
       " 'family': 288,\n",
       " 'both': 289,\n",
       " 'public': 290,\n",
       " 'million': 291,\n",
       " 'weather': 292,\n",
       " 'john': 293,\n",
       " 'believe': 294,\n",
       " 'radio': 295,\n",
       " 'pay': 296,\n",
       " 'trying': 297,\n",
       " 'army': 298,\n",
       " 'court': 299,\n",
       " 'hit': 300,\n",
       " 'pressure': 301,\n",
       " 'however': 302,\n",
       " 'lead': 303,\n",
       " 'chief': 304,\n",
       " 'asked': 305,\n",
       " 'control': 306,\n",
       " 'long': 307,\n",
       " 'best': 308,\n",
       " 'following': 309,\n",
       " 'agreement': 310,\n",
       " 'help': 311,\n",
       " 'adelaide': 312,\n",
       " 'dr': 313,\n",
       " 'few': 314,\n",
       " 'labor': 315,\n",
       " 'australians': 316,\n",
       " 'firefighters': 317,\n",
       " 'high': 318,\n",
       " 'house': 319,\n",
       " 'community': 320,\n",
       " 'taken': 321,\n",
       " 'want': 322,\n",
       " 'close': 323,\n",
       " 'confirmed': 324,\n",
       " 'need': 325,\n",
       " 'queensland': 326,\n",
       " 'service': 327,\n",
       " 'arrest': 328,\n",
       " 'services': 329,\n",
       " 'better': 330,\n",
       " 'play': 331,\n",
       " 'overnight': 332,\n",
       " 'process': 333,\n",
       " 'does': 334,\n",
       " 'came': 335,\n",
       " 'must': 336,\n",
       " 'believed': 337,\n",
       " 'information': 338,\n",
       " 'opposition': 339,\n",
       " 'detainees': 340,\n",
       " 'secretary': 341,\n",
       " 'williams': 342,\n",
       " 'won': 343,\n",
       " 'took': 344,\n",
       " 'held': 345,\n",
       " 'released': 346,\n",
       " 'win': 347,\n",
       " 'accused': 348,\n",
       " 'damage': 349,\n",
       " 'brought': 350,\n",
       " 'governor': 351,\n",
       " 'maintenance': 352,\n",
       " 'did': 353,\n",
       " 'british': 354,\n",
       " 'much': 355,\n",
       " 'hicks': 356,\n",
       " 'shot': 357,\n",
       " 'nations': 358,\n",
       " 'possible': 359,\n",
       " 'her': 360,\n",
       " 'party': 361,\n",
       " 'pentagon': 362,\n",
       " 'peter': 363,\n",
       " 'building': 364,\n",
       " 'airline': 365,\n",
       " 'even': 366,\n",
       " 'return': 367,\n",
       " 'armed': 368,\n",
       " 'asylum': 369,\n",
       " 'lot': 370,\n",
       " 'eastern': 371,\n",
       " 'winds': 372,\n",
       " 'council': 373,\n",
       " 'conditions': 374,\n",
       " 'such': 375,\n",
       " 'york': 376,\n",
       " 'despite': 377,\n",
       " 'got': 378,\n",
       " 'hill': 379,\n",
       " 'change': 380,\n",
       " 'cut': 381,\n",
       " 'safety': 382,\n",
       " 'director': 383,\n",
       " 'without': 384,\n",
       " 'dead': 385,\n",
       " 'across': 386,\n",
       " 'weekend': 387,\n",
       " 'violence': 388,\n",
       " 'kandahar': 389,\n",
       " 'news': 390,\n",
       " 'cricket': 391,\n",
       " 'men': 392,\n",
       " 'lee': 393,\n",
       " 'emergency': 394,\n",
       " 'far': 395,\n",
       " 'crew': 396,\n",
       " 'aircraft': 397,\n",
       " 'strong': 398,\n",
       " 'david': 399,\n",
       " 'here': 400,\n",
       " 'trade': 401,\n",
       " 'fighting': 402,\n",
       " 'strip': 403,\n",
       " 'board': 404,\n",
       " 'palestinians': 405,\n",
       " 'role': 406,\n",
       " 'monday': 407,\n",
       " 'american': 408,\n",
       " 'home': 409,\n",
       " 'see': 410,\n",
       " 'region': 411,\n",
       " 'working': 412,\n",
       " 'captured': 413,\n",
       " 'training': 414,\n",
       " 'anti': 415,\n",
       " 'continuing': 416,\n",
       " 'race': 417,\n",
       " 'december': 418,\n",
       " 'call': 419,\n",
       " 'economic': 420,\n",
       " 'too': 421,\n",
       " 'southern': 422,\n",
       " 'fires': 423,\n",
       " 'country': 424,\n",
       " 'waugh': 425,\n",
       " 'seekers': 426,\n",
       " 'administration': 427,\n",
       " 'industry': 428,\n",
       " 'zinni': 429,\n",
       " 'charged': 430,\n",
       " 'water': 431,\n",
       " 'health': 432,\n",
       " 'used': 433,\n",
       " 'alliance': 434,\n",
       " 'george': 435,\n",
       " 'plans': 436,\n",
       " 'bush': 437,\n",
       " 'bureau': 438,\n",
       " 'act': 439,\n",
       " 'received': 440,\n",
       " 'key': 441,\n",
       " 'offer': 442,\n",
       " 'head': 443,\n",
       " 'past': 444,\n",
       " 'rate': 445,\n",
       " 'leading': 446,\n",
       " 'station': 447,\n",
       " 'risk': 448,\n",
       " 'large': 449,\n",
       " 'department': 450,\n",
       " 'airport': 451,\n",
       " 'legal': 452,\n",
       " 'britain': 453,\n",
       " 'zealand': 454,\n",
       " 'boat': 455,\n",
       " 'least': 456,\n",
       " 'hundreds': 457,\n",
       " 'issue': 458,\n",
       " 'town': 459,\n",
       " 'person': 460,\n",
       " 'decision': 461,\n",
       " 'soldiers': 462,\n",
       " 'known': 463,\n",
       " 'stop': 464,\n",
       " 'within': 465,\n",
       " 'operations': 466,\n",
       " 'captain': 467,\n",
       " 'israelis': 468,\n",
       " 'hih': 469,\n",
       " 'parliament': 470,\n",
       " 'major': 471,\n",
       " 'strikes': 472,\n",
       " 'downer': 473,\n",
       " 'final': 474,\n",
       " 'use': 475,\n",
       " 'your': 476,\n",
       " 'line': 477,\n",
       " 'may': 478,\n",
       " 'network': 479,\n",
       " 'late': 480,\n",
       " 'ahead': 481,\n",
       " 'homes': 482,\n",
       " 'shane': 483,\n",
       " 'great': 484,\n",
       " 'officers': 485,\n",
       " 'kabul': 486,\n",
       " 'remain': 487,\n",
       " 'right': 488,\n",
       " 'western': 489,\n",
       " 'half': 490,\n",
       " 'give': 491,\n",
       " 'due': 492,\n",
       " 'un': 493,\n",
       " 'my': 494,\n",
       " 'laws': 495,\n",
       " 'might': 496,\n",
       " 'plane': 497,\n",
       " 'taking': 498,\n",
       " 'interest': 499,\n",
       " 'weapons': 500,\n",
       " 'series': 501,\n",
       " 'latest': 502,\n",
       " 'every': 503,\n",
       " 'pm': 504,\n",
       " 'hollingworth': 505,\n",
       " 'coast': 506,\n",
       " 'policy': 507,\n",
       " 'position': 508,\n",
       " 'later': 509,\n",
       " 'death': 510,\n",
       " 'able': 511,\n",
       " 'tomorrow': 512,\n",
       " 'behind': 513,\n",
       " 'already': 514,\n",
       " 'flight': 515,\n",
       " 'special': 516,\n",
       " 'storm': 517,\n",
       " 'washington': 518,\n",
       " 'heard': 519,\n",
       " 'really': 520,\n",
       " 'others': 521,\n",
       " 'forced': 522,\n",
       " 'point': 523,\n",
       " 'failed': 524,\n",
       " 'hard': 525,\n",
       " 'continue': 526,\n",
       " 'same': 527,\n",
       " 'territory': 528,\n",
       " 'jihad': 529,\n",
       " 'concerned': 530,\n",
       " 'jobs': 531,\n",
       " 'me': 532,\n",
       " 'victory': 533,\n",
       " 'cup': 534,\n",
       " 'event': 535,\n",
       " 'campaign': 536,\n",
       " 'guilty': 537,\n",
       " 'deaths': 538,\n",
       " 'side': 539,\n",
       " 'seen': 540,\n",
       " 'thought': 541,\n",
       " 'along': 542,\n",
       " 'towards': 543,\n",
       " 'abuse': 544,\n",
       " 'bill': 545,\n",
       " 'life': 546,\n",
       " 'timor': 547,\n",
       " 'cabinet': 548,\n",
       " 'bichel': 549,\n",
       " 'case': 550,\n",
       " 'weeks': 551,\n",
       " 'matter': 552,\n",
       " 'move': 553,\n",
       " 'bombings': 554,\n",
       " 'again': 555,\n",
       " 'middle': 556,\n",
       " 'capital': 557,\n",
       " 'envoy': 558,\n",
       " 'member': 559,\n",
       " 'november': 560,\n",
       " 'countries': 561,\n",
       " 'details': 562,\n",
       " 'space': 563,\n",
       " 'woomera': 564,\n",
       " 'warne': 565,\n",
       " 'detention': 566,\n",
       " 'bowler': 567,\n",
       " 'sunday': 568,\n",
       " 'seven': 569,\n",
       " 'likely': 570,\n",
       " 'situation': 571,\n",
       " 'according': 572,\n",
       " 'buildings': 573,\n",
       " 'mark': 574,\n",
       " 'rates': 575,\n",
       " 'canyoning': 576,\n",
       " 'th': 577,\n",
       " 'bombing': 578,\n",
       " 'hour': 579,\n",
       " 'enough': 580,\n",
       " 'innings': 581,\n",
       " 'important': 582,\n",
       " 'mcgrath': 583,\n",
       " 'launched': 584,\n",
       " 'bus': 585,\n",
       " 'helicopters': 586,\n",
       " 'perth': 587,\n",
       " 'young': 588,\n",
       " 'asio': 589,\n",
       " 'caught': 590,\n",
       " 'movement': 591,\n",
       " 'human': 592,\n",
       " 'money': 593,\n",
       " 'sure': 594,\n",
       " 'own': 595,\n",
       " 'women': 596,\n",
       " 'run': 597,\n",
       " 'disease': 598,\n",
       " 'top': 599,\n",
       " 'immediately': 600,\n",
       " 'dispute': 601,\n",
       " 'cancer': 602,\n",
       " 'adventure': 603,\n",
       " 'targets': 604,\n",
       " 'added': 605,\n",
       " 'aedt': 606,\n",
       " 'show': 607,\n",
       " 'swiss': 608,\n",
       " 'ms': 609,\n",
       " 'guides': 610,\n",
       " 'african': 611,\n",
       " 'raids': 612,\n",
       " 'wants': 613,\n",
       " 'mission': 614,\n",
       " 'opened': 615,\n",
       " 'political': 616,\n",
       " 'warplanes': 617,\n",
       " 'forward': 618,\n",
       " 'reported': 619,\n",
       " 'based': 620,\n",
       " 'rule': 621,\n",
       " 'claimed': 622,\n",
       " 'jail': 623,\n",
       " 'freeze': 624,\n",
       " 'office': 625,\n",
       " 'justice': 626,\n",
       " 'blue': 627,\n",
       " 'deal': 628,\n",
       " 'carried': 629,\n",
       " 'evidence': 630,\n",
       " 'commonwealth': 631,\n",
       " 'boy': 632,\n",
       " 'full': 633,\n",
       " 'certainly': 634,\n",
       " 'become': 635,\n",
       " 'allegations': 636,\n",
       " 'although': 637,\n",
       " 'proposed': 638,\n",
       " 'burning': 639,\n",
       " 'always': 640,\n",
       " 'sector': 641,\n",
       " 'ground': 642,\n",
       " 'break': 643,\n",
       " 'find': 644,\n",
       " 'planning': 645,\n",
       " 'probably': 646,\n",
       " 'flying': 647,\n",
       " 'each': 648,\n",
       " 'result': 649,\n",
       " 'march': 650,\n",
       " 'order': 651,\n",
       " 'access': 652,\n",
       " 'collapse': 653,\n",
       " 'job': 654,\n",
       " 'island': 655,\n",
       " 'face': 656,\n",
       " 'border': 657,\n",
       " 'banks': 658,\n",
       " 'wicket': 659,\n",
       " 'financial': 660,\n",
       " 'prepared': 661,\n",
       " 'reached': 662,\n",
       " 'beat': 663,\n",
       " 'using': 664,\n",
       " 'growth': 665,\n",
       " 'investigation': 666,\n",
       " 'times': 667,\n",
       " 'carrying': 668,\n",
       " 'crash': 669,\n",
       " 'tourists': 670,\n",
       " 'drop': 671,\n",
       " 'tried': 672,\n",
       " 'himself': 673,\n",
       " 'serious': 674,\n",
       " 'bid': 675,\n",
       " 'short': 676,\n",
       " 'surrender': 677,\n",
       " 'believes': 678,\n",
       " 'energy': 679,\n",
       " 'needs': 680,\n",
       " 'thursday': 681,\n",
       " 'road': 682,\n",
       " 'reserve': 683,\n",
       " 'organisation': 684,\n",
       " 'suspected': 685,\n",
       " 'stage': 686,\n",
       " 'calls': 687,\n",
       " 'fight': 688,\n",
       " 'travel': 689,\n",
       " 'responsibility': 690,\n",
       " 'until': 691,\n",
       " 'militant': 692,\n",
       " 'sex': 693,\n",
       " 'quickly': 694,\n",
       " 'management': 695,\n",
       " 'harrison': 696,\n",
       " 'future': 697,\n",
       " 'thousands': 698,\n",
       " 'lives': 699,\n",
       " 'senator': 700,\n",
       " 'bowling': 701,\n",
       " 'immigration': 702,\n",
       " 'howard': 703,\n",
       " 'trees': 704,\n",
       " 'ariel': 705,\n",
       " 'went': 706,\n",
       " 'comes': 707,\n",
       " 'opening': 708,\n",
       " 'wave': 709,\n",
       " 'militia': 710,\n",
       " 'executive': 711,\n",
       " 'argentina': 712,\n",
       " 'inside': 713,\n",
       " 'killing': 714,\n",
       " 'relations': 715,\n",
       " 'outside': 716,\n",
       " 'sent': 717,\n",
       " 'ansett': 718,\n",
       " 'declared': 719,\n",
       " 'kilometres': 720,\n",
       " 'flights': 721,\n",
       " 'afp': 722,\n",
       " 'different': 723,\n",
       " 'alleged': 724,\n",
       " 'soon': 725,\n",
       " 'caves': 726,\n",
       " 'post': 727,\n",
       " 'rejected': 728,\n",
       " 'making': 729,\n",
       " 'allow': 730,\n",
       " 'clear': 731,\n",
       " 'program': 732,\n",
       " 'hewitt': 733,\n",
       " 'getting': 734,\n",
       " 'radical': 735,\n",
       " 'inquiry': 736,\n",
       " 'kallis': 737,\n",
       " 'running': 738,\n",
       " 'decided': 739,\n",
       " 'destroyed': 740,\n",
       " 'jewish': 741,\n",
       " 'biggest': 742,\n",
       " 'refused': 743,\n",
       " 'yacht': 744,\n",
       " 'list': 745,\n",
       " 'response': 746,\n",
       " 'july': 747,\n",
       " 'form': 748,\n",
       " 'witnesses': 749,\n",
       " 'yet': 750,\n",
       " 'increase': 751,\n",
       " 'school': 752,\n",
       " 'alexander': 753,\n",
       " 'look': 754,\n",
       " 'try': 755,\n",
       " 'tanks': 756,\n",
       " 'television': 757,\n",
       " 'quite': 758,\n",
       " 'address': 759,\n",
       " 'something': 760,\n",
       " 'actually': 761,\n",
       " 'attempt': 762,\n",
       " 'among': 763,\n",
       " 'available': 764,\n",
       " 'rights': 765,\n",
       " 'ministers': 766,\n",
       " 'accident': 767,\n",
       " 'rural': 768,\n",
       " 'terms': 769,\n",
       " 'meet': 770,\n",
       " 'agency': 771,\n",
       " 'happened': 772,\n",
       " 'brisbane': 773,\n",
       " 'parties': 774,\n",
       " 'warned': 775,\n",
       " 'means': 776,\n",
       " 'fighter': 777,\n",
       " 'source': 778,\n",
       " 'issues': 779,\n",
       " 'rise': 780,\n",
       " 'understand': 781,\n",
       " 'measures': 782,\n",
       " 'victoria': 783,\n",
       " 'anything': 784,\n",
       " 'offices': 785,\n",
       " 'karzai': 786,\n",
       " 'circumstances': 787,\n",
       " 'sea': 788,\n",
       " 'returned': 789,\n",
       " 'ruddock': 790,\n",
       " 'residents': 791,\n",
       " 'wounded': 792,\n",
       " 'done': 793,\n",
       " 'directors': 794,\n",
       " 'shortly': 795,\n",
       " 'annual': 796,\n",
       " 'changes': 797,\n",
       " 'parts': 798,\n",
       " 'gunmen': 799,\n",
       " 'donald': 800,\n",
       " 'sentence': 801,\n",
       " 'wickets': 802,\n",
       " 'figures': 803,\n",
       " 'ended': 804,\n",
       " 'bombers': 805,\n",
       " 'white': 806,\n",
       " 'sources': 807,\n",
       " 'currently': 808,\n",
       " 'open': 809,\n",
       " 'announced': 810,\n",
       " 'nine': 811,\n",
       " 'glenn': 812,\n",
       " 'law': 813,\n",
       " 'robert': 814,\n",
       " 'huge': 815,\n",
       " 'tennis': 816,\n",
       " 'sir': 817,\n",
       " 'rather': 818,\n",
       " 'beyond': 819,\n",
       " 'began': 820,\n",
       " 'rafter': 821,\n",
       " 'cost': 822,\n",
       " 'airlines': 823,\n",
       " 'expressed': 824,\n",
       " 'tour': 825,\n",
       " 'caused': 826,\n",
       " 'powell': 827,\n",
       " 'less': 828,\n",
       " 'virus': 829,\n",
       " 'beginning': 830,\n",
       " 'friday': 831,\n",
       " 'facility': 832,\n",
       " 'struck': 833,\n",
       " 'period': 834,\n",
       " 'nauru': 835,\n",
       " 'wage': 836,\n",
       " 'prevent': 837,\n",
       " 'car': 838,\n",
       " 'ever': 839,\n",
       " 'crowd': 840,\n",
       " 'difficult': 841,\n",
       " 'though': 842,\n",
       " 'confidence': 843,\n",
       " 'having': 844,\n",
       " 'musharraf': 845,\n",
       " 'gives': 846,\n",
       " 'media': 847,\n",
       " 'advice': 848,\n",
       " 'bomb': 849,\n",
       " 'negotiations': 850,\n",
       " 'fact': 851,\n",
       " 'll': 852,\n",
       " 'highway': 853,\n",
       " 'hobart': 854,\n",
       " 'arrived': 855,\n",
       " 'suharto': 856,\n",
       " 'pacific': 857,\n",
       " 'business': 858,\n",
       " 'expect': 859,\n",
       " 'attorney': 860,\n",
       " 'intelligence': 861,\n",
       " 'deputy': 862,\n",
       " 'blake': 863,\n",
       " 'involved': 864,\n",
       " 'total': 865,\n",
       " 'runs': 866,\n",
       " 'islands': 867,\n",
       " 'elected': 868,\n",
       " 'commissioner': 869,\n",
       " 'step': 870,\n",
       " 'strike': 871,\n",
       " 'supporters': 872,\n",
       " 'away': 873,\n",
       " 'worst': 874,\n",
       " 'ses': 875,\n",
       " 'afternoon': 876,\n",
       " 'market': 877,\n",
       " 'allan': 878,\n",
       " 'recession': 879,\n",
       " 'big': 880,\n",
       " 'gave': 881,\n",
       " 'followed': 882,\n",
       " 'commanders': 883,\n",
       " 'appeared': 884,\n",
       " 'field': 885,\n",
       " 'ensure': 886,\n",
       " 'whose': 887,\n",
       " 'commander': 888,\n",
       " 'feel': 889,\n",
       " 'martin': 890,\n",
       " 'anglican': 891,\n",
       " 'giving': 892,\n",
       " 'speaking': 893,\n",
       " 'anthony': 894,\n",
       " 'americans': 895,\n",
       " 'threat': 896,\n",
       " 'entered': 897,\n",
       " 'terror': 898,\n",
       " 'main': 899,\n",
       " 'lost': 900,\n",
       " 'criticism': 901,\n",
       " 'tasmania': 902,\n",
       " 'together': 903,\n",
       " 'smoke': 904,\n",
       " 'son': 905,\n",
       " 'crisis': 906,\n",
       " 'played': 907,\n",
       " 'level': 908,\n",
       " 'themselves': 909,\n",
       " 'massive': 910,\n",
       " 'significant': 911,\n",
       " 'base': 912,\n",
       " 'never': 913,\n",
       " 'assistance': 914,\n",
       " 'round': 915,\n",
       " 'chairman': 916,\n",
       " 'decide': 917,\n",
       " 'front': 918,\n",
       " 'zimbabwe': 919,\n",
       " 'nearly': 920,\n",
       " 'game': 921,\n",
       " 'ballot': 922,\n",
       " 'conflict': 923,\n",
       " 'crean': 924,\n",
       " 'tragedy': 925,\n",
       " 'labour': 926,\n",
       " 'chance': 927,\n",
       " 'sort': 928,\n",
       " 'longer': 929,\n",
       " 'trip': 930,\n",
       " 'geoff': 931,\n",
       " 'changed': 932,\n",
       " 'france': 933,\n",
       " 'explosives': 934,\n",
       " 'employees': 935,\n",
       " 'protect': 936,\n",
       " 'daryl': 937,\n",
       " 'muslim': 938,\n",
       " 'yes': 939,\n",
       " 'survey': 940,\n",
       " 'conference': 941,\n",
       " 'ambush': 942,\n",
       " 'heavy': 943,\n",
       " 'powers': 944,\n",
       " 'potential': 945,\n",
       " 'finished': 946,\n",
       " 'vote': 947,\n",
       " 'population': 948,\n",
       " 'endeavour': 949,\n",
       " 'russian': 950,\n",
       " 'brett': 951,\n",
       " 'placed': 952,\n",
       " 'storms': 953,\n",
       " 'study': 954,\n",
       " 'reid': 955,\n",
       " 'organisations': 956,\n",
       " 'denied': 957,\n",
       " 'bring': 958,\n",
       " 'places': 959,\n",
       " 'entitlements': 960,\n",
       " 'fast': 961,\n",
       " 'indonesia': 962,\n",
       " 'jacques': 963,\n",
       " 'helicopter': 964,\n",
       " 'saturday': 965,\n",
       " 'hearings': 966,\n",
       " 'plan': 967,\n",
       " 'pakistani': 968,\n",
       " 'quarter': 969,\n",
       " 'costs': 970,\n",
       " 'current': 971,\n",
       " 'boxing': 972,\n",
       " 'debt': 973,\n",
       " 'lockett': 974,\n",
       " 'jenin': 975,\n",
       " 'macgill': 976,\n",
       " 'calling': 977,\n",
       " 'cannot': 978,\n",
       " 'complex': 979,\n",
       " 'cars': 980,\n",
       " 'absolutely': 981,\n",
       " 'continues': 982,\n",
       " 'gun': 983,\n",
       " 'winner': 984,\n",
       " 'ice': 985,\n",
       " 'direct': 986,\n",
       " 'deadly': 987,\n",
       " 'gas': 988,\n",
       " 'moved': 989,\n",
       " 'claim': 990,\n",
       " 'payment': 991,\n",
       " 'receive': 992,\n",
       " 'fired': 993,\n",
       " 'nation': 994,\n",
       " 'accept': 995,\n",
       " 'carry': 996,\n",
       " 'greater': 997,\n",
       " 'professor': 998,\n",
       " 'forecast': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51200b3115175a",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1863230ee617a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KeyedVectors.load_word2vec_format(datapath('high_precision.kv.bin'), binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fc6ebd53904af7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'king' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m king_vector_m2 = \u001b[43mmodel2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mking\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m queen_vector_m2 = model2.get_vector(\u001b[33m'\u001b[39m\u001b[33mqueen\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m man_vector_m2 = model2.get_vector(\u001b[33m'\u001b[39m\u001b[33mman\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:446\u001b[39m, in \u001b[36mKeyedVectors.get_vector\u001b[39m\u001b[34m(self, key, norm)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    423\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[32m    424\u001b[39m \n\u001b[32m    425\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m \n\u001b[32m    445\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[32m    448\u001b[39m         \u001b[38;5;28mself\u001b[39m.fill_norms()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/passau/dlnlp/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:420\u001b[39m, in \u001b[36mKeyedVectors.get_index\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'king' not present\""
     ]
    }
   ],
   "source": [
    "king_vector_m2 = model2.get_vector('king')\n",
    "queen_vector_m2 = model2.get_vector('queen')\n",
    "man_vector_m2 = model2.get_vector('man')\n",
    "woman_vector_m2 = model2.get_vector('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983221de39ed40a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kangaroo.n.01': 0, 'horse.n.01': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4315621799da1",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424f5f1f1e6255ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KeyedVectors.load_word2vec_format(datapath('euclidean_vectors.bin'), binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d8b1800eb7dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector_m3 = model3.get_vector('king')\n",
    "queen_vector_m3 = model3.get_vector('queen')\n",
    "man_vector_m3 = model3.get_vector('man')\n",
    "woman_vector_m3 = model3.get_vector('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "634e71062b11389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'to': 1,\n",
       " 'of': 2,\n",
       " 'in': 3,\n",
       " 'and': 4,\n",
       " 'he': 5,\n",
       " 'is': 6,\n",
       " 'for': 7,\n",
       " 'on': 8,\n",
       " 'said': 9,\n",
       " 'that': 10,\n",
       " 'has': 11,\n",
       " 'says': 12,\n",
       " 'was': 13,\n",
       " 'have': 14,\n",
       " 'it': 15,\n",
       " 'be': 16,\n",
       " 'are': 17,\n",
       " 'with': 18,\n",
       " 'will': 19,\n",
       " 'at': 20,\n",
       " 'mr': 21,\n",
       " 'from': 22,\n",
       " 'by': 23,\n",
       " 'we': 24,\n",
       " 'been': 25,\n",
       " 'as': 26,\n",
       " 'an': 27,\n",
       " 'not': 28,\n",
       " 'his': 29,\n",
       " 'but': 30,\n",
       " 'they': 31,\n",
       " 'after': 32,\n",
       " 'were': 33,\n",
       " 'had': 34,\n",
       " 'there': 35,\n",
       " 'new': 36,\n",
       " 'this': 37,\n",
       " 'australian': 38,\n",
       " 'australia': 39,\n",
       " 'who': 40,\n",
       " 'people': 41,\n",
       " 'palestinian': 42,\n",
       " 'their': 43,\n",
       " 'government': 44,\n",
       " 'two': 45,\n",
       " 'up': 46,\n",
       " 'south': 47,\n",
       " 'us': 48,\n",
       " 'which': 49,\n",
       " 'year': 50,\n",
       " 'one': 51,\n",
       " 'about': 52,\n",
       " 'out': 53,\n",
       " 'if': 54,\n",
       " 'also': 55,\n",
       " 'more': 56,\n",
       " 'when': 57,\n",
       " 'its': 58,\n",
       " 'into': 59,\n",
       " 'would': 60,\n",
       " 'first': 61,\n",
       " 'against': 62,\n",
       " 'last': 63,\n",
       " 'israeli': 64,\n",
       " 'minister': 65,\n",
       " 'arafat': 66,\n",
       " 'all': 67,\n",
       " 'over': 68,\n",
       " 'afghanistan': 69,\n",
       " 'three': 70,\n",
       " 'united': 71,\n",
       " 'no': 72,\n",
       " 'world': 73,\n",
       " 'police': 74,\n",
       " 'or': 75,\n",
       " 'than': 76,\n",
       " 'attacks': 77,\n",
       " 'before': 78,\n",
       " 'fire': 79,\n",
       " 'day': 80,\n",
       " 'security': 81,\n",
       " 'some': 82,\n",
       " 'states': 83,\n",
       " 'you': 84,\n",
       " 'could': 85,\n",
       " 'them': 86,\n",
       " 'say': 87,\n",
       " 'today': 88,\n",
       " 'now': 89,\n",
       " 'told': 90,\n",
       " 'time': 91,\n",
       " 'any': 92,\n",
       " 'very': 93,\n",
       " 'laden': 94,\n",
       " 'bin': 95,\n",
       " 'just': 96,\n",
       " 'can': 97,\n",
       " 'what': 98,\n",
       " 'president': 99,\n",
       " 'sydney': 100,\n",
       " 'company': 101,\n",
       " 'still': 102,\n",
       " 'four': 103,\n",
       " 'man': 104,\n",
       " 'killed': 105,\n",
       " 'taliban': 106,\n",
       " 'al': 107,\n",
       " 'forces': 108,\n",
       " 'our': 109,\n",
       " 'around': 110,\n",
       " 'west': 111,\n",
       " 'being': 112,\n",
       " 'days': 113,\n",
       " 'old': 114,\n",
       " 'other': 115,\n",
       " 'officials': 116,\n",
       " 'where': 117,\n",
       " 'so': 118,\n",
       " 'test': 119,\n",
       " 'qaeda': 120,\n",
       " 'israel': 121,\n",
       " 'next': 122,\n",
       " 'per': 123,\n",
       " 'general': 124,\n",
       " 'think': 125,\n",
       " 'federal': 126,\n",
       " 'she': 127,\n",
       " 'force': 128,\n",
       " 'cent': 129,\n",
       " 'workers': 130,\n",
       " 'leader': 131,\n",
       " 'yesterday': 132,\n",
       " 'under': 133,\n",
       " 'hamas': 134,\n",
       " 'take': 135,\n",
       " 'state': 136,\n",
       " 'him': 137,\n",
       " 'those': 138,\n",
       " 'bank': 139,\n",
       " 'years': 140,\n",
       " 'back': 141,\n",
       " 'meeting': 142,\n",
       " 'suicide': 143,\n",
       " 'made': 144,\n",
       " 'morning': 145,\n",
       " 'action': 146,\n",
       " 'down': 147,\n",
       " 'commission': 148,\n",
       " 're': 149,\n",
       " 'pakistan': 150,\n",
       " 'international': 151,\n",
       " 'attack': 152,\n",
       " 'centre': 153,\n",
       " 'afghan': 154,\n",
       " 'group': 155,\n",
       " 'city': 156,\n",
       " 'well': 157,\n",
       " 'through': 158,\n",
       " 'military': 159,\n",
       " 'members': 160,\n",
       " 'while': 161,\n",
       " 'number': 162,\n",
       " 'five': 163,\n",
       " 'called': 164,\n",
       " 'local': 165,\n",
       " 'area': 166,\n",
       " 'qantas': 167,\n",
       " 'gaza': 168,\n",
       " 'week': 169,\n",
       " 'union': 170,\n",
       " 'national': 171,\n",
       " 'since': 172,\n",
       " 'hours': 173,\n",
       " 'wales': 174,\n",
       " 'september': 175,\n",
       " 'including': 176,\n",
       " 'another': 177,\n",
       " 'east': 178,\n",
       " 'report': 179,\n",
       " 'night': 180,\n",
       " 'off': 181,\n",
       " 'north': 182,\n",
       " 'should': 183,\n",
       " 'six': 184,\n",
       " 'staff': 185,\n",
       " 'between': 186,\n",
       " 'these': 187,\n",
       " 'get': 188,\n",
       " 'second': 189,\n",
       " 'earlier': 190,\n",
       " 'war': 191,\n",
       " 'go': 192,\n",
       " 'islamic': 193,\n",
       " 'further': 194,\n",
       " 'defence': 195,\n",
       " 'end': 196,\n",
       " 'months': 197,\n",
       " 'do': 198,\n",
       " 'because': 199,\n",
       " 'authority': 200,\n",
       " 'foreign': 201,\n",
       " 'going': 202,\n",
       " 'power': 203,\n",
       " 'work': 204,\n",
       " 'areas': 205,\n",
       " 'near': 206,\n",
       " 'team': 207,\n",
       " 'sharon': 208,\n",
       " 'during': 209,\n",
       " 'died': 210,\n",
       " 'month': 211,\n",
       " 'only': 212,\n",
       " 'many': 213,\n",
       " 'india': 214,\n",
       " 'way': 215,\n",
       " 'eight': 216,\n",
       " 'know': 217,\n",
       " 'metres': 218,\n",
       " 'match': 219,\n",
       " 'good': 220,\n",
       " 'make': 221,\n",
       " 've': 222,\n",
       " 'melbourne': 223,\n",
       " 'northern': 224,\n",
       " 'spokesman': 225,\n",
       " 'claims': 226,\n",
       " 'former': 227,\n",
       " 'left': 228,\n",
       " 'air': 229,\n",
       " 'most': 230,\n",
       " 'support': 231,\n",
       " 'osama': 232,\n",
       " 'peace': 233,\n",
       " 'like': 234,\n",
       " 'authorities': 235,\n",
       " 'prime': 236,\n",
       " 'given': 237,\n",
       " 'am': 238,\n",
       " 'ago': 239,\n",
       " 'saying': 240,\n",
       " 'set': 241,\n",
       " 'expected': 242,\n",
       " 'tora': 243,\n",
       " 'put': 244,\n",
       " 'bora': 245,\n",
       " 'looking': 246,\n",
       " 'come': 247,\n",
       " 'place': 248,\n",
       " 'militants': 249,\n",
       " 'fighters': 250,\n",
       " 'several': 251,\n",
       " 'children': 252,\n",
       " 'meanwhile': 253,\n",
       " 'injured': 254,\n",
       " 'christmas': 255,\n",
       " 'groups': 256,\n",
       " 'indian': 257,\n",
       " 'found': 258,\n",
       " 'unions': 259,\n",
       " 'arrested': 260,\n",
       " 'royal': 261,\n",
       " 'river': 262,\n",
       " 'troops': 263,\n",
       " 'child': 264,\n",
       " 'africa': 265,\n",
       " 'talks': 266,\n",
       " 'official': 267,\n",
       " 'whether': 268,\n",
       " 'interim': 269,\n",
       " 'reports': 270,\n",
       " 'then': 271,\n",
       " 'hospital': 272,\n",
       " 'terrorist': 273,\n",
       " 'yasser': 274,\n",
       " 'part': 275,\n",
       " 'industrial': 276,\n",
       " 'don': 277,\n",
       " 'start': 278,\n",
       " 'how': 279,\n",
       " 'statement': 280,\n",
       " 'leaders': 281,\n",
       " 'third': 282,\n",
       " 'early': 283,\n",
       " 'senior': 284,\n",
       " 'terrorism': 285,\n",
       " 'economy': 286,\n",
       " 'mountains': 287,\n",
       " 'weather': 288,\n",
       " 'hit': 289,\n",
       " 'million': 290,\n",
       " 'believe': 291,\n",
       " 'trying': 292,\n",
       " 'family': 293,\n",
       " 'both': 294,\n",
       " 'john': 295,\n",
       " 'army': 296,\n",
       " 'pay': 297,\n",
       " 'court': 298,\n",
       " 'radio': 299,\n",
       " 'public': 300,\n",
       " 'dr': 301,\n",
       " 'asked': 302,\n",
       " 'control': 303,\n",
       " 'lead': 304,\n",
       " 'pressure': 305,\n",
       " 'best': 306,\n",
       " 'long': 307,\n",
       " 'adelaide': 308,\n",
       " 'chief': 309,\n",
       " 'following': 310,\n",
       " 'however': 311,\n",
       " 'agreement': 312,\n",
       " 'help': 313,\n",
       " 'few': 314,\n",
       " 'house': 315,\n",
       " 'play': 316,\n",
       " 'labor': 317,\n",
       " 'australians': 318,\n",
       " 'arrest': 319,\n",
       " 'better': 320,\n",
       " 'want': 321,\n",
       " 'does': 322,\n",
       " 'firefighters': 323,\n",
       " 'high': 324,\n",
       " 'need': 325,\n",
       " 'close': 326,\n",
       " 'service': 327,\n",
       " 'community': 328,\n",
       " 'taken': 329,\n",
       " 'confirmed': 330,\n",
       " 'queensland': 331,\n",
       " 'services': 332,\n",
       " 'overnight': 333,\n",
       " 'process': 334,\n",
       " 'opposition': 335,\n",
       " 'williams': 336,\n",
       " 'must': 337,\n",
       " 'secretary': 338,\n",
       " 'information': 339,\n",
       " 'believed': 340,\n",
       " 'came': 341,\n",
       " 'detainees': 342,\n",
       " 'won': 343,\n",
       " 'governor': 344,\n",
       " 'held': 345,\n",
       " 'shot': 346,\n",
       " 'damage': 347,\n",
       " 'possible': 348,\n",
       " 'her': 349,\n",
       " 'hicks': 350,\n",
       " 'nations': 351,\n",
       " 'much': 352,\n",
       " 'pentagon': 353,\n",
       " 'peter': 354,\n",
       " 'party': 355,\n",
       " 'did': 356,\n",
       " 'released': 357,\n",
       " 'win': 358,\n",
       " 'maintenance': 359,\n",
       " 'took': 360,\n",
       " 'brought': 361,\n",
       " 'british': 362,\n",
       " 'accused': 363,\n",
       " 'safety': 364,\n",
       " 'armed': 365,\n",
       " 'kandahar': 366,\n",
       " 'winds': 367,\n",
       " 'despite': 368,\n",
       " 'even': 369,\n",
       " 'such': 370,\n",
       " 'across': 371,\n",
       " 'eastern': 372,\n",
       " 'violence': 373,\n",
       " 'weekend': 374,\n",
       " 'return': 375,\n",
       " 'conditions': 376,\n",
       " 'without': 377,\n",
       " 'building': 378,\n",
       " 'lot': 379,\n",
       " 'asylum': 380,\n",
       " 'dead': 381,\n",
       " 'cut': 382,\n",
       " 'york': 383,\n",
       " 'change': 384,\n",
       " 'hill': 385,\n",
       " 'director': 386,\n",
       " 'council': 387,\n",
       " 'airline': 388,\n",
       " 'got': 389,\n",
       " 'far': 390,\n",
       " 'news': 391,\n",
       " 'lee': 392,\n",
       " 'waugh': 393,\n",
       " 'trade': 394,\n",
       " 'southern': 395,\n",
       " 'crew': 396,\n",
       " 'continuing': 397,\n",
       " 'monday': 398,\n",
       " 'captured': 399,\n",
       " 'fires': 400,\n",
       " 'see': 401,\n",
       " 'race': 402,\n",
       " 'economic': 403,\n",
       " 'strong': 404,\n",
       " 'call': 405,\n",
       " 'anti': 406,\n",
       " 'emergency': 407,\n",
       " 'cricket': 408,\n",
       " 'region': 409,\n",
       " 'aircraft': 410,\n",
       " 'palestinians': 411,\n",
       " 'men': 412,\n",
       " 'home': 413,\n",
       " 'training': 414,\n",
       " 'seekers': 415,\n",
       " 'working': 416,\n",
       " 'strip': 417,\n",
       " 'country': 418,\n",
       " 'american': 419,\n",
       " 'david': 420,\n",
       " 'board': 421,\n",
       " 'role': 422,\n",
       " 'here': 423,\n",
       " 'december': 424,\n",
       " 'too': 425,\n",
       " 'fighting': 426,\n",
       " 'plans': 427,\n",
       " 'industry': 428,\n",
       " 'george': 429,\n",
       " 'charged': 430,\n",
       " 'act': 431,\n",
       " 'health': 432,\n",
       " 'bush': 433,\n",
       " 'received': 434,\n",
       " 'key': 435,\n",
       " 'alliance': 436,\n",
       " 'rate': 437,\n",
       " 'past': 438,\n",
       " 'administration': 439,\n",
       " 'bureau': 440,\n",
       " 'used': 441,\n",
       " 'head': 442,\n",
       " 'offer': 443,\n",
       " 'water': 444,\n",
       " 'zinni': 445,\n",
       " 'town': 446,\n",
       " 'within': 447,\n",
       " 'boat': 448,\n",
       " 'decision': 449,\n",
       " 'zealand': 450,\n",
       " 'least': 451,\n",
       " 'israelis': 452,\n",
       " 'strikes': 453,\n",
       " 'britain': 454,\n",
       " 'line': 455,\n",
       " 'department': 456,\n",
       " 'soldiers': 457,\n",
       " 'hih': 458,\n",
       " 'station': 459,\n",
       " 'issue': 460,\n",
       " 'downer': 461,\n",
       " 'leading': 462,\n",
       " 'use': 463,\n",
       " 'major': 464,\n",
       " 'person': 465,\n",
       " 'operations': 466,\n",
       " 'hundreds': 467,\n",
       " 'stop': 468,\n",
       " 'final': 469,\n",
       " 'parliament': 470,\n",
       " 'known': 471,\n",
       " 'captain': 472,\n",
       " 'legal': 473,\n",
       " 'large': 474,\n",
       " 'airport': 475,\n",
       " 'risk': 476,\n",
       " 'your': 477,\n",
       " 'may': 478,\n",
       " 'later': 479,\n",
       " 'give': 480,\n",
       " 'ahead': 481,\n",
       " 'officers': 482,\n",
       " 'tomorrow': 483,\n",
       " 'half': 484,\n",
       " 'due': 485,\n",
       " 'un': 486,\n",
       " 'series': 487,\n",
       " 'laws': 488,\n",
       " 'able': 489,\n",
       " 'interest': 490,\n",
       " 'every': 491,\n",
       " 'homes': 492,\n",
       " 'taking': 493,\n",
       " 'weapons': 494,\n",
       " 'coast': 495,\n",
       " 'behind': 496,\n",
       " 'hollingworth': 497,\n",
       " 'policy': 498,\n",
       " 'network': 499,\n",
       " 'western': 500,\n",
       " 'kabul': 501,\n",
       " 'pm': 502,\n",
       " 'great': 503,\n",
       " 'latest': 504,\n",
       " 'late': 505,\n",
       " 'plane': 506,\n",
       " 'my': 507,\n",
       " 'remain': 508,\n",
       " 'might': 509,\n",
       " 'right': 510,\n",
       " 'shane': 511,\n",
       " 'death': 512,\n",
       " 'position': 513,\n",
       " 'already': 514,\n",
       " 'heard': 515,\n",
       " 'deaths': 516,\n",
       " 'forced': 517,\n",
       " 'life': 518,\n",
       " 'hard': 519,\n",
       " 'failed': 520,\n",
       " 'seen': 521,\n",
       " 'continue': 522,\n",
       " 'towards': 523,\n",
       " 'along': 524,\n",
       " 'jihad': 525,\n",
       " 'side': 526,\n",
       " 'timor': 527,\n",
       " 'abuse': 528,\n",
       " 'territory': 529,\n",
       " 'special': 530,\n",
       " 'others': 531,\n",
       " 'guilty': 532,\n",
       " 'campaign': 533,\n",
       " 'bill': 534,\n",
       " 'storm': 535,\n",
       " 'same': 536,\n",
       " 'flight': 537,\n",
       " 'concerned': 538,\n",
       " 'victory': 539,\n",
       " 'cup': 540,\n",
       " 'jobs': 541,\n",
       " 'thought': 542,\n",
       " 'event': 543,\n",
       " 'me': 544,\n",
       " 'point': 545,\n",
       " 'really': 546,\n",
       " 'washington': 547,\n",
       " 'member': 548,\n",
       " 'buildings': 549,\n",
       " 'november': 550,\n",
       " 'case': 551,\n",
       " 'sunday': 552,\n",
       " 'weeks': 553,\n",
       " 'bombing': 554,\n",
       " 'mcgrath': 555,\n",
       " 'bowler': 556,\n",
       " 'matter': 557,\n",
       " 'th': 558,\n",
       " 'innings': 559,\n",
       " 'helicopters': 560,\n",
       " 'bus': 561,\n",
       " 'envoy': 562,\n",
       " 'details': 563,\n",
       " 'countries': 564,\n",
       " 'likely': 565,\n",
       " 'middle': 566,\n",
       " 'canyoning': 567,\n",
       " 'move': 568,\n",
       " 'rates': 569,\n",
       " 'situation': 570,\n",
       " 'cabinet': 571,\n",
       " 'again': 572,\n",
       " 'capital': 573,\n",
       " 'woomera': 574,\n",
       " 'seven': 575,\n",
       " 'bichel': 576,\n",
       " 'warne': 577,\n",
       " 'mark': 578,\n",
       " 'launched': 579,\n",
       " 'according': 580,\n",
       " 'detention': 581,\n",
       " 'enough': 582,\n",
       " 'bombings': 583,\n",
       " 'important': 584,\n",
       " 'space': 585,\n",
       " 'hour': 586,\n",
       " 'office': 587,\n",
       " 'wants': 588,\n",
       " 'boy': 589,\n",
       " 'human': 590,\n",
       " 'adventure': 591,\n",
       " 'perth': 592,\n",
       " 'women': 593,\n",
       " 'young': 594,\n",
       " 'political': 595,\n",
       " 'deal': 596,\n",
       " 'asio': 597,\n",
       " 'based': 598,\n",
       " 'claimed': 599,\n",
       " 'commonwealth': 600,\n",
       " 'evidence': 601,\n",
       " 'sure': 602,\n",
       " 'justice': 603,\n",
       " 'swiss': 604,\n",
       " 'jail': 605,\n",
       " 'reported': 606,\n",
       " 'aedt': 607,\n",
       " 'caught': 608,\n",
       " 'own': 609,\n",
       " 'movement': 610,\n",
       " 'mission': 611,\n",
       " 'african': 612,\n",
       " 'disease': 613,\n",
       " 'added': 614,\n",
       " 'show': 615,\n",
       " 'raids': 616,\n",
       " 'blue': 617,\n",
       " 'opened': 618,\n",
       " 'run': 619,\n",
       " 'money': 620,\n",
       " 'forward': 621,\n",
       " 'immediately': 622,\n",
       " 'guides': 623,\n",
       " 'rule': 624,\n",
       " 'top': 625,\n",
       " 'carried': 626,\n",
       " 'freeze': 627,\n",
       " 'warplanes': 628,\n",
       " 'targets': 629,\n",
       " 'cancer': 630,\n",
       " 'ms': 631,\n",
       " 'dispute': 632,\n",
       " 'wicket': 633,\n",
       " 'times': 634,\n",
       " 'face': 635,\n",
       " 'march': 636,\n",
       " 'always': 637,\n",
       " 'investigation': 638,\n",
       " 'border': 639,\n",
       " 'flying': 640,\n",
       " 'full': 641,\n",
       " 'allegations': 642,\n",
       " 'sector': 643,\n",
       " 'financial': 644,\n",
       " 'although': 645,\n",
       " 'growth': 646,\n",
       " 'ground': 647,\n",
       " 'burning': 648,\n",
       " 'result': 649,\n",
       " 'order': 650,\n",
       " 'crash': 651,\n",
       " 'planning': 652,\n",
       " 'break': 653,\n",
       " 'island': 654,\n",
       " 'job': 655,\n",
       " 'become': 656,\n",
       " 'carrying': 657,\n",
       " 'find': 658,\n",
       " 'using': 659,\n",
       " 'access': 660,\n",
       " 'beat': 661,\n",
       " 'prepared': 662,\n",
       " 'each': 663,\n",
       " 'certainly': 664,\n",
       " 'banks': 665,\n",
       " 'reached': 666,\n",
       " 'proposed': 667,\n",
       " 'probably': 668,\n",
       " 'collapse': 669,\n",
       " 'relations': 670,\n",
       " 'inside': 671,\n",
       " 'reserve': 672,\n",
       " 'allow': 673,\n",
       " 'tourists': 674,\n",
       " 'militia': 675,\n",
       " 'organisation': 676,\n",
       " 'radical': 677,\n",
       " 'militant': 678,\n",
       " 'afp': 679,\n",
       " 'road': 680,\n",
       " 'wave': 681,\n",
       " 'different': 682,\n",
       " 'executive': 683,\n",
       " 'energy': 684,\n",
       " 'clear': 685,\n",
       " 'serious': 686,\n",
       " 'responsibility': 687,\n",
       " 'hewitt': 688,\n",
       " 'post': 689,\n",
       " 'making': 690,\n",
       " 'inquiry': 691,\n",
       " 'sent': 692,\n",
       " 'harrison': 693,\n",
       " 'suspected': 694,\n",
       " 'surrender': 695,\n",
       " 'trees': 696,\n",
       " 'management': 697,\n",
       " 'thousands': 698,\n",
       " 'thursday': 699,\n",
       " 'needs': 700,\n",
       " 'bowling': 701,\n",
       " 'future': 702,\n",
       " 'rejected': 703,\n",
       " 'outside': 704,\n",
       " 'opening': 705,\n",
       " 'travel': 706,\n",
       " 'kilometres': 707,\n",
       " 'short': 708,\n",
       " 'killing': 709,\n",
       " 'ariel': 710,\n",
       " 'quickly': 711,\n",
       " 'howard': 712,\n",
       " 'went': 713,\n",
       " 'drop': 714,\n",
       " 'bid': 715,\n",
       " 'sex': 716,\n",
       " 'himself': 717,\n",
       " 'declared': 718,\n",
       " 'comes': 719,\n",
       " 'fight': 720,\n",
       " 'lives': 721,\n",
       " 'soon': 722,\n",
       " 'ansett': 723,\n",
       " 'immigration': 724,\n",
       " 'caves': 725,\n",
       " 'tried': 726,\n",
       " 'stage': 727,\n",
       " 'argentina': 728,\n",
       " 'believes': 729,\n",
       " 'senator': 730,\n",
       " 'flights': 731,\n",
       " 'calls': 732,\n",
       " 'program': 733,\n",
       " 'getting': 734,\n",
       " 'until': 735,\n",
       " 'alleged': 736,\n",
       " 'sentence': 737,\n",
       " 'circumstances': 738,\n",
       " 'television': 739,\n",
       " 'quite': 740,\n",
       " 'annual': 741,\n",
       " 'rural': 742,\n",
       " 'wounded': 743,\n",
       " 'try': 744,\n",
       " 'open': 745,\n",
       " 'attempt': 746,\n",
       " 'white': 747,\n",
       " 'terms': 748,\n",
       " 'ruddock': 749,\n",
       " 'among': 750,\n",
       " 'offices': 751,\n",
       " 'sea': 752,\n",
       " 'currently': 753,\n",
       " 'tanks': 754,\n",
       " 'available': 755,\n",
       " 'sources': 756,\n",
       " 'ended': 757,\n",
       " 'donald': 758,\n",
       " 'anything': 759,\n",
       " 'refused': 760,\n",
       " 'look': 761,\n",
       " 'figures': 762,\n",
       " 'changes': 763,\n",
       " 'means': 764,\n",
       " 'measures': 765,\n",
       " 'alexander': 766,\n",
       " 'shortly': 767,\n",
       " 'yacht': 768,\n",
       " 'issues': 769,\n",
       " 'form': 770,\n",
       " 'july': 771,\n",
       " 'victoria': 772,\n",
       " 'running': 773,\n",
       " 'bombers': 774,\n",
       " 'agency': 775,\n",
       " 'address': 776,\n",
       " 'response': 777,\n",
       " 'gunmen': 778,\n",
       " 'karzai': 779,\n",
       " 'meet': 780,\n",
       " 'happened': 781,\n",
       " 'directors': 782,\n",
       " 'actually': 783,\n",
       " 'jewish': 784,\n",
       " 'yet': 785,\n",
       " 'something': 786,\n",
       " 'done': 787,\n",
       " 'wickets': 788,\n",
       " 'residents': 789,\n",
       " 'returned': 790,\n",
       " 'destroyed': 791,\n",
       " 'brisbane': 792,\n",
       " 'announced': 793,\n",
       " 'accident': 794,\n",
       " 'warned': 795,\n",
       " 'understand': 796,\n",
       " 'parts': 797,\n",
       " 'rise': 798,\n",
       " 'decided': 799,\n",
       " 'ministers': 800,\n",
       " 'witnesses': 801,\n",
       " 'biggest': 802,\n",
       " 'parties': 803,\n",
       " 'nine': 804,\n",
       " 'list': 805,\n",
       " 'source': 806,\n",
       " 'kallis': 807,\n",
       " 'fighter': 808,\n",
       " 'school': 809,\n",
       " 'increase': 810,\n",
       " 'rights': 811,\n",
       " 'caused': 812,\n",
       " 'allan': 813,\n",
       " 'tour': 814,\n",
       " 'highway': 815,\n",
       " 'deputy': 816,\n",
       " 'media': 817,\n",
       " 'commissioner': 818,\n",
       " 'arrived': 819,\n",
       " 'facility': 820,\n",
       " 'away': 821,\n",
       " 'elected': 822,\n",
       " 'though': 823,\n",
       " 'advice': 824,\n",
       " 'supporters': 825,\n",
       " 'ses': 826,\n",
       " 'total': 827,\n",
       " 'pacific': 828,\n",
       " 'cost': 829,\n",
       " 'law': 830,\n",
       " 'wage': 831,\n",
       " 'fact': 832,\n",
       " 'difficult': 833,\n",
       " 'prevent': 834,\n",
       " 'began': 835,\n",
       " 'confidence': 836,\n",
       " 'negotiations': 837,\n",
       " 'blake': 838,\n",
       " 'huge': 839,\n",
       " 'rafter': 840,\n",
       " 'beyond': 841,\n",
       " 'rather': 842,\n",
       " 'beginning': 843,\n",
       " 'sir': 844,\n",
       " 'glenn': 845,\n",
       " 'business': 846,\n",
       " 'afternoon': 847,\n",
       " 'struck': 848,\n",
       " 'intelligence': 849,\n",
       " 'ensure': 850,\n",
       " 'virus': 851,\n",
       " 'period': 852,\n",
       " 'big': 853,\n",
       " 'tennis': 854,\n",
       " 'islands': 855,\n",
       " 'car': 856,\n",
       " 'having': 857,\n",
       " 'commander': 858,\n",
       " 'appeared': 859,\n",
       " 'step': 860,\n",
       " 'powell': 861,\n",
       " 'strike': 862,\n",
       " 'crowd': 863,\n",
       " 'whose': 864,\n",
       " 'expect': 865,\n",
       " 'ever': 866,\n",
       " 'runs': 867,\n",
       " 'commanders': 868,\n",
       " 'suharto': 869,\n",
       " 'airlines': 870,\n",
       " 'market': 871,\n",
       " 'nauru': 872,\n",
       " 'll': 873,\n",
       " 'less': 874,\n",
       " 'attorney': 875,\n",
       " 'expressed': 876,\n",
       " 'gave': 877,\n",
       " 'worst': 878,\n",
       " 'involved': 879,\n",
       " 'gives': 880,\n",
       " 'followed': 881,\n",
       " 'recession': 882,\n",
       " 'musharraf': 883,\n",
       " 'robert': 884,\n",
       " 'field': 885,\n",
       " 'friday': 886,\n",
       " 'bomb': 887,\n",
       " 'hobart': 888,\n",
       " 'tragedy': 889,\n",
       " 'terror': 890,\n",
       " 'smoke': 891,\n",
       " 'potential': 892,\n",
       " 'plan': 893,\n",
       " 'played': 894,\n",
       " 'feel': 895,\n",
       " 'reid': 896,\n",
       " 'places': 897,\n",
       " 'speaking': 898,\n",
       " 'costs': 899,\n",
       " 'anthony': 900,\n",
       " 'entered': 901,\n",
       " 'muslim': 902,\n",
       " 'hearings': 903,\n",
       " 'lockett': 904,\n",
       " 'helicopter': 905,\n",
       " 'front': 906,\n",
       " 'storms': 907,\n",
       " 'organisations': 908,\n",
       " 'crisis': 909,\n",
       " 'jenin': 910,\n",
       " 'ambush': 911,\n",
       " 'quarter': 912,\n",
       " 'level': 913,\n",
       " 'conflict': 914,\n",
       " 'base': 915,\n",
       " 'zimbabwe': 916,\n",
       " 'endeavour': 917,\n",
       " 'chance': 918,\n",
       " 'macgill': 919,\n",
       " 'longer': 920,\n",
       " 'giving': 921,\n",
       " 'employees': 922,\n",
       " 'nearly': 923,\n",
       " 'explosives': 924,\n",
       " 'assistance': 925,\n",
       " 'yes': 926,\n",
       " 'geoff': 927,\n",
       " 'changed': 928,\n",
       " 'chairman': 929,\n",
       " 'vote': 930,\n",
       " 'criticism': 931,\n",
       " 'heavy': 932,\n",
       " 'main': 933,\n",
       " 'complex': 934,\n",
       " 'threat': 935,\n",
       " 'game': 936,\n",
       " 'brett': 937,\n",
       " 'calling': 938,\n",
       " 'jacques': 939,\n",
       " 'lost': 940,\n",
       " 'population': 941,\n",
       " 'bring': 942,\n",
       " 'study': 943,\n",
       " 'saturday': 944,\n",
       " 'russian': 945,\n",
       " 'fast': 946,\n",
       " 'round': 947,\n",
       " 'indonesia': 948,\n",
       " 'protect': 949,\n",
       " 'sort': 950,\n",
       " 'daryl': 951,\n",
       " 'current': 952,\n",
       " 'never': 953,\n",
       " 'together': 954,\n",
       " 'decide': 955,\n",
       " 'survey': 956,\n",
       " 'conference': 957,\n",
       " 'massive': 958,\n",
       " 'powers': 959,\n",
       " 'boxing': 960,\n",
       " 'anglican': 961,\n",
       " 'labour': 962,\n",
       " 'crean': 963,\n",
       " 'americans': 964,\n",
       " 'themselves': 965,\n",
       " 'martin': 966,\n",
       " 'finished': 967,\n",
       " 'placed': 968,\n",
       " 'denied': 969,\n",
       " 'son': 970,\n",
       " 'entitlements': 971,\n",
       " 'ballot': 972,\n",
       " 'cannot': 973,\n",
       " 'significant': 974,\n",
       " 'pakistani': 975,\n",
       " 'debt': 976,\n",
       " 'france': 977,\n",
       " 'tasmania': 978,\n",
       " 'trip': 979,\n",
       " 'receive': 980,\n",
       " 'flames': 981,\n",
       " 'record': 982,\n",
       " 'activity': 983,\n",
       " 'bomber': 984,\n",
       " 'closed': 985,\n",
       " 'problems': 986,\n",
       " 'tribal': 987,\n",
       " 'low': 988,\n",
       " 'condition': 989,\n",
       " 'concern': 990,\n",
       " 'fleeing': 991,\n",
       " 'share': 992,\n",
       " 'whole': 993,\n",
       " 'paid': 994,\n",
       " 'environment': 995,\n",
       " 'rest': 996,\n",
       " 'professor': 997,\n",
       " 'little': 998,\n",
       " 'hold': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680d6a91045774d",
   "metadata": {},
   "source": [
    "#### Analogy Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe22173b6167b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22200273"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king_mins_man_plus_woman_m3 = (king_vector_m3 - man_vector_m3) + woman_vector_m3\n",
    "\n",
    "# Make sure you have implemented cosine similarity. \n",
    "cosine_similarity(king_mins_man_plus_woman_m3, queen_vector_m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b2f0b3af40ebe",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ae4a24504312ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_google = gensim.downloader.load('word2vec-google-news-300');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "561e32e7da1a6187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_google.get_vector('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6805490c424e7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also try the GLOVE model\n",
    "glove_google = gensim.downloader.load('glove-wiki-gigaword-100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4adb443a866e9fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_google.get_vector('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7464acc6b77a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = word2vec_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba910248b1d4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector_m4 = model4.get_vector('king')\n",
    "queen_vector_m4 = model4.get_vector('queen')\n",
    "man_vector_m4 = model4.get_vector('man')\n",
    "woman_vector_m4 = model4.get_vector('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652677473d83b816",
   "metadata": {},
   "source": [
    "#### Analogy Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "396d43cd93f0c092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7300517"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king_mins_man_plus_woman_m4 = (king_vector_m4 - man_vector_m4) + woman_vector_m4\n",
    "\n",
    "# Make sure you have implemented cosine similarity. \n",
    "cosine_similarity(king_mins_man_plus_woman_m4, queen_vector_m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "459734e0cacc738",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'IDENTIFIED_METHOD'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Find a method to search for similar words given a word\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Hint: you can use a method of the word2vec_google object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m similar_words = \u001b[43mmodel4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIDENTIFIED_METHOD\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mphone\u001b[39m\u001b[33m'\u001b[39m, topn=\u001b[32m10\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word, similarity \u001b[38;5;129;01min\u001b[39;00m similar_words:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'KeyedVectors' object has no attribute 'IDENTIFIED_METHOD'"
     ]
    }
   ],
   "source": [
    "# Find a method to search for similar words given a word\n",
    "# Hint: you can use a method of the word2vec_google object\n",
    "\n",
    "similar_words = model4.IDENTIFIED_METHOD('phone', topn=10)\n",
    "\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0835f5923afa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model4.IDENTIFIED_METHOD('king', topn=10)\n",
    "\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef941245b07d523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find at least five analogies using the method you found above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a03c120788ab27",
   "metadata": {},
   "source": [
    "## Theoretical Question #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e1ef4589d3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_google.IDENTIFIED_METHOD(king_mins_man_plus_woman_m4) # First answer will be King"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
