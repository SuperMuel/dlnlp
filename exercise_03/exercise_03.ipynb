{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310f24e6a7b88cce",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbba8a222d9df960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import re\n",
    "import glob\n",
    "import collections\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8bbdb",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6ae799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define base path to the dataset\n",
    "BASE_DATA_PATH = Path(\"../exercise_01/aclImdb\") \n",
    "\n",
    "TRAIN_PATH = BASE_DATA_PATH / \"train\"\n",
    "TEST_PATH = BASE_DATA_PATH / \"test\"\n",
    "\n",
    "TRAIN_POS_PATH = TRAIN_PATH / \"pos\"\n",
    "TRAIN_NEG_PATH = TRAIN_PATH / \"neg\"\n",
    "TEST_POS_PATH = TEST_PATH / \"pos\"\n",
    "TEST_NEG_PATH = TEST_PATH / \"neg\"\n",
    "\n",
    "assert TRAIN_POS_PATH.exists() and TRAIN_NEG_PATH.exists() and TEST_POS_PATH.exists() and TEST_NEG_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf348bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training reviews.\n",
      "Training labels distribution: Positive (1): 136943, Negative (0): -111943\n",
      "Loaded 25000 test reviews.\n",
      "Test labels distribution: Positive (1): 137824, Negative (0): -112824\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_data(data_path: Path) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Loads movie reviews and their sentiments from the specified path.\n",
    "\n",
    "    \"\"\"\n",
    "    texts:list[str] = []\n",
    "    labels:list[int] = [] \n",
    "\n",
    "    for _, folder_path in [(\"pos\", data_path / \"pos\"), (\"neg\", data_path / \"neg\")]:\n",
    "        if not folder_path.exists():\n",
    "            raise FileNotFoundError(f\"Warning: Path {folder_path} does not exist.\")\n",
    "\n",
    "        for file_path in folder_path.glob(\"*.txt\"):\n",
    "                # Extract score from filename, e.g., \"7_123.txt\" -> score 7\n",
    "            score = int(file_path.name.split('_')[0])\n",
    "            filename_parts = file_path.name.split('_')\n",
    "            score_str = filename_parts[1].split('.')[0]\n",
    "            score = int(score_str)\n",
    "\n",
    "            labels.append(score)\n",
    "            texts.append(file_path.read_text(encoding='utf-8'))\n",
    "    return texts, labels\n",
    "\n",
    "# Load training data\n",
    "train_texts_raw, train_labels = load_imdb_data(TRAIN_PATH)\n",
    "print(f\"Loaded {len(train_texts_raw)} training reviews.\")\n",
    "print(f\"Training labels distribution: Positive (1): {sum(train_labels)}, Negative (0): {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Load test data\n",
    "test_texts_raw, test_labels = load_imdb_data(TEST_PATH)\n",
    "print(f\"Loaded {len(test_texts_raw)} test reviews.\")\n",
    "print(f\"Test labels distribution: Positive (1): {sum(test_labels)}, Negative (0): {len(test_labels) - sum(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8274d38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :\n",
      "For a movie that gets no respect there sure are a ... Label : 9\n",
      "Bizarre horror movie filled with famous faces but ... Label : 8\n",
      "A solid, if unremarkable film. Matthau, as Einstei... Label : 7\n",
      "It's a strange feeling to sit alone in a theater o... Label : 8\n",
      "You probably all already know this by now, but 5 a... Label : 10\n",
      "I saw the movie with two grown children. Although ... Label : 8\n",
      "You're using the IMDb.<br /><br />You've given som... Label : 10\n",
      "This was a good film with a powerful message of lo... Label : 10\n",
      "Made after QUARTET was, TRIO continued the quality... Label : 10\n",
      "For a mature man, to admit that he shed a tear ove... Label : 10\n",
      "\n",
      "\n",
      "Test :\n",
      "Based on an actual story, John Boorman shows the s... Label : 9\n",
      "This is a gem. As a Film Four production - the ant... Label : 9\n",
      "I really like this show. It has drama, romance, an... Label : 9\n",
      "This is the best 3-D experience Disney has at thei... Label : 10\n",
      "Of the Korean movies I've seen, only three had rea... Label : 10\n",
      "this movie is funny funny funny my favorite quote ... Label : 7\n",
      "I'm just starting to explore the so far wonderful ... Label : 10\n",
      "There is no need for me to repeat the synopsis ren... Label : 10\n",
      "I got this movie with my BBC \"Jane Austen Collecti... Label : 9\n",
      "This was a great movie, I would compare it to the ... Label : 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\")\n",
    "for text, label in zip(train_texts_raw[:10], train_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")\n",
    "\n",
    "print(\"\\n\\nTest :\")\n",
    "for text, label in zip(test_texts_raw[:10], test_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefbf6c55ef2c7d",
   "metadata": {},
   "source": [
    "## Task 1: Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28bcbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9b6df8eb909544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pre-processing pipeline\n",
    "pre_processed_data = preprocess(train_texts_raw, to_lowercase=True, tokenize_on_punctuation=True, remove_punctuation_tokens=True, number_replacement_token=\"NUMTOKEN\")\n",
    "pre_processed_data_test = preprocess(test_texts_raw, to_lowercase=True, tokenize_on_punctuation=True, remove_punctuation_tokens=True, number_replacement_token=\"NUMTOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f6e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are']...\n",
      "['bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by']...\n",
      "['a', 'solid', 'if', 'unremarkable', 'film', 'matthau', 'as', 'einstein', 'was', 'wonderful']...\n",
      "['it', 's', 'a', 'strange', 'feeling', 'to', 'sit', 'alone', 'in', 'a']...\n",
      "['you', 'probably', 'all', 'already', 'know', 'this', 'by', 'now', 'but', 'NUMTOKEN']...\n",
      "['i', 'saw', 'the', 'movie', 'with', 'two', 'grown', 'children', 'although', 'it']...\n",
      "['you', 're', 'using', 'the', 'imdb', 'you', 've', 'given', 'some', 'hefty']...\n",
      "['this', 'was', 'a', 'good', 'film', 'with', 'a', 'powerful', 'message', 'of']...\n",
      "['made', 'after', 'quartet', 'was', 'trio', 'continued', 'the', 'quality', 'of', 'the']...\n",
      "['for', 'a', 'mature', 'man', 'to', 'admit', 'that', 'he', 'shed', 'a']...\n"
     ]
    }
   ],
   "source": [
    "for tokens in pre_processed_data[:10]:\n",
    "    print(f\"{tokens[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9e3a91b0ca578",
   "metadata": {},
   "source": [
    "## Task 2: Byte-Pair Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/5) for a detailed explanation of the BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de668e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000s', '00am', '00pm', '00s', '01pm', '02i', '06th', '08th', '0f', '0ne']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set(token for tokens in pre_processed_data for token in tokens )\n",
    "sorted(list(unique_tokens))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a21fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "followings = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "for tokens in pre_processed_data:\n",
    "    for token, next in zip(tokens[:-1], tokens[1:]):\n",
    "        followings[token][next]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5afc7336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 15710),\n",
       " ('film', 10904),\n",
       " ('is', 9467),\n",
       " ('one', 3132),\n",
       " ('was', 2273),\n",
       " ('show', 1095),\n",
       " ('time', 578),\n",
       " ('and', 506),\n",
       " ('a', 462),\n",
       " ('story', 445)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(token, count) for token, count in followings[\"this\"].items()], key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d802e",
   "metadata": {},
   "source": [
    "We can see that \"this\" is most followed by \"movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7898a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_frequency = (token1, token2) -> count\n",
    "\n",
    "def get_pairs_frequency(data:list[list[str]])-> dict[tuple[str,str],int]:\n",
    "    pairs_frequency = defaultdict(int)\n",
    "\n",
    "    for tokens in data:\n",
    "        for token, next in zip(tokens[:-1], tokens[1:]):\n",
    "            pairs_frequency[(token, next)] +=1\n",
    "    \n",
    "    return pairs_frequency\n",
    "\n",
    "pairs_frequency =get_pairs_frequency(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176ab16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 39161),\n",
       " (('in', 'the'), 25226),\n",
       " (('it', 's'), 17221),\n",
       " (('this', 'movie'), 15710),\n",
       " (('the', 'film'), 13523),\n",
       " (('and', 'the'), 13328),\n",
       " (('is', 'a'), 13124),\n",
       " (('to', 'the'), 12004),\n",
       " (('to', 'be'), 11876),\n",
       " (('the', 'movie'), 11855)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_pairs = sorted([((token, next), count) for ((token, next), count) in pairs_frequency.items()], key=lambda x:x[1], reverse=True)\n",
    "most_frequent_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c8e30bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('of', 'the', 39161)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_more_frequent_pair(frequencies:dict[tuple[str,str], int] )-> tuple[str,str,int]:\n",
    "    return sorted([(token, next,count) for ((token, next), count) in frequencies.items()], key=lambda x:x[2], reverse=True)[0]\n",
    "\n",
    "get_more_frequent_pair(pairs_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6d93791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('of', 'the')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge = most_frequent_pairs[0][0]\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "acb98199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens:list[str], a:str, b:str)->list[str]:\n",
    "    _tokens :list[str] = []\n",
    "    did_merge = False\n",
    "    for token, next in zip(tokens[:-1], tokens[1:]):\n",
    "        if did_merge:\n",
    "            did_merge = False\n",
    "            continue\n",
    "        if (token, next) == (a,b):\n",
    "            _tokens.append(token + \" \" + next)\n",
    "            did_merge = True\n",
    "        else:\n",
    "            _tokens.append(token)\n",
    "    if not did_merge:\n",
    "        _tokens.append(tokens[-1])\n",
    "\n",
    "    return _tokens\n",
    "\n",
    "\n",
    "\n",
    "assert merge_tokens(\n",
    "    [\"This\", \"is\", \"an\", \"example\", \"of\", \"a\", \"sentence\"],\n",
    "    \"This\",\n",
    "    \"is\"\n",
    ") == ['This is', 'an', 'example', 'of', 'a', 'sentence']\n",
    "\n",
    "\n",
    "assert merge_tokens(\n",
    "    [\"This\", \"is\", \"an\", \"example\", \"of\", \"a\", \"sentence\"],\n",
    "    \"of\",\n",
    "    \"a\"\n",
    ") == ['This', 'is', 'an', 'example', 'of a', 'sentence']\n",
    "\n",
    "\n",
    "assert merge_tokens(\n",
    "    [\"This\", \"is\", \"an\", \"example\", \"of\", \"a\", \"sentence\"],\n",
    "    \"a\",\n",
    "    \"sentence\"\n",
    ") == ['This', 'is', 'an', 'example', 'of', 'a sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5320cc",
   "metadata": {},
   "source": [
    "### In a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9721da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair : (like, the) count=2009\n",
      "Most frequent pair : (he, was) count=2004\n",
      "Most frequent pair : (in, NUMTOKEN) count=1988\n",
      "Most frequent pair : (more, than) count=1984\n",
      "Most frequent pair : (i, would) count=1980\n",
      "Most frequent pair : (and, it) count=1970\n",
      "Most frequent pair : (to, say) count=1957\n",
      "Most frequent pair : (was, the) count=1924\n",
      "Most frequent pair : (a, movie) count=1894\n",
      "Most frequent pair : (some, of the) count=1894\n",
      "Most frequent pair : (a, little) count=1886\n",
      "Most frequent pair : (the, same) count=1882\n",
      "Most frequent pair : (the, acting) count=1877\n",
      "Most frequent pair : (is, that) count=1868\n",
      "Most frequent pair : (at, all) count=1842\n",
      "Most frequent pair : (the, best) count=1819\n",
      "Most frequent pair : (by, a) count=1798\n",
      "Most frequent pair : (and, then) count=1786\n",
      "Most frequent pair : (you, can) count=1770\n",
      "Most frequent pair : (a, great) count=1767\n",
      "Most frequent pair : (i, saw) count=1758\n",
      "Most frequent pair : (NUMTOKEN, minutes) count=1706\n",
      "Most frequent pair : (over, the) count=1703\n",
      "Most frequent pair : (i, had) count=1701\n",
      "Most frequent pair : (trying, to) count=1684\n",
      "Most frequent pair : (into, a) count=1676\n",
      "Most frequent pair : (in, my) count=1672\n",
      "Most frequent pair : (i, can) count=1654\n",
      "Most frequent pair : (which, is) count=1646\n",
      "Most frequent pair : (such, a) count=1627\n",
      "Most frequent pair : (that, he) count=1626\n",
      "Most frequent pair : (a lot, of) count=1623\n",
      "Most frequent pair : (who, is) count=1619\n",
      "Most frequent pair : (that, it) count=1610\n",
      "Most frequent pair : (with, his) count=1606\n",
      "Most frequent pair : (in the, film) count=1568\n",
      "Most frequent pair : (when, the) count=1565\n",
      "Most frequent pair : (the, most) count=1549\n",
      "Most frequent pair : (they, were) count=1545\n",
      "Most frequent pair : (this, was) count=1543\n",
      "Most frequent pair : (it s, not) count=1541\n",
      "Most frequent pair : (a, bit) count=1539\n",
      "Most frequent pair : (has, been) count=1527\n",
      "Most frequent pair : (want, to) count=1523\n",
      "Most frequent pair : (couldn, t) count=1498\n",
      "Most frequent pair : (in, this movie) count=1462\n",
      "Most frequent pair : (from, a) count=1453\n",
      "Most frequent pair : (in, fact) count=1452\n",
      "Most frequent pair : (of, all) count=1448\n",
      "Most frequent pair : (she, s) count=1444\n",
      "Most frequent pair : (have, a) count=1442\n",
      "Most frequent pair : (so, much) count=1438\n",
      "Most frequent pair : (the, whole) count=1426\n",
      "Most frequent pair : (in the, movie) count=1426\n",
      "Most frequent pair : (NUMTOKEN, years) count=1425\n",
      "Most frequent pair : (of, them) count=1420\n",
      "Most frequent pair : (i, thought) count=1419\n",
      "Most frequent pair : (to be, a) count=1411\n",
      "Most frequent pair : (of, it) count=1405\n",
      "Most frequent pair : (in, this film) count=1404\n",
      "Most frequent pair : (at, a) count=1390\n",
      "Most frequent pair : (she, is) count=1380\n",
      "Most frequent pair : (will, be) count=1380\n",
      "Most frequent pair : (the, characters) count=1378\n",
      "Most frequent pair : (can, be) count=1352\n",
      "Most frequent pair : (does, not) count=1332\n",
      "Most frequent pair : (should, be) count=1331\n",
      "Most frequent pair : (not, a) count=1326\n",
      "Most frequent pair : (ever, seen) count=1307\n",
      "Most frequent pair : (would, have) count=1288\n",
      "Most frequent pair : (you, ll) count=1284\n",
      "Most frequent pair : (to, find) count=1284\n",
      "Most frequent pair : (that, was) count=1276\n",
      "Most frequent pair : (and, is) count=1260\n",
      "Most frequent pair : (they, re) count=1257\n",
      "Most frequent pair : (a, very) count=1254\n",
      "Most frequent pair : (sort, of) count=1251\n",
      "Most frequent pair : (this film, is) count=1237\n",
      "Most frequent pair : (at the, end) count=1236\n",
      "Most frequent pair : (about, a) count=1233\n",
      "Most frequent pair : (for, me) count=1227\n",
      "Most frequent pair : (of, her) count=1225\n",
      "Most frequent pair : (it, is a) count=1222\n",
      "Most frequent pair : (to, go) count=1220\n",
      "Most frequent pair : (to, his) count=1218\n",
      "Most frequent pair : (through, the) count=1208\n",
      "Most frequent pair : (i, d) count=1207\n",
      "Most frequent pair : (won, t) count=1200\n",
      "Most frequent pair : (each, other) count=1198\n",
      "Most frequent pair : (it, a) count=1195\n",
      "Most frequent pair : (to, me) count=1188\n",
      "Most frequent pair : (as, an) count=1178\n",
      "Most frequent pair : (and, he) count=1172\n",
      "Most frequent pair : (have, to) count=1158\n",
      "Most frequent pair : (do, not) count=1158\n",
      "Most frequent pair : (that, they) count=1143\n",
      "Most frequent pair : (the film, is) count=1141\n",
      "Most frequent pair : (like, this) count=1140\n",
      "Most frequent pair : (did, not) count=1136\n",
      "Most frequent pair : (in, which) count=1131\n"
     ]
    }
   ],
   "source": [
    "# data = pre_processed_data\n",
    "\n",
    "# N_MERGES = 100\n",
    "# merges : list[tuple[str,str]] = []\n",
    "\n",
    "for _ in range(100):\n",
    "    frequencies = get_pairs_frequency(data)\n",
    "    a,b,count = get_more_frequent_pair(frequencies)\n",
    "    print(f\"Most frequent pair : ({a}, {b}) {count=}\")\n",
    "    data = [merge_tokens(tokens,a,b) for tokens in data]\n",
    "    merges.append((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722a491",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cdd6448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'future',\n",
       " 'a',\n",
       " 'disparate',\n",
       " 'group',\n",
       " 'of',\n",
       " 'people',\n",
       " 'asleep',\n",
       " 'aboard',\n",
       " 'a',\n",
       " 'commercial',\n",
       " 'spaceship',\n",
       " 'is',\n",
       " 'forced',\n",
       " 'to',\n",
       " 'improvise',\n",
       " 'their',\n",
       " 'survival',\n",
       " 'when',\n",
       " 'the',\n",
       " 'spaceship',\n",
       " 'crash',\n",
       " 'lands',\n",
       " 'on',\n",
       " 'a',\n",
       " 'remote',\n",
       " 'barren',\n",
       " 'planet',\n",
       " 'they',\n",
       " 'already',\n",
       " 'have',\n",
       " 'one',\n",
       " 'problem',\n",
       " 'in',\n",
       " 'that',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'passengers',\n",
       " 'is',\n",
       " 'intense',\n",
       " 'criminal',\n",
       " 'richard',\n",
       " 'riddick',\n",
       " 'vin',\n",
       " 'diesel',\n",
       " 'in',\n",
       " 'his',\n",
       " 'first',\n",
       " 'top',\n",
       " 'billed',\n",
       " 'role',\n",
       " 'however',\n",
       " 'they',\n",
       " 'are',\n",
       " 'soon',\n",
       " 'preyed',\n",
       " 'upon',\n",
       " 'by',\n",
       " 'a',\n",
       " 'strange',\n",
       " 'species',\n",
       " 'of',\n",
       " 'predator',\n",
       " 'that',\n",
       " 'thrives',\n",
       " 'in',\n",
       " 'the',\n",
       " 'darkness',\n",
       " 'and',\n",
       " 'a',\n",
       " 'rare',\n",
       " 'solar',\n",
       " 'eclipse',\n",
       " 'is',\n",
       " 'soon',\n",
       " 'to',\n",
       " 'take',\n",
       " 'place',\n",
       " 'while',\n",
       " 'the',\n",
       " 'script',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'ultimately',\n",
       " 'on',\n",
       " 'the',\n",
       " 'routine',\n",
       " 'side',\n",
       " 'it',\n",
       " 'is',\n",
       " 'decently',\n",
       " 'acted',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'especially',\n",
       " 'well',\n",
       " 'made',\n",
       " 'technically',\n",
       " 'location',\n",
       " 'work',\n",
       " 'photography',\n",
       " 'and',\n",
       " 'design',\n",
       " 'production',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'creature',\n",
       " 'design',\n",
       " 'are',\n",
       " 'all',\n",
       " 'very',\n",
       " 'impressive',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'most',\n",
       " 'original',\n",
       " 'or',\n",
       " 'stimulating',\n",
       " 'science',\n",
       " 'fiction',\n",
       " 'horror',\n",
       " 'picture',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'far',\n",
       " 'from',\n",
       " 'it',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'provides',\n",
       " 'good',\n",
       " 'entertainment',\n",
       " 'diesel',\n",
       " 'is',\n",
       " 'particularly',\n",
       " 'good',\n",
       " 'at',\n",
       " 'getting',\n",
       " 'under',\n",
       " 'the',\n",
       " 'skin',\n",
       " 'of',\n",
       " 'his',\n",
       " 'intimidating',\n",
       " 'character',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'entirely',\n",
       " 'predictable',\n",
       " 'however',\n",
       " 'and',\n",
       " 'gets',\n",
       " 'some',\n",
       " 'points',\n",
       " 'for',\n",
       " 'spoiler',\n",
       " 'having',\n",
       " 'a',\n",
       " 'more',\n",
       " 'politically',\n",
       " 'correct',\n",
       " 'ending',\n",
       " 'than',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'type',\n",
       " 'filmed',\n",
       " 'on',\n",
       " 'location',\n",
       " 'in',\n",
       " 'the',\n",
       " 'desolate',\n",
       " 'coober',\n",
       " 'pedy',\n",
       " 'area',\n",
       " 'of',\n",
       " 'queensland',\n",
       " 'in',\n",
       " 'australia',\n",
       " 'a',\n",
       " 'sequel',\n",
       " 'of',\n",
       " 'sorts',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'works',\n",
       " 'NUMTOKEN',\n",
       " 'NUMTOKEN']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pre_processed_data_test[15]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30db4233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'think')\n",
      "Merged (i, think)\n",
      "['i think']\n",
      "('the', 'film')\n",
      "Merged (the, film)\n",
      "['i think', 'the film']\n",
      "('is', 'very')\n",
      "('very', 'good')\n",
      "('i think', 'the film')\n",
      "('the film', 'is')\n",
      "Merged (the film, is)\n",
      "['i think', 'the film is']\n",
      "('very', 'good')\n",
      "('i think', 'the film is')\n",
      "('the film is', 'very')\n",
      "('very', 'good')\n",
      "Nothing to merge, exiting\n",
      "['i think', 'the film is', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "tokens:list[str] = [\"i\", \"think\",\"the\", \"film\", \"is\", \"very\", \"good\"]\n",
    "\n",
    "while True:\n",
    "    did_merge = False\n",
    "    last_merge = None\n",
    "    new_tokens= []\n",
    "    for i in range(len(tokens)):\n",
    "        if did_merge:\n",
    "            did_merge = False\n",
    "            continue\n",
    "\n",
    "        if i == len(tokens)-1:\n",
    "            # only add the last token if we didn't merge\n",
    "            assert not did_merge\n",
    "            new_tokens.append(tokens[i])\n",
    "        else:\n",
    "            token, next = tokens[i], tokens[i+1]\n",
    "            print((token,next))\n",
    "            if (token,next) in merges:\n",
    "                new_tokens.append(token + \" \" + next)\n",
    "                did_merge = True\n",
    "                last_merge = (token, next)\n",
    "                print(f\"Merged ({token}, {next})\")\n",
    "                print(new_tokens)\n",
    "            else:\n",
    "                did_merge = False\n",
    "                new_tokens.append(token)\n",
    "    tokens = new_tokens[::]\n",
    "    if last_merge is None:\n",
    "        print(\"Nothing to merge, exiting\")\n",
    "        print(tokens)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42b1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df512eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7472f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19a091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f602d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ae46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded80ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816c211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9a1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284e2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dd7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd69a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultdict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec91f5322befbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, base_vocab: str, num_merges: int = 1000):\n",
    "        self.base_vocab = base_vocab\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = None\n",
    "        \n",
    "\n",
    "    def train(self, texts: list[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> list[list[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b962dead76165",
   "metadata": {},
   "source": [
    "### 2 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098e2f03e8756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique characters from the data\n",
    "unique_chars = # todo\n",
    "\n",
    "# Train the tokenizer\n",
    "bpe_char_based = BPETokenizer(base_vocab=unique_chars, num_merges=1000)\n",
    "bpe_char_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f703149dc6f6212",
   "metadata": {},
   "source": [
    "### 2 (b): Base Vocabulary = ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f3df97629f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = list(string.printable)\n",
    "\n",
    "bpe_ascii_based = BPETokenizer(base_vocab=ascii_chars, num_merges=1000)\n",
    "bpe_ascii_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040785368d1f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the two vocabularies\n",
    "print(\"Char-based initial_vocab sample:\")\n",
    "print(list(bpe_char_based.vocab.items())[:10])\n",
    "\n",
    "print(\"ASCII-based initial_vocab sample:\")\n",
    "print(list(bpe_ascii_based.vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0accb841574290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "\n",
    "print(\"Char-based tokenization:\", bpe_char_based.tokenize_word(unknown_word))\n",
    "print(\"ASCII-based tokenization:\", bpe_ascii_based.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6a186147a1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf732a0f4ab46",
   "metadata": {},
   "source": [
    "## Task 3: WordPiece Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/6) for a detailed explanation of the WordPiece algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623cc3770053da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=1000, initial_vocab=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.initial_vocab = set(initial_vocab) if initial_vocab else set()\n",
    "        self.vocab = {}\n",
    "        \n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> List[List[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be380a10518ee01a",
   "metadata": {},
   "source": [
    "### 3 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d7d6c2b72b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = # todo\n",
    "tokenizer_a = WordPieceTokenizer(vocab_size=1000, initial_vocab=corpus_chars)\n",
    "vocab_a = tokenizer_a.train(pre_processed_data)\n",
    "print(f\"Vocabulary A (corpus chars): {sorted(vocab_a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a2c84bd061f15",
   "metadata": {},
   "source": [
    "### 3 (b): Base Vocabulary = Characters from Reviews + ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb4ac09dba82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = set(string.printable)\n",
    "initial_vocab = sorted(corpus_chars.union(ascii_chars))\n",
    "tokenizer_b = WordPieceTokenizer(vocab_size=1000, initial_vocab=initial_vocab)\n",
    "vocab_b = tokenizer_b.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae659f632c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "print(\"WordPiece A tokenization:\", tokenizer_a.tokenize_word(unknown_word))\n",
    "print(\"WordPiece B tokenization:\", tokenizer_b.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4e4d14435b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816e87ce958841",
   "metadata": {},
   "source": [
    "## Task 4: Hugging Face Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163964ec653e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Byte-Pair Encoder (BPE)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790713337f1a1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face WordPiece Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5887194c04445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the different tokenizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
