{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310f24e6a7b88cce",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbba8a222d9df960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import re\n",
    "import glob\n",
    "import collections\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8bbdb",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6ae799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define base path to the dataset\n",
    "BASE_DATA_PATH = Path(\"../exercise_01/aclImdb\") \n",
    "\n",
    "TRAIN_PATH = BASE_DATA_PATH / \"train\"\n",
    "TEST_PATH = BASE_DATA_PATH / \"test\"\n",
    "\n",
    "TRAIN_POS_PATH = TRAIN_PATH / \"pos\"\n",
    "TRAIN_NEG_PATH = TRAIN_PATH / \"neg\"\n",
    "TEST_POS_PATH = TEST_PATH / \"pos\"\n",
    "TEST_NEG_PATH = TEST_PATH / \"neg\"\n",
    "\n",
    "assert TRAIN_POS_PATH.exists() and TRAIN_NEG_PATH.exists() and TEST_POS_PATH.exists() and TEST_NEG_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf348bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training reviews.\n",
      "Training labels distribution: Positive (1): 136943, Negative (0): -111943\n",
      "Loaded 25000 test reviews.\n",
      "Test labels distribution: Positive (1): 137824, Negative (0): -112824\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_data(data_path: Path) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Loads movie reviews and their sentiments from the specified path.\n",
    "\n",
    "    \"\"\"\n",
    "    texts:list[str] = []\n",
    "    labels:list[int] = [] \n",
    "\n",
    "    for _, folder_path in [(\"pos\", data_path / \"pos\"), (\"neg\", data_path / \"neg\")]:\n",
    "        if not folder_path.exists():\n",
    "            raise FileNotFoundError(f\"Warning: Path {folder_path} does not exist.\")\n",
    "\n",
    "        for file_path in folder_path.glob(\"*.txt\"):\n",
    "                # Extract score from filename, e.g., \"7_123.txt\" -> score 7\n",
    "            score = int(file_path.name.split('_')[0])\n",
    "            filename_parts = file_path.name.split('_')\n",
    "            score_str = filename_parts[1].split('.')[0]\n",
    "            score = int(score_str)\n",
    "\n",
    "            labels.append(score)\n",
    "            texts.append(file_path.read_text(encoding='utf-8'))\n",
    "    return texts, labels\n",
    "\n",
    "# Load training data\n",
    "train_texts_raw, train_labels = load_imdb_data(TRAIN_PATH)\n",
    "print(f\"Loaded {len(train_texts_raw)} training reviews.\")\n",
    "print(f\"Training labels distribution: Positive (1): {sum(train_labels)}, Negative (0): {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Load test data\n",
    "test_texts_raw, test_labels = load_imdb_data(TEST_PATH)\n",
    "print(f\"Loaded {len(test_texts_raw)} test reviews.\")\n",
    "print(f\"Test labels distribution: Positive (1): {sum(test_labels)}, Negative (0): {len(test_labels) - sum(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8274d38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :\n",
      "For a movie that gets no respect there sure are a ... Label : 9\n",
      "Bizarre horror movie filled with famous faces but ... Label : 8\n",
      "A solid, if unremarkable film. Matthau, as Einstei... Label : 7\n",
      "It's a strange feeling to sit alone in a theater o... Label : 8\n",
      "You probably all already know this by now, but 5 a... Label : 10\n",
      "I saw the movie with two grown children. Although ... Label : 8\n",
      "You're using the IMDb.<br /><br />You've given som... Label : 10\n",
      "This was a good film with a powerful message of lo... Label : 10\n",
      "Made after QUARTET was, TRIO continued the quality... Label : 10\n",
      "For a mature man, to admit that he shed a tear ove... Label : 10\n",
      "\n",
      "\n",
      "Test :\n",
      "Based on an actual story, John Boorman shows the s... Label : 9\n",
      "This is a gem. As a Film Four production - the ant... Label : 9\n",
      "I really like this show. It has drama, romance, an... Label : 9\n",
      "This is the best 3-D experience Disney has at thei... Label : 10\n",
      "Of the Korean movies I've seen, only three had rea... Label : 10\n",
      "this movie is funny funny funny my favorite quote ... Label : 7\n",
      "I'm just starting to explore the so far wonderful ... Label : 10\n",
      "There is no need for me to repeat the synopsis ren... Label : 10\n",
      "I got this movie with my BBC \"Jane Austen Collecti... Label : 9\n",
      "This was a great movie, I would compare it to the ... Label : 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\")\n",
    "for text, label in zip(train_texts_raw[:10], train_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")\n",
    "\n",
    "print(\"\\n\\nTest :\")\n",
    "for text, label in zip(test_texts_raw[:10], test_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefbf6c55ef2c7d",
   "metadata": {},
   "source": [
    "## Task 1: Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28bcbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff9b6df8eb909544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pre-processing pipeline\n",
    "pre_processed_data = preprocess(train_texts_raw, to_lowercase=True)\n",
    "pre_processed_data_test = preprocess(test_texts_raw, to_lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f6e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are']...\n",
      "['bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by']...\n",
      "['a', 'solid,', 'if', 'unremarkable', 'film.', 'matthau,', 'as', 'einstein,', 'was', 'wonderful.']...\n",
      "[\"it's\", 'a', 'strange', 'feeling', 'to', 'sit', 'alone', 'in', 'a', 'theater']...\n",
      "['you', 'probably', 'all', 'already', 'know', 'this', 'by', 'now,', 'but', 'NUMTOKEN']...\n",
      "['i', 'saw', 'the', 'movie', 'with', 'two', 'grown', 'children.', 'although', 'it']...\n",
      "[\"you're\", 'using', 'the', \"imdb.you've\", 'given', 'some', 'hefty', 'votes', 'to', 'some']...\n",
      "['this', 'was', 'a', 'good', 'film', 'with', 'a', 'powerful', 'message', 'of']...\n",
      "['made', 'after', 'quartet', 'was,', 'trio', 'continued', 'the', 'quality', 'of', 'the']...\n",
      "['for', 'a', 'mature', 'man,', 'to', 'admit', 'that', 'he', 'shed', 'a']...\n"
     ]
    }
   ],
   "source": [
    "for tokens in pre_processed_data[:10]:\n",
    "    print(f\"{tokens[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9e3a91b0ca578",
   "metadata": {},
   "source": [
    "## Task 2: Byte-Pair Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/5) for a detailed explanation of the BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec91f5322befbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, base_vocab: str, num_merges: int = 1000):\n",
    "        self.base_vocab = base_vocab\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = None\n",
    "        \n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> List[List[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b962dead76165",
   "metadata": {},
   "source": [
    "### 2 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098e2f03e8756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique characters from the data\n",
    "unique_chars = # todo\n",
    "\n",
    "# Train the tokenizer\n",
    "bpe_char_based = BPETokenizer(base_vocab=unique_chars, num_merges=1000)\n",
    "bpe_char_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f703149dc6f6212",
   "metadata": {},
   "source": [
    "### 2 (b): Base Vocabulary = ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f3df97629f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = list(string.printable)\n",
    "\n",
    "bpe_ascii_based = BPETokenizer(base_vocab=ascii_chars, num_merges=1000)\n",
    "bpe_ascii_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040785368d1f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the two vocabularies\n",
    "print(\"Char-based initial_vocab sample:\")\n",
    "print(list(bpe_char_based.vocab.items())[:10])\n",
    "\n",
    "print(\"ASCII-based initial_vocab sample:\")\n",
    "print(list(bpe_ascii_based.vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0accb841574290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "\n",
    "print(\"Char-based tokenization:\", bpe_char_based.tokenize_word(unknown_word))\n",
    "print(\"ASCII-based tokenization:\", bpe_ascii_based.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6a186147a1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf732a0f4ab46",
   "metadata": {},
   "source": [
    "## Task 3: WordPiece Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/6) for a detailed explanation of the WordPiece algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623cc3770053da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=1000, initial_vocab=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.initial_vocab = set(initial_vocab) if initial_vocab else set()\n",
    "        self.vocab = {}\n",
    "        \n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> List[List[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be380a10518ee01a",
   "metadata": {},
   "source": [
    "### 3 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d7d6c2b72b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = # todo\n",
    "tokenizer_a = WordPieceTokenizer(vocab_size=1000, initial_vocab=corpus_chars)\n",
    "vocab_a = tokenizer_a.train(pre_processed_data)\n",
    "print(f\"Vocabulary A (corpus chars): {sorted(vocab_a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a2c84bd061f15",
   "metadata": {},
   "source": [
    "### 3 (b): Base Vocabulary = Characters from Reviews + ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb4ac09dba82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = set(string.printable)\n",
    "initial_vocab = sorted(corpus_chars.union(ascii_chars))\n",
    "tokenizer_b = WordPieceTokenizer(vocab_size=1000, initial_vocab=initial_vocab)\n",
    "vocab_b = tokenizer_b.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae659f632c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "print(\"WordPiece A tokenization:\", tokenizer_a.tokenize_word(unknown_word))\n",
    "print(\"WordPiece B tokenization:\", tokenizer_b.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4e4d14435b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816e87ce958841",
   "metadata": {},
   "source": [
    "## Task 4: Hugging Face Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163964ec653e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Byte-Pair Encoder (BPE)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790713337f1a1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face WordPiece Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5887194c04445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the different tokenizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
