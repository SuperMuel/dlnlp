{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language and Code - Exercise 2\n",
    "\n",
    "**Prof. Dr. Steffen Herbold**\n",
    "**SoSe 2025**\n",
    "**Due on 2025/05/15**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## General information for all exercises (read carefully!)\n",
    "\n",
    "Within the \"Deep Learning for Natural Language and Code Exercise,\" you will execute different tasks that relate various NLP concepts. The main goal of these exercises is to teach you *how* to develop approaches. Once you have gained this knowledge, you will have the opportunity to use your own solution and compare it with existing solutions from popular libraries. This means that these exercises are not just about knowing how to use the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Problem description\n",
    "\n",
    "One task that is executed in NLP is sentiment analysis. In this exercise, we will focus on developing models that are able to classify reviews in different sentiments/emotions. For this, we assume that you have an initial implementation of the BOW from exercise # 1. For this exercise, we are going to work with a dataset that consists of movie reviews:\n",
    "\n",
    "*   Dataset: [https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "In StudIP, a Jupyter Notebook is provided as a template for this exercise. In the template, a series of sections are provided in order to help you organize and structure the code. Additionally, some blocks of codes are also provided to facilitate some exercise tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data set description\n",
    "\n",
    "For this exercise, we are going to work with the Large Movie Review Dataset [1]. This dataset was built for binary sentiment classification. It is composed of 25,000 highly polar movie reviews for training, and 25,000 for testing. Meaning that the middle values (i.e., scores of 5) are ignored. The dataset also contains unlabeled data. However, for this exercise, those are not going to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string \n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Loading\n",
    "\n",
    "We will load the Large Movie Review Dataset (IMDb).\n",
    "The dataset is divided into `train` and `test` sets, and each of these has `pos` (positive) and `neg` (negative) review folders.\n",
    "Filenames in `pos` and `neg` folders include the rating, e.g., `9_1.txt` (score 9, id 1).\n",
    "- Positive reviews: score >= 7\n",
    "- Negative reviews: score <= 4\n",
    "- Neutral reviews (score 5 or 6) are ignored in the main sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path to the dataset\n",
    "BASE_DATA_PATH = Path(\"../exercise_01/aclImdb\") \n",
    "\n",
    "TRAIN_PATH = BASE_DATA_PATH / \"train\"\n",
    "TEST_PATH = BASE_DATA_PATH / \"test\"\n",
    "\n",
    "TRAIN_POS_PATH = TRAIN_PATH / \"pos\"\n",
    "TRAIN_NEG_PATH = TRAIN_PATH / \"neg\"\n",
    "TEST_POS_PATH = TEST_PATH / \"pos\"\n",
    "TEST_NEG_PATH = TEST_PATH / \"neg\"\n",
    "\n",
    "assert TRAIN_POS_PATH.exists() and TRAIN_NEG_PATH.exists() and TEST_POS_PATH.exists() and TEST_NEG_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 training reviews.\n",
      "Training labels distribution: Positive (1): 136943, Negative (0): -111943\n",
      "Loaded 25000 test reviews.\n",
      "Test labels distribution: Positive (1): 137824, Negative (0): -112824\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_data(data_path: Path) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Loads movie reviews and their sentiments from the specified path.\n",
    "\n",
    "    \"\"\"\n",
    "    texts:list[str] = []\n",
    "    labels:list[int] = [] \n",
    "\n",
    "    for _, folder_path in [(\"pos\", data_path / \"pos\"), (\"neg\", data_path / \"neg\")]:\n",
    "        if not folder_path.exists():\n",
    "            raise FileNotFoundError(f\"Warning: Path {folder_path} does not exist.\")\n",
    "\n",
    "        for file_path in folder_path.glob(\"*.txt\"):\n",
    "                # Extract score from filename, e.g., \"7_123.txt\" -> score 7\n",
    "            score = int(file_path.name.split('_')[0])\n",
    "            filename_parts = file_path.name.split('_')\n",
    "            score_str = filename_parts[1].split('.')[0]\n",
    "            score = int(score_str)\n",
    "\n",
    "            labels.append(score)\n",
    "            texts.append(file_path.read_text(encoding='utf-8'))\n",
    "    return texts, labels\n",
    "\n",
    "# Load training data\n",
    "train_texts_raw, train_labels = load_imdb_data(TRAIN_PATH)\n",
    "print(f\"Loaded {len(train_texts_raw)} training reviews.\")\n",
    "print(f\"Training labels distribution: Positive (1): {sum(train_labels)}, Negative (0): {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Load test data\n",
    "test_texts_raw, test_labels = load_imdb_data(TEST_PATH)\n",
    "print(f\"Loaded {len(test_texts_raw)} test reviews.\")\n",
    "print(f\"Test labels distribution: Positive (1): {sum(test_labels)}, Negative (0): {len(test_labels) - sum(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :\n",
      "Train :\n",
      "For a movie that gets no respect there sure are a ... Label : 9\n",
      "Bizarre horror movie filled with famous faces but ... Label : 8\n",
      "A solid, if unremarkable film. Matthau, as Einstei... Label : 7\n",
      "It's a strange feeling to sit alone in a theater o... Label : 8\n",
      "You probably all already know this by now, but 5 a... Label : 10\n",
      "I saw the movie with two grown children. Although ... Label : 8\n",
      "You're using the IMDb.<br /><br />You've given som... Label : 10\n",
      "This was a good film with a powerful message of lo... Label : 10\n",
      "Made after QUARTET was, TRIO continued the quality... Label : 10\n",
      "For a mature man, to admit that he shed a tear ove... Label : 10\n",
      "\n",
      "\n",
      "Test :\n",
      "Based on an actual story, John Boorman shows the s... Label : 9\n",
      "This is a gem. As a Film Four production - the ant... Label : 9\n",
      "I really like this show. It has drama, romance, an... Label : 9\n",
      "This is the best 3-D experience Disney has at thei... Label : 10\n",
      "Of the Korean movies I've seen, only three had rea... Label : 10\n",
      "this movie is funny funny funny my favorite quote ... Label : 7\n",
      "I'm just starting to explore the so far wonderful ... Label : 10\n",
      "There is no need for me to repeat the synopsis ren... Label : 10\n",
      "I got this movie with my BBC \"Jane Austen Collecti... Label : 9\n",
      "This was a great movie, I would compare it to the ... Label : 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\")\n",
    "for text, label in zip(train_texts_raw[:10], train_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")\n",
    "\n",
    "print(\"\\n\\nTest :\")\n",
    "for text, label in zip(test_texts_raw[:10], test_labels):\n",
    "    print(f\"{text[:50]}... Label : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class PreprocessingConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration for the text preprocessing pipeline.\n",
    "    Defines various steps and their parameters for cleaning and preparing raw text data.\n",
    "    The order of application in a pipeline would typically be:\n",
    "    1. Lowercasing (if enabled)\n",
    "    2. Number replacement (if enabled)\n",
    "    3. Tokenization\n",
    "    4. Hyphenated word splitting (if enabled)\n",
    "    5. Punctuation removal (if enabled)\n",
    "    6. (External: Frequent term removal, after vocab construction)\n",
    "    \"\"\"\n",
    "\n",
    "    # PT#1 (a) Tokenization strategy\n",
    "    tokenize_on_punctuation: list[str] | Literal[False] = Field(\n",
    "        default=,\n",
    "        description=\"If True, tokenize based on spaces and punctuation characters (e.g., 'word.' -> ['word', '.']). \"\n",
    "                    \"If False, tokenize only on spaces (e.g., 'word.' -> ['word.']).\"\n",
    "    )\n",
    "\n",
    "    # PT#1 (b) Case conversion\n",
    "    lowercase: bool = Field(\n",
    "        default=True,\n",
    "        description=\"If True, convert all text to lowercase. If False, keep original capitalization.\"\n",
    "    )\n",
    "\n",
    "    # PT#1 (c) Punctuation removal\n",
    "    remove_punctuation: str | None = Field(\n",
    "        default=string.punctuation,\n",
    "        description=\"If str, remove all characters in the string. \"\n",
    "                    \"If 'tokenize_on_punctuation' is True, punctuation tokens are removed. \"\n",
    "                    \"If 'tokenize_on_punctuation' is False, punctuation is stripped from token ends. \"\n",
    "                    \"Care should be taken not to remove structural parts of special tokens like '<NUM>'.\"\n",
    "    )\n",
    "\n",
    "    # PT#1 (d) High-frequency term removal\n",
    "    max_df_percentage: float | None = Field(\n",
    "        default=None,\n",
    "        gt=0.0,\n",
    "        lt=1.0,\n",
    "        description=\"Remove terms that appear in more than this percentage of documents. \"\n",
    "                    \"Value must be between 0.0 and 1.0 (exclusive). If None, no terms are removed based on high frequency. \"\n",
    "    )\n",
    "\n",
    "    # PT#1 (e) Number replacement\n",
    "    number_replacement_token: str | None= Field(\n",
    "        default=\"<NUM>\",\n",
    "        min_length=1,\n",
    "        description=\"The special token to use when replacing numbers. If None, numbers are not replaced.\"\n",
    "    )\n",
    "\n",
    "    # split_hyphen\n",
    "    split_hyphenated_words: bool = Field(\n",
    "        default=False, # Often, keeping hyphenated words as single units is beneficial\n",
    "        description=\"If True, attempts to split hyphenated words into components. \"\n",
    "                    \"E.g., 'state-of-the-art' could become ['state', 'of', 'the', 'art'] or ['state', '-', 'of', '-', 'the', '-', 'art'] \"\n",
    "                    \"depending on the splitting logic and whether hyphens themselves are kept. \"\n",
    "                    \"This is typically applied after initial tokenization. \"\n",
    "                    \"If 'tokenize_on_punctuation' is True and hyphens are delimiters, this might be redundant or refine further.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags from text while preserving content.\n",
    "    Special case: preserves \"<3\" (heart symbol).\n",
    "    \"\"\"\n",
    "    # Temporarily replace \"<3\" with a placeholder\n",
    "    text_with_placeholder = text.replace(\"<3\", \"HEART_SYMBOL_PLACEHOLDER_XYZ\")\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = re.sub(r\"<[^>]*>\", \"\", text_with_placeholder)\n",
    "    # Restore the heart symbols\n",
    "    cleaned_text_final = cleaned_text.replace(\"HEART_SYMBOL_PLACEHOLDER_XYZ\", \"<3\")\n",
    "    return cleaned_text_final\n",
    "\n",
    "# Test basic HTML tag removal\n",
    "assert clean_html_tags(\"<p>Hello world</p>\") == \"Hello world\"\n",
    "\n",
    "# Test nested tags\n",
    "assert clean_html_tags(\"<div><p>Nested content</p></div>\") == \"Nested content\"\n",
    "\n",
    "# Test with attributes\n",
    "assert clean_html_tags('<a href=\"https://example.com\">Link text</a>') == \"Link text\"\n",
    "\n",
    "# Test with multiple tags and text\n",
    "assert clean_html_tags(\"<h1>Title</h1><p>Paragraph</p>\") == \"TitleParagraph\"\n",
    "\n",
    "# Test with the special case of \"<3\" (heart symbol)\n",
    "assert (\n",
    "    clean_html_tags(\n",
    "        \"I LUVED IT SO MUCH <3 <br /><br />its about a women...<br /><br /> her<br /><br />\"\n",
    "    )\n",
    "    == \"I LUVED IT SO MUCH <3 its about a women... her\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Tasks (PT)\n",
    "\n",
    "### PT1: Build a pipeline for cleaning the raw reviews.\n",
    "\n",
    "In this pipeline you should be able to activate or deactivate the following options:\n",
    "(a) Tokenize based on spaces and punctuation or only spaces.\n",
    "(b) Convert everything to lower case or keep the capitalization as it is.\n",
    "(c) Remove punctuation.\n",
    "(d) Remove the terms that appear more often than a certain percentage. You should be able to vary this percentage (i.e., it should be a parameter).\n",
    "(e) Replace the numbers with a token (i.e., a special token `<NUM>`).\n",
    "\n",
    "We will implement a function `pipeline_clean_review` that takes a raw text and flags for these options.\n",
    "The option (d) is a corpus-level operation and will be handled after tokenizing all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUMBER_RE = re.compile(r\"^\\d+(?:\\.\\d+)?$\")          # integers & simple floats\n",
    "_PUNCT_SPLIT_RE = re.compile(r\"\\w+|[^\\w\\s]\")         # words OR punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "    text: str,\n",
    "    mode: Literal[\"space\", \"space_punct\"] = \"space_punct\",\n",
    "    split_hyphen: bool = True\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenise *text* according to *mode* and optionally split on hyphens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Raw input string.\n",
    "    mode : {\"space\", \"space_punct\"}, default=\"space_punct\"\n",
    "        ``\"space\"`` – classic ``str.split()`` (split on all whitespace).\n",
    "        ``\"space_punct\"`` – split on whitespace **and** return punctuation as\n",
    "        standalone tokens.\n",
    "    split_hyphen : bool, default=True\n",
    "        If True, split tokens on hyphens (e.g., 'a-composed-word' -> ['a', 'composed', 'word']).\n",
    "        If False, keep hyphenated words as single tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Sequence of tokens.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> tokenize(\"Hello, world!\", mode=\"space\")\n",
    "    ['Hello,', 'world!']\n",
    "    >>> tokenize(\"Hello, world!\", mode=\"space_punct\")\n",
    "    ['Hello', ',', 'world', '!']\n",
    "    >>> tokenize(\"a-composed-word\", mode=\"space_punct\", split_hyphen=True)\n",
    "    ['a', 'composed', 'word']\n",
    "    >>> tokenize(\"a-composed-word\", mode=\"space_punct\", split_hyphen=False)\n",
    "    ['a-composed-word']\n",
    "    \"\"\"\n",
    "    if mode == \"space\":\n",
    "        tokens = text.split()\n",
    "    elif mode == \"space_punct\":\n",
    "        tokens = _PUNCT_SPLIT_RE.findall(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tokenisation mode: {mode}\")\n",
    "\n",
    "    if split_hyphen:\n",
    "        split_tokens: list[str] = []\n",
    "        for tok in tokens:\n",
    "            if '-' in tok and len(tok) > 1:\n",
    "                split_tokens.extend([t for t in tok.split('-') if t])\n",
    "            else:\n",
    "                split_tokens.append(tok)\n",
    "        tokens = split_tokens\n",
    "    return tokens\n",
    "\n",
    "assert tokenize(\"Hi there\", \"space\") == [\"Hi\", \"there\"]\n",
    "assert tokenize(\"Hi, there!\", \"space_punct\") == [\"Hi\", \",\", \"there\", \"!\"]\n",
    "sample_review_text = \"This is a GREAT movie  from 2023!! Loved it <3. Cost: $10. What's up? It's well-done.\"\n",
    "tokenize(sample_review_text)\n",
    "\n",
    "# Hyphen splitting assertions\n",
    "assert tokenize(\"a-composed-word\", mode=\"space_punct\", split_hyphen=True) == [\"a\", \"composed\", \"word\"]\n",
    "assert tokenize(\"a-composed-word\", mode=\"space_punct\", split_hyphen=False) == [\"a-composed-word\"]\n",
    "assert tokenize(\"well-done!\", mode=\"space_punct\", split_hyphen=True) == [\"well\", \"done\", \"!\"]\n",
    "assert tokenize(\"well-done!\", mode=\"space_punct\", split_hyphen=False) == [\"well-done\", \"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"a-composed-word\", mode=\"space_punct\", split_hyphen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(\n",
    "    token: str,\n",
    "    *,\n",
    "    lowercase: bool,\n",
    "    remove_punct: bool,\n",
    "    replace_numbers: bool,\n",
    ") -> str | None:\n",
    "    \"\"\"\n",
    "    Clean a single token according to the options provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The input token (a single word or symbol).\n",
    "    lowercase : bool\n",
    "        If True, convert the token to lower case (e.g., 'Hello' -> 'hello').\n",
    "    remove_punct : bool\n",
    "        If True, remove any punctuation characters from the token (e.g., 'hello!' -> 'hello').\n",
    "        If the token becomes empty after punctuation removal, returns None.\n",
    "    replace_numbers : bool\n",
    "        If True, any token consisting only of digits (or a simple decimal number) is replaced\n",
    "        with the string '<NUM>'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        The cleaned token, or None if the token is entirely removed (e.g., only punctuation).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> clean_token(\"Hello,\", lowercase=True, remove_punct=True, replace_numbers=False)\n",
    "    'hello'\n",
    "    >>> clean_token(\"123\", lowercase=False, remove_punct=False, replace_numbers=True)\n",
    "    '<NUM>'\n",
    "    >>> clean_token(\"!!!\", lowercase=False, remove_punct=True, replace_numbers=False)\n",
    "    None\n",
    "    >>> clean_token(\"Test\", lowercase=False, remove_punct=False, replace_numbers=False)\n",
    "    'Test'\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        token = token.lower()\n",
    "\n",
    "    if remove_punct:\n",
    "        token = token.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        if token == \"\":\n",
    "            return None\n",
    "\n",
    "    if replace_numbers and _NUMBER_RE.fullmatch(token):\n",
    "        token = \"<NUM>\"\n",
    "\n",
    "    return token\n",
    "\n",
    "assert clean_token(\"Hello,\", lowercase=True, remove_punct=True, replace_numbers=False) == \"hello\"\n",
    "assert clean_token(\"!!,\", lowercase=False, remove_punct=True, replace_numbers=False) is None\n",
    "assert clean_token(\"42\", lowercase=False, remove_punct=False, replace_numbers=True) == \"<NUM>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cleaning(\n",
    "    tokens: list[str],\n",
    "    *,\n",
    "    lowercase: bool,\n",
    "    remove_punct: bool,\n",
    "    replace_numbers: bool,\n",
    ") -> list[str]:\n",
    "    \"\"\"Vectorised wrapper around :pyfunc:`clean_token`.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> apply_cleaning([\"Hello,\", \"World!\", \"123\"], lowercase=True,\n",
    "    ...               remove_punct=True, replace_numbers=True)\n",
    "    ['hello', 'world', '<NUM>']\n",
    "    \"\"\"\n",
    "    cleaned: list[str] = []\n",
    "    for tok in tokens:\n",
    "        new_tok = clean_token(\n",
    "            tok,\n",
    "            lowercase=lowercase,\n",
    "            remove_punct=remove_punct,\n",
    "            replace_numbers=replace_numbers,\n",
    "        )\n",
    "        if new_tok:\n",
    "            cleaned.append(new_tok)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "assert apply_cleaning([\n",
    "    \"Hello,\", \"World!\", \"123\"\n",
    "], lowercase=True, remove_punct=True, replace_numbers=True) == [\n",
    "    \"hello\", \"world\", \"<NUM>\"\n",
    "]\n",
    "\n",
    "assert apply_cleaning([\n",
    "    \"Hello,\", \"!\", \"123\"\n",
    "], lowercase=True, remove_punct=True, replace_numbers=False) == [\n",
    "    \"hello\", \"123\"\n",
    "]\n",
    "\n",
    "\n",
    "assert apply_cleaning([\n",
    "    \"Hello,\", \"!\", \"123\"\n",
    "], lowercase=False, remove_punct=False, replace_numbers=False) == [\n",
    "'Hello,', '!', '123'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def remove_high_df_tokens(\n",
    "    docs: list[list[str]],\n",
    "    max_df: float,\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Remove tokens that appear in more than a given proportion of documents.\n",
    "\n",
    "    Document frequency (DF) is defined as the number of documents in which a token appears\n",
    "    at least once, divided by the total number of documents. This method removes any token\n",
    "    whose document frequency is greater than `max_df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list[list[str]]\n",
    "        Corpus represented as a list of tokenized documents (each document is a list of tokens).\n",
    "    max_df : float\n",
    "        Upper threshold for document frequency (0 < max_df < 1). For example,\n",
    "        max_df=0.8 will remove any token appearing in more than 80% of the documents.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_docs : list[list[str]]\n",
    "        Corpus with high-DF tokens removed from each document.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> docs = [\n",
    "    ...   [\"good\", \"movie\", \"the\"],\n",
    "    ...   [\"the\", \"bad\", \"movie\"],\n",
    "    ...   [\"the\", \"movie\", \"average\"]\n",
    "    ... ]\n",
    "    >>> remove_high_df_tokens(docs, max_df=0.7)\n",
    "    [['good'], ['bad'], ['average']]\n",
    "    # 'the' and 'movie' are removed because they appear in all 3 documents (DF=1.0)\n",
    "    \"\"\"\n",
    "    if not 0 < max_df < 1:\n",
    "        raise ValueError(\"max_df must be between 0 and 1 (exclusive)\")\n",
    "\n",
    "    num_docs = len(docs)\n",
    "    # Count in how many documents each token appears (DF)\n",
    "    df_counter = Counter()\n",
    "    for doc in docs:\n",
    "        df_counter.update(set(doc))  # set(doc) ensures each token counted once per doc\n",
    "\n",
    "    # Find tokens exceeding the DF threshold\n",
    "    high_df_tokens = {\n",
    "        token for token, df in df_counter.items()\n",
    "        if df / num_docs > max_df\n",
    "    }\n",
    "\n",
    "    # Remove high-DF tokens from each document\n",
    "    cleaned_docs = [\n",
    "        [tok for tok in doc if tok not in high_df_tokens]\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "docs = [\n",
    "    [\"good\", \"movie\", \"the\"],\n",
    "    [\"the\", \"bad\", \"movie\"],\n",
    "    [\"the\", \"movie\", \"average\"]\n",
    "]\n",
    "out = remove_high_df_tokens(docs, max_df=0.7)\n",
    "assert out == [[\"good\"], [\"bad\"], [\"average\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review_text = \"This is a GREAT <br>movie</br>  from 2023!! Loved it <3. Cost: $10. What's up? It's well-done.\"\n",
    "\n",
    "assert tokenize(sample_review_text) == [\n",
    "    \"This\",\n",
    "    \"is\",\n",
    "    \"a\",\n",
    "    \"GREAT\",\n",
    "    \"<\",\n",
    "    \"br\",\n",
    "    \">\",\n",
    "    \"movie\",\n",
    "    \"<\",\n",
    "    \"/\",\n",
    "    \"br\",\n",
    "    \">\",\n",
    "    \"from\",\n",
    "    \"2023\",\n",
    "    \"!\",\n",
    "    \"!\",\n",
    "    \"Loved\",\n",
    "    \"it\",\n",
    "    \"<\",\n",
    "    \"3\",\n",
    "    \".\",\n",
    "    \"Cost\",\n",
    "    \":\",\n",
    "    \"$\",\n",
    "    \"10\",\n",
    "    \".\",\n",
    "    \"What\",\n",
    "    \"'\",\n",
    "    \"s\",\n",
    "    \"up\",\n",
    "    \"?\",\n",
    "    \"It\",\n",
    "    \"'\",\n",
    "    \"s\",\n",
    "    \"well\",\n",
    "    \"-\",\n",
    "    \"done\",\n",
    "    \".\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
